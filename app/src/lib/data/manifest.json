{
  "quizName": "aws-ml-specialty-exam",
  "tree": [
    {
      "type": "topic",
      "id": "domain1-data-engineering",
      "label": "Domain 1: Data Engineering",
      "description": "Preparing data for model training",
      "questionIds": [
        "8d67b7868ed99f2129f09953830c3a067c3156d8bc47c46a2acef025bf127c28",
        "8fee6f6b2c4730cf0b5546d4afa26726965907530a3d257614bdfef73a712161",
        "3f5f0212f52bc521df634d50921b308da858b987e2a60152f4836e5dafd6c3b4",
        "3db8e4a9211504e753aaeeefc12613277a3ca0464f7aafd6b42a2050646dbf9e",
        "732c5f64bb2338e1a2087e029c45dfab6c17c60971768ad2db54395d3cd70c64",
        "5213e585c696e2d04e6ea6656e12116e2f933d928780e9f55cd5afa458cd92a7",
        "24f572fa39fd37482f2daec86f7d704fd347216d8d9ffb6f0028db9b57eb0a25",
        "027bcfbaa1eb0c174287e393d37d7b3e500c7e38b52c0607117d1b910819d292",
        "d943c6db0a22d45874231b549e508e34afb724baad9a804b7b294a52a624bc36",
        "ec655dcacca35a72499d840b22d2d7772f41f55a99212390ce77d893ef7baea6",
        "781b5793f8aae29d1bd08b92e89b6cc38fd5cbfe58b884276c60408eb194860d",
        "dcb3b7feb307632e8fe7dc588b0fc68e2031ebb2809362cef2c65681670e9a59",
        "32dbadab788a2a59615c3d4522db1734683e019f68258bd2b87ceb90261aac1c",
        "95d1b41b7f3c433f0fb90d2050c74fe4d4ea921a78c176e90fa6516ea0d2eb06",
        "0e0a16eaf13713e9b0f83e0df4698093dc1be94d02c1e7048f2ad32f3a4597ce",
        "e97113c078d3da93083dcfdf282984789df1e1888c5a11ee483ac25d7dab8671",
        "bc1f0fcc015e2d6d20e4f99d9400d379bc606bca6c3b48b6f8374b43622b7900",
        "ba3a65a408ed03c67f80ab3bce8edfb6734a480963809a7ffedc3f8e6403a701",
        "04443a569b0665f264c3ac26ae35b52051c9b80871ec68bf0667e6037ced5336",
        "ae66e4ab05528d301d8fc624a03cf298251ca58494f5deb2549ea7020244b967",
        "3d96753a85d420bc97ee4fb00bafda0e4ea772baa13a3659ef4c7a3224702aae",
        "e1a97a48449d99d9a4ae187a5d2af11065be488516047cb26ff1e308c8f98790",
        "703a4e34a231b8f945e2e9ce132401216341681097c36ed5c240ac67c6f33626",
        "06eca2657838df7f072ef245cf043775fb023a92a3700ba529ec592a0bf4171a",
        "be3ca8561dfa2f52680ef7d5ca02dd3124a45d16c93abe8978688bddab9fa454"
      ],
      "children": [
        {
          "type": "subtopic",
          "id": "domain1-data-engineering/data-repositories-for-ml",
          "label": "Data repositories for ML",
          "description": "",
          "questionIds": [
            "8d67b7868ed99f2129f09953830c3a067c3156d8bc47c46a2acef025bf127c28",
            "8fee6f6b2c4730cf0b5546d4afa26726965907530a3d257614bdfef73a712161",
            "3f5f0212f52bc521df634d50921b308da858b987e2a60152f4836e5dafd6c3b4",
            "3db8e4a9211504e753aaeeefc12613277a3ca0464f7aafd6b42a2050646dbf9e",
            "732c5f64bb2338e1a2087e029c45dfab6c17c60971768ad2db54395d3cd70c64",
            "5213e585c696e2d04e6ea6656e12116e2f933d928780e9f55cd5afa458cd92a7"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain1-data-engineering/data-ingestion-solutions",
          "label": "Data ingestion solutions",
          "description": "",
          "questionIds": [
            "24f572fa39fd37482f2daec86f7d704fd347216d8d9ffb6f0028db9b57eb0a25",
            "027bcfbaa1eb0c174287e393d37d7b3e500c7e38b52c0607117d1b910819d292",
            "d943c6db0a22d45874231b549e508e34afb724baad9a804b7b294a52a624bc36",
            "ec655dcacca35a72499d840b22d2d7772f41f55a99212390ce77d893ef7baea6",
            "781b5793f8aae29d1bd08b92e89b6cc38fd5cbfe58b884276c60408eb194860d",
            "dcb3b7feb307632e8fe7dc588b0fc68e2031ebb2809362cef2c65681670e9a59",
            "32dbadab788a2a59615c3d4522db1734683e019f68258bd2b87ceb90261aac1c",
            "95d1b41b7f3c433f0fb90d2050c74fe4d4ea921a78c176e90fa6516ea0d2eb06"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain1-data-engineering/data-transformation-solutions-(etl/elt)",
          "label": "Data transformation solutions (ETL/ELT)",
          "description": "",
          "questionIds": [
            "0e0a16eaf13713e9b0f83e0df4698093dc1be94d02c1e7048f2ad32f3a4597ce",
            "e97113c078d3da93083dcfdf282984789df1e1888c5a11ee483ac25d7dab8671",
            "bc1f0fcc015e2d6d20e4f99d9400d379bc606bca6c3b48b6f8374b43622b7900",
            "ba3a65a408ed03c67f80ab3bce8edfb6734a480963809a7ffedc3f8e6403a701",
            "04443a569b0665f264c3ac26ae35b52051c9b80871ec68bf0667e6037ced5336",
            "ae66e4ab05528d301d8fc624a03cf298251ca58494f5deb2549ea7020244b967",
            "3d96753a85d420bc97ee4fb00bafda0e4ea772baa13a3659ef4c7a3224702aae",
            "e1a97a48449d99d9a4ae187a5d2af11065be488516047cb26ff1e308c8f98790"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain1-data-engineering/cataloging,-query,-and-access-patterns",
          "label": "Cataloging, query, and access patterns",
          "description": "",
          "questionIds": [
            "703a4e34a231b8f945e2e9ce132401216341681097c36ed5c240ac67c6f33626",
            "06eca2657838df7f072ef245cf043775fb023a92a3700ba529ec592a0bf4171a"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain1-data-engineering/data-quality,-governance,-and-security-(de-focused)",
          "label": "Data quality, governance, and security (DE-focused)",
          "description": "",
          "questionIds": [
            "be3ca8561dfa2f52680ef7d5ca02dd3124a45d16c93abe8978688bddab9fa454"
          ],
          "children": []
        }
      ]
    },
    {
      "type": "topic",
      "id": "domain2-exploratory-data-analysis",
      "label": "Domain 2: Exploratory Data Analysis",
      "description": "Techniques for understanding, validating, and visualizing data to prepare it for machine learning.",
      "questionIds": [
        "07637b950f386c074a70c3ca365e1519d7b2feed579b6ede296a0dd616c6e26b",
        "1eef8d10e49d8d6c5623f0dd76d466b80f2787b61859df005e1cbbac2d9e936b",
        "82b5dac0c75a459c58246b438303267a567d3860a428735a02a9e2c02739f5cb",
        "b39caa1db3de20f183b81fbc220d470bba8eef3395bef5eda73e32b24448ece4",
        "90ab803b281594fe3f2beb6eeda2ee4d77b2a234914ac9eace6652e68efed22e",
        "d19b8c8530c22d697cf89463e7891d0475c39f3e3c54dfd7976e6e7c0e323b00",
        "afc3095d30ce55404106cd416a91373d9628c6c2e5e906f20395653eadbc1454",
        "2ef062ad3fb1224e5cd62a4687018788f44469588e1a3e8953aeb5a94c806e6b",
        "e8349a5cb6f917d4ff73ae1fd0a8358bbe3c43abc0f8b0a1572159e262d04ae9",
        "de487813518549e54d2461355e116136f40320732c98400fc13d201b71cebdd4",
        "3fc7c039a5b151b41ac4e136719e11be25f9065ce24b2c6cd43d96e556c301a3",
        "7bf7e922bbf9f11c733824368cf9fa313366d21f9c6288a89e3bddfad2bdc319",
        "1a466ae34a9caef012522d22ad8e7507c015e9a82bbbc39e1a478a245763743c",
        "8f363c62cc10cf0eac7c7a15ae5d3537605dfbb9f3cce89ba3ffc29d6957ae9c",
        "391c70806ef17de9a58b669b45b827d78b2ff26876e5564ff0d2c2c82dac55fa",
        "2e54eb6c8f76ae9a707b4c655728b17cfaf4e73be72e116b203ced7af9b6cabc",
        "96e486b88f6f2de3587da2931e973a6ebbada4afdeb57ea230434c5a2d7524d6",
        "d26a0df9c876f33725c092be2617665c79660f61f10917dd40a8d0c3b65dd5e1",
        "d9cfb1ca931d7f4034c2a95213296f0cc4bf7bd9b762bb65562d8aec36f206b2",
        "c5f9f82ec6c1c740c325cb89936250da114099c6ecc060ecad0a3a5b56bb400f",
        "20b7c80394f2ef3f31c9d43af363f0a7dd694e1a5e2deee032f9bddcd03ae2dd",
        "68e7993db859d2754bc89908aa95e0ee71d6a4dbf5a6f58366c8e778b2d5e91e",
        "11bdede4568a0ed9a6aec6a4ab504413aa66291a7ce9d9a45c35dd2a52e8a4a6",
        "7f7696d0aba82bdbb3515de3b0f395a5778103c93c1ee7d59a0a73a37ee810ce",
        "063c14bd87b1eeb5312084cf464226afb782a1b862e0456bde09818ee62fc802",
        "1a17cc242b757ede998c73ba3fa7e9a996b2757d3c681a434d7600bb28e362e4",
        "b7d5a505715b6e0e15dccf0b6cbd860181b2ec6a9cfda69b2f4b9733698b6818",
        "6bb7196b561250eb84537d8220b839febee444ee22fe98d433d6fea002e45b6b",
        "142e33d82c2c6b98997930f533b46e17eec4b980b6ae73054b369f01a09015cd",
        "b938805c658c2eb9eea325235d06b1aff31aba64051e9098a5e8cb992ea0e6ef",
        "de0e61e52f8be4bc2324a7e0d2768fa0fd75b4f4a5267c7093a5403c2a70fe65",
        "d8e1967c4ae06f218c4c68a953cddd70494223ab285efdaf3bc60f47995d2e7c",
        "c5c7e0b4ea9796e682dbec217a1fb61e5042a552b381b62d9d1bc1f339395c3e",
        "c83c9568209d30432005188201bea6c70ce8d9e35ccb2bab3117aa8c53f169c6",
        "dcd41c8b5d49f808eaf22e5de217bfa8ffa2f7407ea81797c7693da9291f94b8"
      ],
      "children": [
        {
          "type": "subtopic",
          "id": "domain2-exploratory-data-analysis/dataset-understanding-&-framing",
          "label": "Dataset understanding & framing",
          "description": "",
          "questionIds": [
            "07637b950f386c074a70c3ca365e1519d7b2feed579b6ede296a0dd616c6e26b",
            "1eef8d10e49d8d6c5623f0dd76d466b80f2787b61859df005e1cbbac2d9e936b",
            "82b5dac0c75a459c58246b438303267a567d3860a428735a02a9e2c02739f5cb"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain2-exploratory-data-analysis/data-quality-checks-&-cleaning-signals",
          "label": "Data quality checks & cleaning signals",
          "description": "",
          "questionIds": [
            "b39caa1db3de20f183b81fbc220d470bba8eef3395bef5eda73e32b24448ece4",
            "90ab803b281594fe3f2beb6eeda2ee4d77b2a234914ac9eace6652e68efed22e",
            "d19b8c8530c22d697cf89463e7891d0475c39f3e3c54dfd7976e6e7c0e323b00",
            "afc3095d30ce55404106cd416a91373d9628c6c2e5e906f20395653eadbc1454"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain2-exploratory-data-analysis/descriptive-statistics-&-summaries",
          "label": "Descriptive statistics & summaries",
          "description": "",
          "questionIds": [
            "2ef062ad3fb1224e5cd62a4687018788f44469588e1a3e8953aeb5a94c806e6b",
            "e8349a5cb6f917d4ff73ae1fd0a8358bbe3c43abc0f8b0a1572159e262d04ae9",
            "de487813518549e54d2461355e116136f40320732c98400fc13d201b71cebdd4",
            "3fc7c039a5b151b41ac4e136719e11be25f9065ce24b2c6cd43d96e556c301a3"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain2-exploratory-data-analysis/univariate-analysis-&-distributions",
          "label": "Univariate analysis & distributions",
          "description": "",
          "questionIds": [
            "7bf7e922bbf9f11c733824368cf9fa313366d21f9c6288a89e3bddfad2bdc319",
            "1a466ae34a9caef012522d22ad8e7507c015e9a82bbbc39e1a478a245763743c",
            "8f363c62cc10cf0eac7c7a15ae5d3537605dfbb9f3cce89ba3ffc29d6957ae9c",
            "391c70806ef17de9a58b669b45b827d78b2ff26876e5564ff0d2c2c82dac55fa"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain2-exploratory-data-analysis/outliers-&-anomalies",
          "label": "Outliers & anomalies",
          "description": "",
          "questionIds": [
            "2e54eb6c8f76ae9a707b4c655728b17cfaf4e73be72e116b203ced7af9b6cabc",
            "96e486b88f6f2de3587da2931e973a6ebbada4afdeb57ea230434c5a2d7524d6",
            "d26a0df9c876f33725c092be2617665c79660f61f10917dd40a8d0c3b65dd5e1"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain2-exploratory-data-analysis/bivariate-relationships-&-association",
          "label": "Bivariate relationships & association",
          "description": "",
          "questionIds": [
            "d9cfb1ca931d7f4034c2a95213296f0cc4bf7bd9b762bb65562d8aec36f206b2",
            "c5f9f82ec6c1c740c325cb89936250da114099c6ecc060ecad0a3a5b56bb400f",
            "20b7c80394f2ef3f31c9d43af363f0a7dd694e1a5e2deee032f9bddcd03ae2dd",
            "68e7993db859d2754bc89908aa95e0ee71d6a4dbf5a6f58366c8e778b2d5e91e",
            "11bdede4568a0ed9a6aec6a4ab504413aa66291a7ce9d9a45c35dd2a52e8a4a6"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain2-exploratory-data-analysis/multivariate-structure-&-dimensionality-intuition",
          "label": "Multivariate structure & dimensionality intuition",
          "description": "",
          "questionIds": [
            "7f7696d0aba82bdbb3515de3b0f395a5778103c93c1ee7d59a0a73a37ee810ce",
            "063c14bd87b1eeb5312084cf464226afb782a1b862e0456bde09818ee62fc802",
            "1a17cc242b757ede998c73ba3fa7e9a996b2757d3c681a434d7600bb28e362e4"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain2-exploratory-data-analysis/visualization-best-practices-&-pitfalls",
          "label": "Visualization best practices & pitfalls",
          "description": "",
          "questionIds": [
            "b7d5a505715b6e0e15dccf0b6cbd860181b2ec6a9cfda69b2f4b9733698b6818",
            "6bb7196b561250eb84537d8220b839febee444ee22fe98d433d6fea002e45b6b",
            "142e33d82c2c6b98997930f533b46e17eec4b980b6ae73054b369f01a09015cd"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain2-exploratory-data-analysis/target-focused-eda-&-problem-specific-diagnostics",
          "label": "Target-focused EDA & problem-specific diagnostics",
          "description": "",
          "questionIds": [
            "b938805c658c2eb9eea325235d06b1aff31aba64051e9098a5e8cb992ea0e6ef",
            "de0e61e52f8be4bc2324a7e0d2768fa0fd75b4f4a5267c7093a5403c2a70fe65",
            "d8e1967c4ae06f218c4c68a953cddd70494223ab285efdaf3bc60f47995d2e7c",
            "c5c7e0b4ea9796e682dbec217a1fb61e5042a552b381b62d9d1bc1f339395c3e"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain2-exploratory-data-analysis/eda-outputs-→-feature-engineering-&-modeling-decisions",
          "label": "EDA outputs → feature engineering & modeling decisions",
          "description": "",
          "questionIds": [
            "c83c9568209d30432005188201bea6c70ce8d9e35ccb2bab3117aa8c53f169c6",
            "dcd41c8b5d49f808eaf22e5de217bfa8ffa2f7407ea81797c7693da9291f94b8"
          ],
          "children": []
        }
      ]
    },
    {
      "type": "topic",
      "id": "domain3-modeling",
      "label": "Domain 3: Modeling",
      "description": "How to frame ML problems, select appropriate models, train effectively, tune hyperparameters, and evaluate models correctly.",
      "questionIds": [
        "e889c8c606260f3442c21e93e59ad7aa0005dad8fe00722c9498f456c4742fc3",
        "58c4118cff7ebf26d451a2e3ca87c8a115e7a6fd6331900ae98dcb702a7a2971",
        "12088deb4ef06b7cdd890f0ff8b69e3c75882f3174fc5dd6380cf4d390680898",
        "5ec5d2dcfc8a2e10c559610c97d10451702102c6f51397d3922db367b05d1b9d",
        "bcfd3e39798b276cdabe5787bc1e7859d2aeaf6ce4eaa82f249ca41f96cc7b4e",
        "557c0b56870c1a5168168de68588f91bfb203d9f69404edcd7977ebdbf410260",
        "db5ba23dec64a865fb2e3cf864d2e345c0ca0082cf26dc7de5e65a7aa8c170b0",
        "a0cc2a04e60945cb09a663ddfdf733926da0162b55d27271b3bb7e128c36d06f",
        "a5c36464c083da5c3341646b05af30c9ea86df03b4bfac119b7b860e18958d01",
        "f874e254314b78e9481ff006ea19fc87262c7d3ed43162a501c6cb00c7924efd",
        "4bceb98511693fd54da6a71fec4e4e9e8162fdc5584e503bb0c36cc5c6accd56",
        "b5058767a1221f07658f913431c6494038bc8cabf3849cfce501199ccb80bb9c",
        "55825b88215aa0681bc7f4c0633c57070b00de079633a657407aaa18ff4c0083",
        "2a77e8ed7fceb9ab77f2eb4f85708327ad624430dea843c5ceb3c7b9e76c4705",
        "680a8ea9c1d40e631a454330b1a873fff75da6ef9b1acba8779113a2e7598356",
        "15d644afe7acbe16a3096cd0422250b78453c7ba69ec6daaf26f9662aff45cdf",
        "8418f67aa59bed1211f82029e113a9274aab5f26af760df7a1a7f503d2af671f",
        "13a3d72165612d61be9f1b432d419037687a9595d9e4270e00b95c101c39cb7a",
        "1e1cc1fe580163bd2184fcb1b369c5a435a1fffb8086031154dff46c525afb9f",
        "77f3e6be375a4b5faf54d395dbebef9fdc79fabfd7f4a4c09fb4bfddef37b761",
        "cf4bea82c9639008f97623c0cb0e184b605fa26ab31d4f0f4ea90344c84016f5",
        "3b05b123d48f14f4030266b4146326422d053016b935db325a12e8a7c50b12a1",
        "3974986d53e997b06a26ac96206aed24ab66457749dd479ec52564dd96ddcf35",
        "e58b9a0e7170bad9e88d1f61dfc798a50871f546c8c7c907f011a79eafec8727",
        "ab0460fff7582a74f4aedf766769ff9874993fdc55fa09982bb8bfa53f5c03bb",
        "da852f424e5d40a206a152f8ca05d0c7c26e18304502e18071f36796d7e9e63e",
        "947283a44e8294a5fc32258f02160f51a5f84c0872fbded44237c4cf7f097341",
        "cf0a349a90afbf299e092011c710b9e97a879ce2e475a489ca0d5422ed3d3cdb",
        "8c0694072c67f86b808493f5420e6f8cfb0ecd29808c64337b9e081311729ee1",
        "f5292bfdd4e47cd17b259fdcc7d2df2ae5a5a73b7989049518a91e8635c53ab1",
        "21451ae13dc6a382228153cba0a5c980e6afdbed443c21847922fc6b94a4eb89",
        "b2efeba35e8674bbbb4167829f31cf514a81656d2e2d99122d334d0c63a43fe3",
        "0d5730e27be5aa96658e3338bf48aa21ec9631b214cbe31e0e84828156dd177f",
        "53c4d079acaeff5a48ff425ad5b38a1f9e5790bbb56a88fcfc494aab02eb74a2",
        "e62367ade744dce20986bdafbebe03977f660ebf74c71131f9762cc392497fea",
        "1622038bcf56d1a31a797751690bb6729bd26869fedbfc9050c41fc23bf849ac",
        "d0557f6464108b5ae7d4fcb90488391435c00da5f611eaa168c2d356f13242f4",
        "e19eb1ad93816e0822fb23a551032561c17c9fac2d8e7dfd2d05dae5ed6d1d14",
        "4683de478ede4643b0f087b0b12b51834ce2183804ae4579924959673d6c1ab9",
        "fdced844b8a6b8ada024d0e17799a566142c2cc3930c3ca4ea18e9071b6d726b",
        "c9ee45d8bdc5b35bd5a2fee36d988b1c58f0e335a8cba15f3add19c747f2dce7",
        "b351664dfd1aba6fed6d25f7118ae1ca332df285de763ecb053c99abeeb18903",
        "adc49b25a749d44c8606ab402ee986de71b537ab88b8936e1dac1ed9a34c1566",
        "c35a540cc2c27a0aff68e049a2dc7b88a31ea19dcccb11318392b25918009e0c",
        "70fe7657274e2809fcfbe31774b0ab0cb77a5ac5bc8d42d8762aa095fbe1817d",
        "e71c1a24fd6fa28d5e909d1f5f2457f8d8de0fa7563f1a638236be5db85a3bba",
        "731ddf985b23b018e70a2325cfdae73a6eae8e5a30590cf47199352753d8f978",
        "ccdefb876ce8d05db5d5e81d6a3feb5b7c42ece03059fe25682f31574449dbb4",
        "e743cf5d13d606a54c5fa7dc32a264394a39d16271d4bc6389b6ffbfcd8d7af1",
        "59fa9bf550a2d2d7911fea110e7134c810c21b700b9295f687d3ed6b8da15295"
      ],
      "children": [
        {
          "type": "subtopic",
          "id": "domain3-modeling/3.1-framing-business-problems-as-ml-problems",
          "label": "3.1 Framing business problems as ML problems",
          "description": "",
          "questionIds": [
            "e889c8c606260f3442c21e93e59ad7aa0005dad8fe00722c9498f456c4742fc3",
            "58c4118cff7ebf26d451a2e3ca87c8a115e7a6fd6331900ae98dcb702a7a2971",
            "12088deb4ef06b7cdd890f0ff8b69e3c75882f3174fc5dd6380cf4d390680898",
            "5ec5d2dcfc8a2e10c559610c97d10451702102c6f51397d3922db367b05d1b9d",
            "bcfd3e39798b276cdabe5787bc1e7859d2aeaf6ce4eaa82f249ca41f96cc7b4e",
            "557c0b56870c1a5168168de68588f91bfb203d9f69404edcd7977ebdbf410260",
            "db5ba23dec64a865fb2e3cf864d2e345c0ca0082cf26dc7de5e65a7aa8c170b0",
            "a0cc2a04e60945cb09a663ddfdf733926da0162b55d27271b3bb7e128c36d06f"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain3-modeling/3.2-model-selection-&-problem–algorithm-fit",
          "label": "3.2 Model selection & problem–algorithm fit",
          "description": "",
          "questionIds": [
            "a5c36464c083da5c3341646b05af30c9ea86df03b4bfac119b7b860e18958d01",
            "f874e254314b78e9481ff006ea19fc87262c7d3ed43162a501c6cb00c7924efd",
            "4bceb98511693fd54da6a71fec4e4e9e8162fdc5584e503bb0c36cc5c6accd56",
            "b5058767a1221f07658f913431c6494038bc8cabf3849cfce501199ccb80bb9c",
            "55825b88215aa0681bc7f4c0633c57070b00de079633a657407aaa18ff4c0083",
            "2a77e8ed7fceb9ab77f2eb4f85708327ad624430dea843c5ceb3c7b9e76c4705",
            "680a8ea9c1d40e631a454330b1a873fff75da6ef9b1acba8779113a2e7598356",
            "15d644afe7acbe16a3096cd0422250b78453c7ba69ec6daaf26f9662aff45cdf",
            "8418f67aa59bed1211f82029e113a9274aab5f26af760df7a1a7f503d2af671f",
            "13a3d72165612d61be9f1b432d419037687a9595d9e4270e00b95c101c39cb7a",
            "1e1cc1fe580163bd2184fcb1b369c5a435a1fffb8086031154dff46c525afb9f",
            "77f3e6be375a4b5faf54d395dbebef9fdc79fabfd7f4a4c09fb4bfddef37b761",
            "cf4bea82c9639008f97623c0cb0e184b605fa26ab31d4f0f4ea90344c84016f5"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain3-modeling/3.3-training-fundamentals-&-optimization-behavior",
          "label": "3.3 Training fundamentals & optimization behavior",
          "description": "",
          "questionIds": [
            "3b05b123d48f14f4030266b4146326422d053016b935db325a12e8a7c50b12a1",
            "3974986d53e997b06a26ac96206aed24ab66457749dd479ec52564dd96ddcf35",
            "e58b9a0e7170bad9e88d1f61dfc798a50871f546c8c7c907f011a79eafec8727",
            "ab0460fff7582a74f4aedf766769ff9874993fdc55fa09982bb8bfa53f5c03bb",
            "da852f424e5d40a206a152f8ca05d0c7c26e18304502e18071f36796d7e9e63e",
            "947283a44e8294a5fc32258f02160f51a5f84c0872fbded44237c4cf7f097341",
            "cf0a349a90afbf299e092011c710b9e97a879ce2e475a489ca0d5422ed3d3cdb",
            "8c0694072c67f86b808493f5420e6f8cfb0ecd29808c64337b9e081311729ee1",
            "f5292bfdd4e47cd17b259fdcc7d2df2ae5a5a73b7989049518a91e8635c53ab1",
            "21451ae13dc6a382228153cba0a5c980e6afdbed443c21847922fc6b94a4eb89"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain3-modeling/regularization-&-generalization-control",
          "label": "Regularization & generalization control",
          "description": "",
          "questionIds": [
            "b2efeba35e8674bbbb4167829f31cf514a81656d2e2d99122d334d0c63a43fe3",
            "0d5730e27be5aa96658e3338bf48aa21ec9631b214cbe31e0e84828156dd177f",
            "53c4d079acaeff5a48ff425ad5b38a1f9e5790bbb56a88fcfc494aab02eb74a2",
            "e62367ade744dce20986bdafbebe03977f660ebf74c71131f9762cc392497fea",
            "1622038bcf56d1a31a797751690bb6729bd26869fedbfc9050c41fc23bf849ac"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain3-modeling/3.4-hyperparameter-optimization",
          "label": "3.4 Hyperparameter optimization",
          "description": "",
          "questionIds": [
            "d0557f6464108b5ae7d4fcb90488391435c00da5f611eaa168c2d356f13242f4",
            "e19eb1ad93816e0822fb23a551032561c17c9fac2d8e7dfd2d05dae5ed6d1d14",
            "4683de478ede4643b0f087b0b12b51834ce2183804ae4579924959673d6c1ab9",
            "fdced844b8a6b8ada024d0e17799a566142c2cc3930c3ca4ea18e9071b6d726b",
            "c9ee45d8bdc5b35bd5a2fee36d988b1c58f0e335a8cba15f3add19c747f2dce7",
            "b351664dfd1aba6fed6d25f7118ae1ca332df285de763ecb053c99abeeb18903",
            "adc49b25a749d44c8606ab402ee986de71b537ab88b8936e1dac1ed9a34c1566"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain3-modeling/3.5-evaluate-machine-learning-models",
          "label": "3.5 Evaluate machine learning models",
          "description": "",
          "questionIds": [
            "c35a540cc2c27a0aff68e049a2dc7b88a31ea19dcccb11318392b25918009e0c",
            "70fe7657274e2809fcfbe31774b0ab0cb77a5ac5bc8d42d8762aa095fbe1817d",
            "e71c1a24fd6fa28d5e909d1f5f2457f8d8de0fa7563f1a638236be5db85a3bba",
            "731ddf985b23b018e70a2325cfdae73a6eae8e5a30590cf47199352753d8f978",
            "ccdefb876ce8d05db5d5e81d6a3feb5b7c42ece03059fe25682f31574449dbb4",
            "e743cf5d13d606a54c5fa7dc32a264394a39d16271d4bc6389b6ffbfcd8d7af1",
            "59fa9bf550a2d2d7911fea110e7134c810c21b700b9295f687d3ed6b8da15295"
          ],
          "children": []
        }
      ]
    },
    {
      "type": "topic",
      "id": "domain4-ml-implementation-and-operations",
      "label": "Domain 4: Machine Learning Implementation and Operations",
      "description": "Deploy, operate, secure, and scale ML solutions on AWS—covering SageMaker lifecycle, MLOps practices, reliability patterns, and service selection.",
      "questionIds": [
        "1fa8972bfd54272d12b22be6c810058138b1130441e080d207fdcd0cc10b6f82",
        "2b8750b2bfd823f21ada2e5fd3a0525e9fa3a2525d127f4b95b0c15069092615",
        "074783996c736ce3a48cd86e4a3284985aaa403b0a09a6520431d421a6931dac",
        "b0c81e130c937bcdc686841e6af1e6b0c711ada03202dda23c8e0a4af7311c89",
        "c3db816d297747421671f951a8802435fe32d9a5327582ee2d2a64b9db131184",
        "c3053ef9c81ae7592ad7cbfc90c73367ee14a3318bc06be0ede17ccb7d9b3382",
        "c7e2f480fc62d412da666b021bf8b14283a297cecf1a1850f655ffe90c100d9d",
        "da9304293116a1671b62158c311c0f9aaaa15f446a55b04b832db120346a5b22",
        "f8df098e73ba844bd99bac7cfabb9d6f00ee370fabbf380231f7b9b1ec8b3d40",
        "28f682f246cb513a680136de83c7d96a1f798c4a6d9779571901043b9d541076",
        "0e1b8f816712b5d61421260b48d35cb688faf7665d6a401771e0c60957dc5c79",
        "bbee6baf6ec61fc8823d1a4a155287bfd6db747d2c34e32053d266e4beac2a23",
        "a5c9e663a5efca1f53107edb066cc3d37c5689a32950c24038cad8518fac1257",
        "37db7cdfdcf547b71985c6506485100cdd07232acbd92e6eff25f53e6ebe54b6",
        "6e052c18b1d7fb3e23649caaffbad83b79b0f78f5229c0b8eac4bd3c317bce79",
        "54d2d763afb03df09855f60b6c9ba6c2fcbad7bcad52ae176d0785d516216a99",
        "0a22cbac2c6e1a17af9179940b652aff16404537f400aef31d952962837cca90",
        "91c96ede3f472ee91de21658b622eeac59af57777463dd070c01ad4c6937bf83",
        "86101ad84ba9adca77f5e6bd52c4e698aa2afb6697b8c617a629ef122dae9882",
        "245777e8b71a736ad67487c6172fbdf5cca6e931cce275866a58d8fcac4afbeb",
        "f1c2c26a14cbc1e74300b09fcef6c0726563800cd60497c3927f745eb0d5c616",
        "4a84d3126626c59e73c44737611f7ef781c7d15cadf4911fc728a7a93214995a",
        "c03690e6d65b0ec2dc4873c8c041e61f5abcc7e86e2bd2ce1bc7fb88ba23cc19",
        "1d5d99c9d89b7bf06669a1e830873e7622b9d5122c4a1609a0ffdea3acce90d1",
        "27020cffff2c9dc13b3630aa61237c964324b905739fb720a725f944ae461421"
      ],
      "children": [
        {
          "type": "subtopic",
          "id": "domain4-ml-implementation-and-operations/4.4-deploy-and-operationalize-ml-solutions-(mlops)",
          "label": "4.4 Deploy and operationalize ML solutions (MLOps)",
          "description": "",
          "questionIds": [
            "1fa8972bfd54272d12b22be6c810058138b1130441e080d207fdcd0cc10b6f82",
            "2b8750b2bfd823f21ada2e5fd3a0525e9fa3a2525d127f4b95b0c15069092615",
            "074783996c736ce3a48cd86e4a3284985aaa403b0a09a6520431d421a6931dac",
            "b0c81e130c937bcdc686841e6af1e6b0c711ada03202dda23c8e0a4af7311c89",
            "c3db816d297747421671f951a8802435fe32d9a5327582ee2d2a64b9db131184",
            "c3053ef9c81ae7592ad7cbfc90c73367ee14a3318bc06be0ede17ccb7d9b3382",
            "c7e2f480fc62d412da666b021bf8b14283a297cecf1a1850f655ffe90c100d9d",
            "da9304293116a1671b62158c311c0f9aaaa15f446a55b04b832db120346a5b22",
            "f8df098e73ba844bd99bac7cfabb9d6f00ee370fabbf380231f7b9b1ec8b3d40"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain4-ml-implementation-and-operations/4.1-performance,-availability,-scalability,-resiliency,-fault-tolerance",
          "label": "4.1 Performance, availability, scalability, resiliency, fault tolerance",
          "description": "",
          "questionIds": [
            "28f682f246cb513a680136de83c7d96a1f798c4a6d9779571901043b9d541076",
            "0e1b8f816712b5d61421260b48d35cb688faf7665d6a401771e0c60957dc5c79",
            "bbee6baf6ec61fc8823d1a4a155287bfd6db747d2c34e32053d266e4beac2a23",
            "a5c9e663a5efca1f53107edb066cc3d37c5689a32950c24038cad8518fac1257",
            "37db7cdfdcf547b71985c6506485100cdd07232acbd92e6eff25f53e6ebe54b6",
            "6e052c18b1d7fb3e23649caaffbad83b79b0f78f5229c0b8eac4bd3c317bce79"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain4-ml-implementation-and-operations/4.2-choose-and-implement-the-right-aws-ml-services/features",
          "label": "4.2 Choose and implement the right AWS ML services/features",
          "description": "",
          "questionIds": [
            "54d2d763afb03df09855f60b6c9ba6c2fcbad7bcad52ae176d0785d516216a99",
            "0a22cbac2c6e1a17af9179940b652aff16404537f400aef31d952962837cca90",
            "91c96ede3f472ee91de21658b622eeac59af57777463dd070c01ad4c6937bf83",
            "86101ad84ba9adca77f5e6bd52c4e698aa2afb6697b8c617a629ef122dae9882",
            "245777e8b71a736ad67487c6172fbdf5cca6e931cce275866a58d8fcac4afbeb",
            "f1c2c26a14cbc1e74300b09fcef6c0726563800cd60497c3927f745eb0d5c616"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain4-ml-implementation-and-operations/4.3-aws-security-practices-for-ml-solutions",
          "label": "4.3 AWS security practices for ML solutions",
          "description": "",
          "questionIds": [
            "4a84d3126626c59e73c44737611f7ef781c7d15cadf4911fc728a7a93214995a",
            "c03690e6d65b0ec2dc4873c8c041e61f5abcc7e86e2bd2ce1bc7fb88ba23cc19",
            "1d5d99c9d89b7bf06669a1e830873e7622b9d5122c4a1609a0ffdea3acce90d1",
            "27020cffff2c9dc13b3630aa61237c964324b905739fb720a725f944ae461421"
          ],
          "children": []
        }
      ]
    },
    {
      "type": "topic",
      "id": "domain5-amazon-sagemaker-built-in-algorithms",
      "label": "Domain 5: Solving Problems with Amazon SageMaker Built-in Algorithms",
      "description": "Select and apply SageMaker built-in algorithms across tabular, time-series, recommendation, NLP, vision, and unsupervised/anomaly use cases—focusing on problem framing, tradeoffs, and practical deployment considerations.",
      "questionIds": [
        "2802d327bc0d264cfee178e143ca7e53175685e35f96b3ed466b86f4219b8fe3",
        "2f5d49bac3973bee60bcbfd0174c498b676a006568cdeda410bff7facd3f4180",
        "5cad28df708735efb4d730086914efd112709bffee5f0e93326617c86ca778a2",
        "ff20ef33dc313fe729eb51fee6e9127f0181bafacaf12c766caa15953448dc37",
        "1b5d10e37c8d878b6e18ae92d93864079074ce8f2008f618839a4bdc284db597",
        "06714558c521426134bc2731cf940012cdc886cc3cfb0693b1feb101dcb4b44f",
        "7a58b7639936fd9c78ce804cb4c5e5bb628a20bc8060636ca958d8ada06f3a3b",
        "b9c8ac6fce7fbdbbf9e01b41fed016d69cbeedb122fc3bcfdb00bdb544e6f081",
        "3a2bfd0f2debb1f59edad4e44b45b645fea14bf6de7cad129816c03964f177cc",
        "c1590cb6257c454446139efffb416323907bb08d032c84e461fb2ab23e7431aa",
        "c81ede70ae49e7eb5aaffe71dfe3c55473dbb5009aa9037e84a6fd38a18edf14",
        "cd1030705ee3bb872ad06799594dc6a587f675dbe7ec5f9b99333779eaa8e63c",
        "eda571e87df2dea86a524a17c2592ccefc71429c864f679d3f455984a8bc5388",
        "22010babd978d7bc2150512b080828c85805420340d8b8ac1af2cdba820f2936",
        "b839474be57d867c9306edaf7ea8c3a42579fd9d5d8175554df9c14815587e72",
        "c5c28ede8e9734a79fdbf599705a2caf67b296d0c5be1cfbc00584563086ed67",
        "7bf078f5ece78c299989dc69a68eab62e8c07eac9929fcb3134bcafe674f10ff",
        "17bcb686f9d9a8dd90c00e69a5f441eb456362b74783cb7a9bc40a7decb4db8a",
        "1819d6f8054c5b8a3c2524134ed425de5e0b3083e64b5eb387ddf602a7ff97c9",
        "cdcc25d1118055469cd09a0fa5e57cd6fadf00c8d895d5a6861c2a5484ff9157",
        "aa46ecb96669c5f9a386286fdcf53bb2bb458cfbad44f53c0c2cbf762d5a8b8f",
        "5da5f7de76c5252a68ca645218797c39105a55d8c952120fd5de87bb1d602067",
        "1f15f604239d28a333f484b678439c078cf4ecde278aaf4eaaedc8b1697fe4e1",
        "876971f1b0b306146b8f3ab3491493cb663815f2bb364ae6fea9488d41dc5f9b",
        "d54da27070b53d8c4b5a1be4c04d2e8b3b7436f5166f000e99281263532b3889",
        "000c0cd046613b11e9917ddbdaefc2d3737c7b5c720e90f55dbe241dc3e2aa53",
        "af5dfa1b3648002aef01d8f42a07940d4bf6f13d98d921eb48528dacabe9ff91",
        "12167a4b7e7bb67059c99e80adf1815a6cd2e35c1289f6ea5e51cacd0a14874e",
        "39661fd28b45def760ae4f5aabebb2f475115fdd3962d69180d55b1d0fd0b184",
        "7a34f3c3ddb717c6910719e690cb4188d14caa3f5f7698a94baa7af31ad83616",
        "00222b3e551c2e401fe93fe8775b529b973c5bb6f97a3f8feba76c7bfede5dde",
        "8c2135a4675a90383fb3ffd1ad998b6ae95feb7afbff91e32d4f944cd120e654",
        "1a41ec048551eb5f58950506df4758b822dda64851713fae538b88af0e1badea",
        "e81527624e4d6eaf53ccee6fe2f211037646e556ef96844f9cdf45108024891c",
        "f395da31ad6f2d3d50c4f91e5a5cfe89f6956f8f5f3c7389c71df661c4c4bfc2"
      ],
      "children": [
        {
          "type": "subtopic",
          "id": "domain5-amazon-sagemaker-built-in-algorithms/problem-→-algorithm-selection-cheat-sheet-(decisioning-+-tradeoffs)",
          "label": "Problem → algorithm selection cheat-sheet (decisioning + tradeoffs)",
          "description": "",
          "questionIds": [
            "2802d327bc0d264cfee178e143ca7e53175685e35f96b3ed466b86f4219b8fe3",
            "2f5d49bac3973bee60bcbfd0174c498b676a006568cdeda410bff7facd3f4180",
            "5cad28df708735efb4d730086914efd112709bffee5f0e93326617c86ca778a2",
            "ff20ef33dc313fe729eb51fee6e9127f0181bafacaf12c766caa15953448dc37"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain5-amazon-sagemaker-built-in-algorithms/supervised-tabular:-classification-/-regression-/-ranking-(core-workhorse-set)",
          "label": "Supervised tabular: classification / regression / ranking (core workhorse set)",
          "description": "",
          "questionIds": [
            "1b5d10e37c8d878b6e18ae92d93864079074ce8f2008f618839a4bdc284db597",
            "06714558c521426134bc2731cf940012cdc886cc3cfb0693b1feb101dcb4b44f",
            "7a58b7639936fd9c78ce804cb4c5e5bb628a20bc8060636ca958d8ada06f3a3b",
            "b9c8ac6fce7fbdbbf9e01b41fed016d69cbeedb122fc3bcfdb00bdb544e6f081",
            "3a2bfd0f2debb1f59edad4e44b45b645fea14bf6de7cad129816c03964f177cc",
            "c1590cb6257c454446139efffb416323907bb08d032c84e461fb2ab23e7431aa",
            "c81ede70ae49e7eb5aaffe71dfe3c55473dbb5009aa9037e84a6fd38a18edf14",
            "cd1030705ee3bb872ad06799594dc6a587f675dbe7ec5f9b99333779eaa8e63c",
            "eda571e87df2dea86a524a17c2592ccefc71429c864f679d3f455984a8bc5388"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain5-amazon-sagemaker-built-in-algorithms/time-series-forecasting-(deepar-and-probabilistic-forecasting)",
          "label": "Time-series forecasting (DeepAR and probabilistic forecasting)",
          "description": "",
          "questionIds": [
            "22010babd978d7bc2150512b080828c85805420340d8b8ac1af2cdba820f2936",
            "b839474be57d867c9306edaf7ea8c3a42579fd9d5d8175554df9c14815587e72",
            "c5c28ede8e9734a79fdbf599705a2caf67b296d0c5be1cfbc00584563086ed67",
            "7bf078f5ece78c299989dc69a68eab62e8c07eac9929fcb3134bcafe674f10ff",
            "17bcb686f9d9a8dd90c00e69a5f441eb456362b74783cb7a9bc40a7decb4db8a"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain5-amazon-sagemaker-built-in-algorithms/recommenders-+-sparse-/-high-dimensional-data-(factorization-machines)",
          "label": "Recommenders + sparse / high-dimensional data (Factorization Machines)",
          "description": "",
          "questionIds": [
            "1819d6f8054c5b8a3c2524134ed425de5e0b3083e64b5eb387ddf602a7ff97c9",
            "cdcc25d1118055469cd09a0fa5e57cd6fadf00c8d895d5a6861c2a5484ff9157",
            "aa46ecb96669c5f9a386286fdcf53bb2bb458cfbad44f53c0c2cbf762d5a8b8f"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain5-amazon-sagemaker-built-in-algorithms/nlp-/-text:-embeddings,-classification,-seq2seq,-topic-modeling",
          "label": "NLP / text: embeddings, classification, seq2seq, topic modeling",
          "description": "",
          "questionIds": [
            "5da5f7de76c5252a68ca645218797c39105a55d8c952120fd5de87bb1d602067",
            "1f15f604239d28a333f484b678439c078cf4ecde278aaf4eaaedc8b1697fe4e1",
            "876971f1b0b306146b8f3ab3491493cb663815f2bb364ae6fea9488d41dc5f9b",
            "d54da27070b53d8c4b5a1be4c04d2e8b3b7436f5166f000e99281263532b3889",
            "000c0cd046613b11e9917ddbdaefc2d3737c7b5c720e90f55dbe241dc3e2aa53"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain5-amazon-sagemaker-built-in-algorithms/computer-vision:-image-classification,-object-detection,-semantic-segmentation",
          "label": "Computer vision: image classification, object detection, semantic segmentation",
          "description": "",
          "questionIds": [
            "af5dfa1b3648002aef01d8f42a07940d4bf6f13d98d921eb48528dacabe9ff91",
            "12167a4b7e7bb67059c99e80adf1815a6cd2e35c1289f6ea5e51cacd0a14874e",
            "39661fd28b45def760ae4f5aabebb2f475115fdd3962d69180d55b1d0fd0b184",
            "7a34f3c3ddb717c6910719e690cb4188d14caa3f5f7698a94baa7af31ad83616",
            "00222b3e551c2e401fe93fe8775b529b973c5bb6f97a3f8feba76c7bfede5dde"
          ],
          "children": []
        },
        {
          "type": "subtopic",
          "id": "domain5-amazon-sagemaker-built-in-algorithms/unsupervised-+-anomaly-+-dimensionality-reduction",
          "label": "Unsupervised + anomaly + dimensionality reduction",
          "description": "",
          "questionIds": [
            "8c2135a4675a90383fb3ffd1ad998b6ae95feb7afbff91e32d4f944cd120e654",
            "1a41ec048551eb5f58950506df4758b822dda64851713fae538b88af0e1badea",
            "e81527624e4d6eaf53ccee6fe2f211037646e556ef96844f9cdf45108024891c",
            "f395da31ad6f2d3d50c4f91e5a5cfe89f6956f8f5f3c7389c71df661c4c4bfc2"
          ],
          "children": []
        }
      ]
    }
  ],
  "questions": [
    {
      "id": "8d67b7868ed99f2129f09953830c3a067c3156d8bc47c46a2acef025bf127c28",
      "question": "You need to store 5 TB of semi-structured clickstream events for ML training, and you expect frequent reprocessing and downstream analytics. Which storage choice best fits cost + scalability?",
      "tags": [
        "s3",
        "datalake",
        "storage"
      ],
      "answers": [
        {
          "text": "S3 (data lake) using columnar formats and partitions",
          "explanation": "S3 scales cheaply, supports reprocessing, and works well with analytics/ML pipelines.",
          "category": "correct"
        },
        {
          "text": "Put the events in S3 as a data lake and store them in a query-friendly layout (e.g., partitioned columnar files)",
          "explanation": "This is the same idea as the other correct option: S3 as the system of record with an efficient layout for analytics and training.",
          "category": "correct"
        },
        {
          "text": "EBS volumes attached to a single EC2 instance",
          "explanation": "It can work at small scale, but it doesn’t scale operationally or cost-effectively for multi‑TB pipelines.",
          "category": "partially_correct"
        },
        {
          "text": "Store in EFS shared storage",
          "explanation": "EFS can scale, but it’s typically not the best cost/performance choice for a large data lake compared with S3.",
          "category": "partially_correct"
        },
        {
          "text": "ElastiCache (Redis) as the primary store",
          "explanation": "Great for caching, not for durable, multi‑TB data lake storage.",
          "category": "incorrect"
        },
        {
          "text": "Store primarily in an SQS queue for later training",
          "explanation": "Queues aren’t intended as a long-term system of record for TB-scale datasets.",
          "category": "incorrect"
        },
        {
          "text": "Store the events in an email inbox",
          "explanation": "Email is not a storage system for structured ML datasets.",
          "category": "ridiculous"
        },
        {
          "text": "Print the clickstream and file it alphabetically",
          "explanation": "Physically impossible for ongoing ingestion and processing.",
          "category": "ridiculous"
        },
        {
          "text": "Upload them as Instagram stories for “distributed storage”",
          "explanation": "Ephemeral and not a data engineering solution.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data repositories for ML"
    },
    {
      "id": "8fee6f6b2c4730cf0b5546d4afa26726965907530a3d257614bdfef73a712161",
      "question": "A team wants low-latency key-value access to user profiles (features) during inference, with high read throughput and single-digit millisecond latency. Which repository is the best fit?",
      "tags": [
        "dynamodb",
        "inference",
        "latency"
      ],
      "answers": [
        {
          "text": "DynamoDB",
          "explanation": "Designed for low-latency key-value lookups at high throughput; common for online feature retrieval.",
          "category": "correct"
        },
        {
          "text": "Use a managed key-value database designed for millisecond reads at scale (DynamoDB)",
          "explanation": "Same substance: a managed KV store optimized for fast lookups during inference.",
          "category": "correct"
        },
        {
          "text": "S3 with a single large JSON file",
          "explanation": "Durable, but object retrieval isn’t optimized for millisecond key lookups inside huge objects.",
          "category": "partially_correct"
        },
        {
          "text": "Serve from S3 objects with aggressive caching",
          "explanation": "Caching can help, but S3 is not an ideal primary KV store for consistent single-digit-ms lookups.",
          "category": "partially_correct"
        },
        {
          "text": "Athena",
          "explanation": "Athena is a query engine for S3, not a low-latency key-value database.",
          "category": "incorrect"
        },
        {
          "text": "Use Athena to fetch a row per request",
          "explanation": "Query engines are far too slow and costly for online per-request inference reads.",
          "category": "incorrect"
        },
        {
          "text": "A spreadsheet opened in the browser",
          "explanation": "Not scalable for inference traffic.",
          "category": "ridiculous"
        },
        {
          "text": "A sticky note wall labeled “FEATURE STORE”",
          "explanation": "Not an API-backed repository.",
          "category": "ridiculous"
        },
        {
          "text": "Have an intern Ctrl+F the profile on demand",
          "explanation": "Latency measured in minutes, not milliseconds.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data repositories for ML"
    },
    {
      "id": "3f5f0212f52bc521df634d50921b308da858b987e2a60152f4836e5dafd6c3b4",
      "question": "You have structured transactional data already in a relational database and want to preserve constraints and SQL joins for feature generation. Which repository best matches that need?",
      "tags": [
        "rds",
        "sql",
        "features"
      ],
      "answers": [
        {
          "text": "RDS (relational database)",
          "explanation": "Supports schema constraints and SQL joins directly.",
          "category": "correct"
        },
        {
          "text": "Keep it in a relational database service where joins and constraints are first-class (RDS)",
          "explanation": "Same idea: relational storage preserves constraints and enables joins for feature generation.",
          "category": "correct"
        },
        {
          "text": "S3 data lake + Athena",
          "explanation": "You can query and join, but you lose native transactional constraints and OLTP semantics.",
          "category": "partially_correct"
        },
        {
          "text": "Export to S3 and query with Athena",
          "explanation": "Possible for analytics, but not equivalent to transactional constraints and OLTP behavior.",
          "category": "partially_correct"
        },
        {
          "text": "DynamoDB for complex joins",
          "explanation": "Not designed for arbitrary joins; you model around access patterns instead.",
          "category": "incorrect"
        },
        {
          "text": "Use a document store and “simulate joins” client-side",
          "explanation": "Client-side joins don’t provide the same integrity, performance, or semantics as SQL joins.",
          "category": "incorrect"
        },
        {
          "text": "A folder of PNG screenshots of tables",
          "explanation": "Not queryable for joins/constraints.",
          "category": "ridiculous"
        },
        {
          "text": "A “join” performed by manually merging printouts",
          "explanation": "Not realistic or scalable.",
          "category": "ridiculous"
        },
        {
          "text": "Take photos of the ER diagram and train on the pixels",
          "explanation": "Not feature generation.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data repositories for ML"
    },
    {
      "id": "3db8e4a9211504e753aaeeefc12613277a3ca0464f7aafd6b42a2050646dbf9e",
      "question": "In an S3-based data lake for ML, which design choice most directly improves query performance and cost in Athena?",
      "tags": [
        "athena",
        "partitioning",
        "s3"
      ],
      "answers": [
        {
          "text": "Partition data by common filter keys (e.g., date/region) and store as columnar (e.g., Parquet)",
          "explanation": "Reduces scanned data and enables pruning, improving speed and cost.",
          "category": "correct"
        },
        {
          "text": "Partition by common WHERE filters (like date/region) and use columnar storage to scan less data",
          "explanation": "Same substance: pruning reduces bytes scanned, improving performance and cost.",
          "category": "correct"
        },
        {
          "text": "Use larger files instead of many tiny files",
          "explanation": "Helps reduce overhead, but partitioning/columnar layout usually yields bigger gains.",
          "category": "partially_correct"
        },
        {
          "text": "Merge tiny files into fewer larger files",
          "explanation": "Often helpful, but secondary to partitioning and columnar formats for Athena.",
          "category": "partially_correct"
        },
        {
          "text": "Randomize filenames to improve “fairness”",
          "explanation": "Filename randomness doesn’t affect Athena scan cost/performance.",
          "category": "incorrect"
        },
        {
          "text": "Turn on versioning to speed up queries",
          "explanation": "Versioning helps recovery/auditing, not query pruning or scan cost.",
          "category": "incorrect"
        },
        {
          "text": "Rename every object “athena_fast.parquet”",
          "explanation": "Names don’t create partition pruning.",
          "category": "ridiculous"
        },
        {
          "text": "Put the data in a ZIP and hope SQL works anyway",
          "explanation": "Compressed archives don’t make Athena magically query inside efficiently.",
          "category": "ridiculous"
        },
        {
          "text": "Put “FAST” in every filename",
          "explanation": "Not a performance feature.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data repositories for ML"
    },
    {
      "id": "732c5f64bb2338e1a2087e029c45dfab6c17c60971768ad2db54395d3cd70c64",
      "question": "A dataset is updated daily and consumed by training jobs that filter by date ranges and customer region. What S3 organization pattern best supports this access pattern?",
      "tags": [
        "partitioning",
        "s3",
        "layout"
      ],
      "answers": [
        {
          "text": "Hive-style partitions like s3://.../date=YYYY-MM-DD/region=us-west-2/...",
          "explanation": "Matches common filters and enables partition pruning.",
          "category": "correct"
        },
        {
          "text": "Use partitioned prefixes keyed by date and region (e.g., date=.../region=.../)",
          "explanation": "Same idea: align folder structure with common filters for efficient scanning.",
          "category": "correct"
        },
        {
          "text": "One folder per day only",
          "explanation": "Helps date filtering but region filtering still scans extra data.",
          "category": "partially_correct"
        },
        {
          "text": "Partition by date only",
          "explanation": "Improves time filtering but not regional pruning.",
          "category": "partially_correct"
        },
        {
          "text": "One huge folder with all data forever",
          "explanation": "Forces large scans and brittle pipelines.",
          "category": "incorrect"
        },
        {
          "text": "Store everything in a single flat prefix forever",
          "explanation": "Eliminates pruning and increases query cost.",
          "category": "incorrect"
        },
        {
          "text": "Store it by “vibes” and “mood” folders",
          "explanation": "Not a meaningful access pattern.",
          "category": "ridiculous"
        },
        {
          "text": "Organize by file size (tiny/medium/large)",
          "explanation": "Unrelated to typical training/query filters.",
          "category": "ridiculous"
        },
        {
          "text": "Organize by the first letter of the uploader’s name",
          "explanation": "Doesn’t match access patterns.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data repositories for ML"
    },
    {
      "id": "5213e585c696e2d04e6ea6656e12116e2f933d928780e9f55cd5afa458cd92a7",
      "question": "For large-scale ML training data in S3, which file format generally provides the best compression + column pruning behavior for analytics-style access?",
      "tags": [
        "parquet",
        "format",
        "compression"
      ],
      "answers": [
        {
          "text": "Parquet",
          "explanation": "Columnar storage enables column pruning and strong compression.",
          "category": "correct"
        },
        {
          "text": "Use a columnar format that supports pruning and compression well (Parquet)",
          "explanation": "Same substance: columnar + compression reduces scan cost and speeds processing.",
          "category": "correct"
        },
        {
          "text": "ORC",
          "explanation": "Also columnar and often excellent, but Parquet is broadly adopted across many stacks.",
          "category": "partially_correct"
        },
        {
          "text": "Use ORC",
          "explanation": "Another good columnar option; depends on ecosystem compatibility.",
          "category": "partially_correct"
        },
        {
          "text": "Raw CSV",
          "explanation": "Row-based, larger scans, and slower parsing than columnar formats.",
          "category": "incorrect"
        },
        {
          "text": "Use JSON Lines for maximum performance",
          "explanation": "Convenient but typically slower and larger scans than columnar storage.",
          "category": "incorrect"
        },
        {
          "text": "A Word document table",
          "explanation": "Not designed for analytics engines.",
          "category": "ridiculous"
        },
        {
          "text": "Morse code",
          "explanation": "Not a practical analytics format.",
          "category": "ridiculous"
        },
        {
          "text": "Save as a PowerPoint deck",
          "explanation": "Not queryable as a dataset.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data repositories for ML"
    },
    {
      "id": "24f572fa39fd37482f2daec86f7d704fd347216d8d9ffb6f0028db9b57eb0a25",
      "question": "You need to ingest IoT sensor data continuously with near-real-time delivery to S3 for downstream processing. Which AWS service is most appropriate?",
      "tags": [
        "kinesis",
        "streaming",
        "s3"
      ],
      "answers": [
        {
          "text": "Kinesis Data Firehose → S3",
          "explanation": "Managed ingestion with buffering and direct delivery to S3.",
          "category": "correct"
        },
        {
          "text": "Use a managed delivery stream that buffers and writes directly to S3 (Kinesis Data Firehose)",
          "explanation": "Same approach: a delivery service built to land streaming data into S3.",
          "category": "correct"
        },
        {
          "text": "Kinesis Data Streams → custom consumer writing to S3",
          "explanation": "Works, but requires more operational effort than Firehose for direct S3 delivery.",
          "category": "partially_correct"
        },
        {
          "text": "Use Kinesis Streams plus a consumer to write to S3",
          "explanation": "Viable but adds code and consumer management.",
          "category": "partially_correct"
        },
        {
          "text": "Athena",
          "explanation": "Athena queries data; it doesn’t ingest streams.",
          "category": "incorrect"
        },
        {
          "text": "Use CloudFront to “ingest” data",
          "explanation": "CloudFront is a CDN, not a data ingestion pipeline.",
          "category": "incorrect"
        },
        {
          "text": "Have the sensors email attachments to S3",
          "explanation": "Not a scalable ingestion method.",
          "category": "ridiculous"
        },
        {
          "text": "Ask each sensor to run aws s3 cp once a day",
          "explanation": "Not near-real-time and brittle.",
          "category": "ridiculous"
        },
        {
          "text": "Have devices fax the readings to AWS",
          "explanation": "Not an ingestion service.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data ingestion solutions"
    },
    {
      "id": "027bcfbaa1eb0c174287e393d37d7b3e500c7e38b52c0607117d1b910819d292",
      "question": "A pipeline must capture streaming events, allow multiple consumers, and support replay from a point in time after a consumer failure. Which ingestion service best matches this?",
      "tags": [
        "kinesis",
        "replay",
        "consumers"
      ],
      "answers": [
        {
          "text": "Kinesis Data Streams",
          "explanation": "Supports multiple consumers and replay/retention-based reprocessing.",
          "category": "correct"
        },
        {
          "text": "Use a stream with retention and replay semantics for multiple consumers (Kinesis Data Streams)",
          "explanation": "Same substance: a replayable log with consumer fan-out.",
          "category": "correct"
        },
        {
          "text": "Firehose",
          "explanation": "Great for delivery, but replay/multi-consumer isn’t its main model.",
          "category": "partially_correct"
        },
        {
          "text": "Use Firehose to land data in S3 and reprocess from S3",
          "explanation": "Replay is possible by rereading S3, but it’s not the same as stream-style retention replay.",
          "category": "partially_correct"
        },
        {
          "text": "S3 event notifications alone",
          "explanation": "Notifications don’t provide replayable stream semantics.",
          "category": "incorrect"
        },
        {
          "text": "Use SNS topics only",
          "explanation": "Pub/sub doesn’t give you a durable replay log by default.",
          "category": "incorrect"
        },
        {
          "text": "A shared group chat",
          "explanation": "Not durable replayable streaming.",
          "category": "ridiculous"
        },
        {
          "text": "A single clipboard passed around",
          "explanation": "No durability or concurrency control.",
          "category": "ridiculous"
        },
        {
          "text": "Write each event onto a balloon and hope it comes back",
          "explanation": "Replay is not guaranteed.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data ingestion solutions"
    },
    {
      "id": "d943c6db0a22d45874231b549e508e34afb724baad9a804b7b294a52a624bc36",
      "question": "You need to move data from RDS to S3 on a schedule with minimal custom code, to support a SageMaker training pipeline. Which service is best aligned?",
      "tags": [
        "batch",
        "rds",
        "pipeline"
      ],
      "answers": [
        {
          "text": "AWS Data Pipeline (scheduled movement)",
          "explanation": "Designed for orchestrated data movement with scheduling.",
          "category": "correct"
        },
        {
          "text": "Use a managed scheduled data movement/orchestration service (AWS Data Pipeline)",
          "explanation": "Same substance: scheduled orchestration for data movement with minimal custom code.",
          "category": "correct"
        },
        {
          "text": "Cron on EC2 running export scripts",
          "explanation": "Works, but adds servers and ops toil.",
          "category": "partially_correct"
        },
        {
          "text": "Use an EC2 cron job exporting snapshots",
          "explanation": "Possible, but not “minimal ops” compared to managed orchestration.",
          "category": "partially_correct"
        },
        {
          "text": "DynamoDB Streams",
          "explanation": "Streams changes from DynamoDB, not scheduled RDS-to-S3 exports.",
          "category": "incorrect"
        },
        {
          "text": "Use Route 53 health checks to trigger exports",
          "explanation": "Unrelated to data movement scheduling.",
          "category": "incorrect"
        },
        {
          "text": "Copy/paste rows from the console daily",
          "explanation": "Not scalable or reliable.",
          "category": "ridiculous"
        },
        {
          "text": "Screenshot the RDS dashboard and upload it",
          "explanation": "Not usable data.",
          "category": "ridiculous"
        },
        {
          "text": "Email the database to S3 every night",
          "explanation": "Not a data pipeline.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data ingestion solutions"
    },
    {
      "id": "ec655dcacca35a72499d840b22d2d7772f41f55a99212390ce77d893ef7baea6",
      "question": "Your ingestion system must handle bursts of traffic and automatically scale without managing servers, and deliver events directly into S3 with buffering. Which solution best fits?",
      "tags": [
        "firehose",
        "serverless",
        "buffering"
      ],
      "answers": [
        {
          "text": "Kinesis Data Firehose",
          "explanation": "Managed scaling with buffering and direct S3 delivery.",
          "category": "correct"
        },
        {
          "text": "Use Firehose for autoscaled ingestion with buffering and S3 delivery",
          "explanation": "Same idea: a managed delivery service that smooths bursts and lands data in S3.",
          "category": "correct"
        },
        {
          "text": "Kinesis Data Streams + Lambda + S3",
          "explanation": "Viable, but more moving parts and tuning than Firehose.",
          "category": "partially_correct"
        },
        {
          "text": "Streams + Lambda + S3",
          "explanation": "Works, but requires consumer scaling and batching logic.",
          "category": "partially_correct"
        },
        {
          "text": "RDS as an ingestion buffer",
          "explanation": "Relational DBs are not ideal for bursty event buffering at scale.",
          "category": "incorrect"
        },
        {
          "text": "Use a single t2.micro to accept all events",
          "explanation": "Won’t scale reliably under bursts.",
          "category": "incorrect"
        },
        {
          "text": "Write events to a local laptop and “sync later”",
          "explanation": "No strong durability or scaling.",
          "category": "ridiculous"
        },
        {
          "text": "Use a public pastebin",
          "explanation": "Not secure or reliable.",
          "category": "ridiculous"
        },
        {
          "text": "Ask customers to retry until it works",
          "explanation": "Terrible UX and still risks loss.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data ingestion solutions"
    },
    {
      "id": "781b5793f8aae29d1bd08b92e89b6cc38fd5cbfe58b884276c60408eb194860d",
      "question": "A batch ingestion workflow loads 200 GB of CSV files nightly into S3. Which improvement most directly reduces downstream ETL time and cost?",
      "tags": [
        "batch",
        "csv",
        "optimization"
      ],
      "answers": [
        {
          "text": "Convert the data to partitioned Parquet",
          "explanation": "Columnar format + partitioning speeds downstream processing and reduces scan cost.",
          "category": "correct"
        },
        {
          "text": "Rewrite the dataset into partitioned columnar files (e.g., Parquet) before heavy processing",
          "explanation": "Same substance: reduce parsing overhead and enable pruning/compression.",
          "category": "correct"
        },
        {
          "text": "Compress the CSV files (gzip)",
          "explanation": "Helps transfer and storage, but still slower parsing and limited pruning compared with columnar.",
          "category": "partially_correct"
        },
        {
          "text": "Compress CSVs",
          "explanation": "Useful, but doesn’t address row-based scan inefficiency.",
          "category": "partially_correct"
        },
        {
          "text": "Split into millions of tiny CSV files",
          "explanation": "Increases overhead and slows distributed processing.",
          "category": "incorrect"
        },
        {
          "text": "Convert CSV to XML for “structure”",
          "explanation": "Usually worse for analytics/ETL performance.",
          "category": "incorrect"
        },
        {
          "text": "Manually retype it into a database",
          "explanation": "Not feasible at this scale.",
          "category": "ridiculous"
        },
        {
          "text": "Store it as a single emoji string",
          "explanation": "Not machine-readable.",
          "category": "ridiculous"
        },
        {
          "text": "Store as an audio file and “listen” for rows",
          "explanation": "Not usable for ETL.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data ingestion solutions"
    },
    {
      "id": "dcb3b7feb307632e8fe7dc588b0fc68e2031ebb2809362cef2c65681670e9a59",
      "question": "You’re ingesting records that may arrive late or out-of-order. Which strategy best supports correctness in downstream processing?",
      "tags": [
        "streaming",
        "watermark",
        "ordering"
      ],
      "answers": [
        {
          "text": "Use event time with watermarking/windowing to handle late data",
          "explanation": "Preserves correctness while allowing bounded lateness.",
          "category": "correct"
        },
        {
          "text": "Process using event-time semantics with bounded lateness (watermarks/windows) so late data is incorporated correctly",
          "explanation": "Same substance: event-time + watermarks is the standard correctness strategy for late/out-of-order data.",
          "category": "correct"
        },
        {
          "text": "Sort everything at the end of the day",
          "explanation": "Can work for batch, but increases latency and loses near-real-time correctness.",
          "category": "partially_correct"
        },
        {
          "text": "Hold all records for a long delay and then process",
          "explanation": "Potentially correct, but adds significant latency.",
          "category": "partially_correct"
        },
        {
          "text": "Ignore late events",
          "explanation": "Biases aggregates and training data.",
          "category": "incorrect"
        },
        {
          "text": "Treat arrival time as truth and overwrite anything “late”",
          "explanation": "Breaks correctness when event time matters.",
          "category": "incorrect"
        },
        {
          "text": "Time travel to deliver events in order",
          "explanation": "Not a real strategy.",
          "category": "ridiculous"
        },
        {
          "text": "Delete out-of-order records to “keep it tidy”",
          "explanation": "Destructive and incorrect.",
          "category": "ridiculous"
        },
        {
          "text": "Send a stern letter to late packets",
          "explanation": "Ineffective.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data ingestion solutions"
    },
    {
      "id": "32dbadab788a2a59615c3d4522db1734683e019f68258bd2b87ceb90261aac1c",
      "question": "A business requires that no events are lost during ingestion, even if a downstream consumer is unavailable temporarily. Which ingestion pattern best supports this requirement?",
      "tags": [
        "durability",
        "buffering",
        "decoupling"
      ],
      "answers": [
        {
          "text": "Durable buffering/decoupling so producers don’t depend on consumers",
          "explanation": "A durable buffer lets events persist through consumer downtime.",
          "category": "correct"
        },
        {
          "text": "Decouple producers from consumers with durable buffering/retention so events persist through consumer downtime",
          "explanation": "Same substance: durability + decoupling prevents loss during outages.",
          "category": "correct"
        },
        {
          "text": "Retry in the producer only",
          "explanation": "Helps, but producer failures can still cause loss without durable buffering.",
          "category": "partially_correct"
        },
        {
          "text": "Use client retries with exponential backoff",
          "explanation": "Useful, but weaker than a durable buffer if producers fail or retry budgets are exceeded.",
          "category": "partially_correct"
        },
        {
          "text": "Send directly to the consumer over HTTP with no retries",
          "explanation": "Fragile; drops on outages.",
          "category": "incorrect"
        },
        {
          "text": "Drop events when the consumer is slow to keep latency low",
          "explanation": "Violates the “no loss” requirement.",
          "category": "incorrect"
        },
        {
          "text": "Yell the events loudly until someone hears them",
          "explanation": "Not durable.",
          "category": "ridiculous"
        },
        {
          "text": "Write events on a napkin and mail them",
          "explanation": "Not reliable ingestion.",
          "category": "ridiculous"
        },
        {
          "text": "Write events in disappearing ink",
          "explanation": "Guaranteed loss.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data ingestion solutions"
    },
    {
      "id": "95d1b41b7f3c433f0fb90d2050c74fe4d4ea921a78c176e90fa6516ea0d2eb06",
      "question": "Your ML team needs to ingest data from multiple sources (RDS exports, S3 drops, streaming events) and keep lineage of when data arrived. Which approach best supports traceability?",
      "tags": [
        "lineage",
        "metadata",
        "ingestion"
      ],
      "answers": [
        {
          "text": "Record ingestion metadata (source, ingest timestamp, version/batch id) alongside the data",
          "explanation": "Enables traceability, auditing, and debugging.",
          "category": "correct"
        },
        {
          "text": "Attach ingestion metadata (source, ingest timestamp, batch/version id) as part of the dataset/records",
          "explanation": "Same substance: explicit metadata makes lineage and audits possible.",
          "category": "correct"
        },
        {
          "text": "Rely only on S3 LastModified timestamps",
          "explanation": "Helpful, but limited and misleading for re-uploads/backfills.",
          "category": "partially_correct"
        },
        {
          "text": "Rely on folder naming conventions alone",
          "explanation": "Useful, but fragile and easy to break.",
          "category": "partially_correct"
        },
        {
          "text": "Assume file names are always accurate",
          "explanation": "Not reliable governance.",
          "category": "incorrect"
        },
        {
          "text": "Assume the newest file is always the correct one",
          "explanation": "Breaks with backfills and retries.",
          "category": "incorrect"
        },
        {
          "text": "Trust “someone will remember”",
          "explanation": "Institutional memory fails.",
          "category": "ridiculous"
        },
        {
          "text": "Write lineage on a whiteboard",
          "explanation": "Not durable or searchable.",
          "category": "ridiculous"
        },
        {
          "text": "Use astrology charts to infer data freshness",
          "explanation": "Not traceability.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data ingestion solutions"
    },
    {
      "id": "0e0a16eaf13713e9b0f83e0df4698093dc1be94d02c1e7048f2ad32f3a4597ce",
      "question": "You need to perform large-scale ETL on TB-scale datasets in S3 using Spark, producing partitioned Parquet outputs for training. Which service is the best fit?",
      "tags": [
        "glue",
        "spark",
        "etl"
      ],
      "answers": [
        {
          "text": "AWS Glue (Spark ETL)",
          "explanation": "Managed Spark for large-scale transformations and writing curated outputs to S3.",
          "category": "correct"
        },
        {
          "text": "Run the ETL on a managed Spark service (AWS Glue) and output curated, partitioned Parquet to S3",
          "explanation": "Same substance: managed Spark ETL is the right fit for TB-scale transformations.",
          "category": "correct"
        },
        {
          "text": "EMR Spark",
          "explanation": "Powerful but more cluster management than Glue for many ETL workloads.",
          "category": "partially_correct"
        },
        {
          "text": "Run Spark on EMR",
          "explanation": "Works well, but typically more operational overhead.",
          "category": "partially_correct"
        },
        {
          "text": "Athena CTAS as the only tool",
          "explanation": "Can help for some transforms, but not a full replacement for complex Spark ETL.",
          "category": "incorrect"
        },
        {
          "text": "Use CloudWatch Logs Insights as your ETL engine",
          "explanation": "Not designed for TB-scale dataset transformations.",
          "category": "incorrect"
        },
        {
          "text": "Open the dataset in Excel",
          "explanation": "Not feasible at TB scale.",
          "category": "ridiculous"
        },
        {
          "text": "Use MS Paint to “transform” the data",
          "explanation": "Wrong tool.",
          "category": "ridiculous"
        },
        {
          "text": "Use a calculator app to transform terabytes",
          "explanation": "Also wrong tool.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data transformation solutions (ETL/ELT)"
    },
    {
      "id": "e97113c078d3da93083dcfdf282984789df1e1888c5a11ee483ac25d7dab8671",
      "question": "You need to apply lightweight transformations (e.g., redact PII fields, add a derived column) to streaming data before it lands in S3. Which approach best fits?",
      "tags": [
        "lambda",
        "streaming",
        "pii"
      ],
      "answers": [
        {
          "text": "Lambda-based transformation in the ingestion path",
          "explanation": "Good for small/quick transforms like redaction/enrichment before landing.",
          "category": "correct"
        },
        {
          "text": "Apply an inline serverless transform (e.g., Lambda in the ingestion path) to redact/enrich before writing to S3",
          "explanation": "Same substance: transform before landing to minimize exposure of raw sensitive data.",
          "category": "correct"
        },
        {
          "text": "Transform after landing (batch job later)",
          "explanation": "Works, but sensitive fields still land unredacted.",
          "category": "partially_correct"
        },
        {
          "text": "Land raw data then run a batch cleanup",
          "explanation": "Possible, but increases risk if raw data is sensitive.",
          "category": "partially_correct"
        },
        {
          "text": "Do nothing; rely on “security by obscurity”",
          "explanation": "Not a control.",
          "category": "incorrect"
        },
        {
          "text": "Base64-encode PII and call it “redacted”",
          "explanation": "Base64 is not redaction or encryption.",
          "category": "incorrect"
        },
        {
          "text": "Ask users to redact their own PII",
          "explanation": "Not reliable or enforceable.",
          "category": "ridiculous"
        },
        {
          "text": "Hope PII “evaporates”",
          "explanation": "It won’t.",
          "category": "ridiculous"
        },
        {
          "text": "Rename the “ssn” column to “not_ssn”",
          "explanation": "Renaming does not remove sensitive content.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data transformation solutions (ETL/ELT)"
    },
    {
      "id": "bc1f0fcc015e2d6d20e4f99d9400d379bc606bca6c3b48b6f8374b43622b7900",
      "question": "You need a transformation job that joins clickstream events with reference data, handles schema evolution, and outputs a curated dataset for feature engineering. Which tool is most appropriate?",
      "tags": [
        "glue",
        "joins",
        "schema"
      ],
      "answers": [
        {
          "text": "AWS Glue ETL (with catalog integration)",
          "explanation": "Designed for joins/transformations at scale and works well with evolving schemas and curated outputs.",
          "category": "correct"
        },
        {
          "text": "Use a managed ETL framework that supports joins and evolving schemas (Glue ETL + catalog integration)",
          "explanation": "Same substance: managed ETL + metadata support is ideal for curation with joins and schema evolution.",
          "category": "correct"
        },
        {
          "text": "Custom Spark on EC2",
          "explanation": "Can do it, but higher operational overhead than managed ETL.",
          "category": "partially_correct"
        },
        {
          "text": "Write custom Python scripts for joins",
          "explanation": "Possible for small data, but hard to scale and manage schema evolution reliably.",
          "category": "partially_correct"
        },
        {
          "text": "DynamoDB Streams",
          "explanation": "Not an ETL join engine.",
          "category": "incorrect"
        },
        {
          "text": "Use S3 Select to do large multi-table joins",
          "explanation": "Not intended for large joins across datasets.",
          "category": "incorrect"
        },
        {
          "text": "Join by matching rows that “look similar” by eye",
          "explanation": "Not scalable or correct.",
          "category": "ridiculous"
        },
        {
          "text": "Ask a committee to vote on each join key",
          "explanation": "Not practical.",
          "category": "ridiculous"
        },
        {
          "text": "Let a random number generator assign join keys",
          "explanation": "Incorrect by design.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data transformation solutions (ETL/ELT)"
    },
    {
      "id": "ba3a65a408ed03c67f80ab3bce8edfb6734a480963809a7ffedc3f8e6403a701",
      "question": "Which statement best distinguishes ETL vs ELT in an ML data pipeline context?",
      "tags": [
        "etl",
        "elt",
        "pipeline"
      ],
      "answers": [
        {
          "text": "ETL transforms before loading into the target; ELT loads raw first then transforms in the target/processing layer",
          "explanation": "The key distinction is when/where transforms happen.",
          "category": "correct"
        },
        {
          "text": "ETL transforms before loading into the target; ELT loads first, then transforms where the data lives/compute runs",
          "explanation": "Same definition: timing/location of transformation distinguishes ETL vs ELT.",
          "category": "correct"
        },
        {
          "text": "ETL is “old” and ELT is “new”",
          "explanation": "Sometimes true historically, but not the definition.",
          "category": "partially_correct"
        },
        {
          "text": "ETL is for batch and ELT is for streaming",
          "explanation": "Not the definition; either can be batch or streaming depending on architecture.",
          "category": "partially_correct"
        },
        {
          "text": "ETL and ELT are identical terms",
          "explanation": "They differ in where/when transformation occurs.",
          "category": "incorrect"
        },
        {
          "text": "ELT means you don’t need transformations",
          "explanation": "False—transformations still happen, just later/elsewhere.",
          "category": "incorrect"
        },
        {
          "text": "ETL is “Extra Tasty Lunch”",
          "explanation": "Delicious, but irrelevant.",
          "category": "ridiculous"
        },
        {
          "text": "ELT is “Extremely Loud Thunder”",
          "explanation": "Wrong acronym.",
          "category": "ridiculous"
        },
        {
          "text": "ETL = “Eager To Learn”",
          "explanation": "Nice attitude, wrong definition.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data transformation solutions (ETL/ELT)"
    },
    {
      "id": "04443a569b0665f264c3ac26ae35b52051c9b80871ec68bf0667e6037ced5336",
      "question": "A transformation step produces training data, but the model performance drops unexpectedly. Which data engineering check is most likely to reveal a root cause related to transformation?",
      "tags": [
        "leakage",
        "validation",
        "drift"
      ],
      "answers": [
        {
          "text": "Check for data leakage (especially label leakage) introduced by transformations",
          "explanation": "Leakage is a common silent cause of surprising metric shifts.",
          "category": "correct"
        },
        {
          "text": "Audit transformations for leakage or train/serve skew introduced during feature building",
          "explanation": "Same idea: transformation bugs often cause leakage or skew that changes performance.",
          "category": "correct"
        },
        {
          "text": "Increase model size",
          "explanation": "May change performance, but does not diagnose transformation correctness.",
          "category": "partially_correct"
        },
        {
          "text": "Try a different model family",
          "explanation": "Could help, but doesn’t identify the transformation problem.",
          "category": "partially_correct"
        },
        {
          "text": "Disable validation",
          "explanation": "Hides the issue rather than diagnosing it.",
          "category": "incorrect"
        },
        {
          "text": "Remove the test set to “stabilize” metrics",
          "explanation": "Eliminates detection of issues and risks overfitting.",
          "category": "incorrect"
        },
        {
          "text": "Blame the GPU",
          "explanation": "Data issues won’t be fixed by blaming hardware.",
          "category": "ridiculous"
        },
        {
          "text": "Shake the dataset to “mix it better”",
          "explanation": "Not a real diagnostic.",
          "category": "ridiculous"
        },
        {
          "text": "Blame Mercury retrograde",
          "explanation": "Not evidence-based.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data transformation solutions (ETL/ELT)"
    },
    {
      "id": "ae66e4ab05528d301d8fc624a03cf298251ca58494f5deb2549ea7020244b967",
      "question": "You must transform data as it passes through a streaming pipeline, and you need windowed aggregations (e.g., rolling 5-minute counts). Which service capability best supports this?",
      "tags": [
        "windows",
        "streaming",
        "aggregation"
      ],
      "answers": [
        {
          "text": "Streaming windowing/aggregation (time-windowed computations)",
          "explanation": "Windowed processing is required to compute rolling aggregates correctly.",
          "category": "correct"
        },
        {
          "text": "Use stream processing that supports time windows and aggregations (windowed computations)",
          "explanation": "Same substance: time-window primitives are needed for rolling counts and late data handling.",
          "category": "correct"
        },
        {
          "text": "Batch aggregation every 5 minutes",
          "explanation": "Can approximate, but may mishandle late/out-of-order events and true window semantics.",
          "category": "partially_correct"
        },
        {
          "text": "Compute counts in periodic micro-batches",
          "explanation": "Approximates streaming, but not equivalent to event-time window processing.",
          "category": "partially_correct"
        },
        {
          "text": "Only store raw events and never aggregate",
          "explanation": "Doesn’t meet the requirement for rolling aggregates.",
          "category": "incorrect"
        },
        {
          "text": "Aggregate only once at the end of the month",
          "explanation": "Too infrequent and not “rolling 5-minute”.",
          "category": "incorrect"
        },
        {
          "text": "Use a stopwatch and tally marks",
          "explanation": "Not scalable.",
          "category": "ridiculous"
        },
        {
          "text": "Ask users to count their own events",
          "explanation": "Not reliable.",
          "category": "ridiculous"
        },
        {
          "text": "Count with fingers and toes",
          "explanation": "Limited capacity.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data transformation solutions (ETL/ELT)"
    },
    {
      "id": "3d96753a85d420bc97ee4fb00bafda0e4ea772baa13a3659ef4c7a3224702aae",
      "question": "You have mixed structured and unstructured data in S3 and need to build a unified training dataset with metadata and consistent schemas. What is the most practical first step?",
      "tags": [
        "schema",
        "catalog",
        "curation"
      ],
      "answers": [
        {
          "text": "Define a canonical schema and metadata strategy, then curate raw inputs into standardized datasets",
          "explanation": "You need schema/metadata decisions before unification.",
          "category": "correct"
        },
        {
          "text": "Establish a canonical schema/metadata model and curate raw inputs into standardized, documented training tables/files",
          "explanation": "Same substance: standardization + metadata is the prerequisite for unifying disparate sources.",
          "category": "correct"
        },
        {
          "text": "Dump everything into one folder",
          "explanation": "Centralizes location but doesn’t unify schemas or metadata.",
          "category": "partially_correct"
        },
        {
          "text": "Sample and train separately per source",
          "explanation": "May work short-term, but doesn’t create a unified, governed dataset.",
          "category": "partially_correct"
        },
        {
          "text": "Train directly on raw bytes without documentation",
          "explanation": "Rarely workable and not maintainable.",
          "category": "incorrect"
        },
        {
          "text": "Ignore metadata and hope downstream users “figure it out”",
          "explanation": "Leads to brittle pipelines and errors.",
          "category": "incorrect"
        },
        {
          "text": "Rename files until they “feel consistent”",
          "explanation": "Not a governance approach.",
          "category": "ridiculous"
        },
        {
          "text": "Ask the model to guess the schema",
          "explanation": "Not a reliable pipeline.",
          "category": "ridiculous"
        },
        {
          "text": "Name everything final_final_v7",
          "explanation": "Not schema or metadata.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data transformation solutions (ETL/ELT)"
    },
    {
      "id": "e1a97a48449d99d9a4ae187a5d2af11065be488516047cb26ff1e308c8f98790",
      "question": "A dataset includes high-cardinality categorical values that cause huge sparse encodings downstream. Which transformation approach best mitigates this before modeling?",
      "tags": [
        "cardinality",
        "encoding",
        "features"
      ],
      "answers": [
        {
          "text": "Use cardinality-aware encoding (e.g., hashing, top-K + “other”, frequency bucketing)",
          "explanation": "Reduces dimensionality while preserving useful signal.",
          "category": "correct"
        },
        {
          "text": "Use cardinality-aware encodings (hashing, top-K + other, frequency bucketing) to limit dimensionality",
          "explanation": "Same substance: reduce feature explosion while keeping information.",
          "category": "correct"
        },
        {
          "text": "Drop the categorical feature entirely",
          "explanation": "Sometimes acceptable but may remove valuable signal.",
          "category": "partially_correct"
        },
        {
          "text": "Drop rare categories only",
          "explanation": "Helps some, but may still leave high dimensionality without a broader strategy.",
          "category": "partially_correct"
        },
        {
          "text": "One-hot encode every unique value no matter what",
          "explanation": "Can explode dimensionality and harm training.",
          "category": "incorrect"
        },
        {
          "text": "Force one-hot encoding of every unique value",
          "explanation": "Same issue: uncontrolled blow-up in feature space.",
          "category": "incorrect"
        },
        {
          "text": "Encode categories by horoscope sign",
          "explanation": "Arbitrary and unrelated.",
          "category": "ridiculous"
        },
        {
          "text": "Use the length of the string as the category",
          "explanation": "Very lossy and arbitrary.",
          "category": "ridiculous"
        },
        {
          "text": "Map categories to Pokémon types",
          "explanation": "Fun, but wrong.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data transformation solutions (ETL/ELT)"
    },
    {
      "id": "703a4e34a231b8f945e2e9ce132401216341681097c36ed5c240ac67c6f33626",
      "question": "You want to run SQL queries over S3 data and avoid manual schema definitions across many datasets. Which combination best supports this?",
      "tags": [
        "glue",
        "catalog",
        "athena"
      ],
      "answers": [
        {
          "text": "Glue Data Catalog + Athena",
          "explanation": "Catalog stores schemas/partitions; Athena queries S3 using that metadata.",
          "category": "correct"
        },
        {
          "text": "Register schemas/partitions in a metadata catalog (Glue Data Catalog) and query via Athena",
          "explanation": "Same substance: catalog-managed metadata reduces manual schema definitions and enables SQL over S3.",
          "category": "correct"
        },
        {
          "text": "Athena with manual CREATE TABLE for everything",
          "explanation": "Works, but doesn’t avoid manual schema definitions.",
          "category": "partially_correct"
        },
        {
          "text": "Only Athena with hand-written DDL per dataset",
          "explanation": "Still manual schema work.",
          "category": "partially_correct"
        },
        {
          "text": "DynamoDB + Athena",
          "explanation": "Not the standard way to run SQL over S3 datasets with shared metadata.",
          "category": "incorrect"
        },
        {
          "text": "Use CloudTrail to infer schemas",
          "explanation": "CloudTrail logs API calls, not dataset schemas.",
          "category": "incorrect"
        },
        {
          "text": "Schema is optional; just vibe with it",
          "explanation": "SQL requires schemas.",
          "category": "ridiculous"
        },
        {
          "text": "Use Google Sheets as the catalog",
          "explanation": "Fragile and not integrated with Athena/S3.",
          "category": "ridiculous"
        },
        {
          "text": "Declare “schema free SQL”",
          "explanation": "Not how SQL works.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Cataloging, query, and access patterns"
    },
    {
      "id": "06eca2657838df7f072ef245cf043775fb023a92a3700ba529ec592a0bf4171a",
      "question": "A data scientist wants to quickly validate that yesterday’s ingest produced plausible distributions and row counts without loading everything into a notebook. What’s the best AWS-native approach?",
      "tags": [
        "athena",
        "profiling",
        "validation"
      ],
      "answers": [
        {
          "text": "Run Athena queries for counts and basic aggregates directly over the landed data",
          "explanation": "Fast sanity checks without pulling all data locally.",
          "category": "correct"
        },
        {
          "text": "Run quick sanity-check SQL (counts, min/max, basic checks) using Athena directly over the landed data",
          "explanation": "Same substance: use SQL over S3 data for validation.",
          "category": "correct"
        },
        {
          "text": "Download a sample locally and eyeball it",
          "explanation": "Helpful, but can miss large-scale issues.",
          "category": "partially_correct"
        },
        {
          "text": "Download a small sample and spot-check",
          "explanation": "Still incomplete vs full-scale checks.",
          "category": "partially_correct"
        },
        {
          "text": "Wait for the model to fail",
          "explanation": "Too late; validation should happen before training.",
          "category": "incorrect"
        },
        {
          "text": "Wait for downstream training to detect problems",
          "explanation": "Late detection increases cost and delays.",
          "category": "incorrect"
        },
        {
          "text": "Ask a psychic about row counts",
          "explanation": "Not reliable.",
          "category": "ridiculous"
        },
        {
          "text": "Measure the folder size with a ruler",
          "explanation": "Not how distributions/counts are measured.",
          "category": "ridiculous"
        },
        {
          "text": "Guess the row count based on file color",
          "explanation": "No.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Cataloging, query, and access patterns"
    },
    {
      "id": "be3ca8561dfa2f52680ef7d5ca02dd3124a45d16c93abe8978688bddab9fa454",
      "question": "Your training data includes sensitive fields. Which control most directly ensures that only authorized roles can decrypt the data at rest while keeping auditability?",
      "tags": [
        "kms",
        "iam",
        "encryption"
      ],
      "answers": [
        {
          "text": "Encrypt with KMS and restrict decrypt via IAM/KMS key policy",
          "explanation": "Controls who can decrypt and supports auditable access controls.",
          "category": "correct"
        },
        {
          "text": "Use KMS-backed encryption and restrict decrypt permissions via IAM/KMS key policy so only approved roles can decrypt",
          "explanation": "Same substance: KMS + IAM policies govern cryptographic access and auditing.",
          "category": "correct"
        },
        {
          "text": "S3 bucket policy only",
          "explanation": "Controls access to objects, but doesn’t manage decrypt permissions the same way as KMS policies.",
          "category": "partially_correct"
        },
        {
          "text": "Rely on bucket policies alone",
          "explanation": "Helpful for access, but not equivalent to controlling decryption rights via KMS key policy.",
          "category": "partially_correct"
        },
        {
          "text": "Base64 encode the data",
          "explanation": "Base64 is not encryption.",
          "category": "incorrect"
        },
        {
          "text": "Obfuscate with Base64 and hide the key in code",
          "explanation": "Not proper encryption or key management.",
          "category": "incorrect"
        },
        {
          "text": "Lock the data with a “password123” sticky note",
          "explanation": "Not secure.",
          "category": "ridiculous"
        },
        {
          "text": "Hide the S3 bucket name",
          "explanation": "Security through obscurity is not a control.",
          "category": "ridiculous"
        },
        {
          "text": "Hide the data under a folder named do_not_open",
          "explanation": "Not encryption or access control.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain1-data-engineering",
      "subtopic": "Data quality, governance, and security (DE-focused)"
    },
    {
      "id": "07637b950f386c074a70c3ca365e1519d7b2feed579b6ede296a0dd616c6e26b",
      "question": "What is the primary purpose of exploratory data analysis (EDA) in a machine learning workflow?",
      "tags": [
        "eda",
        "workflow"
      ],
      "answers": [
        {
          "text": "To understand the data (quality, distributions, relationships) so you can make informed preprocessing, feature, and modeling decisions.",
          "explanation": "EDA reduces risk by revealing issues and opportunities before training and deployment.",
          "category": "correct"
        },
        {
          "text": "To surface issues and patterns early—so you choose the right cleaning, features, and models before training.",
          "explanation": "EDA guides decisions and prevents costly mistakes later in the pipeline.",
          "category": "correct"
        },
        {
          "text": "To make charts and summarize the dataset.",
          "explanation": "Visualization and summaries are tools of EDA, but the goal is actionable insight.",
          "category": "partially_correct"
        },
        {
          "text": "To get a quick feel for what the dataset looks like.",
          "explanation": "This is part of EDA, but EDA also drives concrete modeling and preprocessing choices.",
          "category": "partially_correct"
        },
        {
          "text": "To train the model and measure final production accuracy.",
          "explanation": "EDA is performed before training and does not replace evaluation.",
          "category": "incorrect"
        },
        {
          "text": "To guarantee the model won’t overfit.",
          "explanation": "EDA can reduce risk but cannot guarantee generalization.",
          "category": "incorrect"
        },
        {
          "text": "To skip preprocessing because EDA fixes the data automatically.",
          "explanation": "EDA informs preprocessing; it does not apply fixes by itself.",
          "category": "incorrect"
        },
        {
          "text": "To ask the dataset how it’s feeling today.",
          "explanation": "Datasets don’t have feelings; EDA is analytical.",
          "category": "ridiculous"
        },
        {
          "text": "To hypnotize the data into revealing its secrets.",
          "explanation": "EDA uses statistics and plots, not hypnosis.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Dataset understanding & framing"
    },
    {
      "id": "1eef8d10e49d8d6c5623f0dd76d466b80f2787b61859df005e1cbbac2d9e936b",
      "question": "Before running any plots, what key information should you confirm about the target/label column?",
      "tags": [
        "target",
        "labeling"
      ],
      "answers": [
        {
          "text": "Confirm the target exists, its type (classification/regression), valid values/range, and how it’s encoded.",
          "explanation": "You must frame the ML problem correctly and avoid misinterpreting analyses.",
          "category": "correct"
        },
        {
          "text": "Verify what the label means and how it’s represented (classes vs numeric), including any invalid/unknown codes.",
          "explanation": "Bad label semantics or encoding can invalidate EDA conclusions.",
          "category": "correct"
        },
        {
          "text": "Confirm the target column name.",
          "explanation": "Helpful, but you also need meaning, type, and valid values.",
          "category": "partially_correct"
        },
        {
          "text": "Check whether the label is numeric or text.",
          "explanation": "Good start, but still missing the label’s semantics and allowed values.",
          "category": "partially_correct"
        },
        {
          "text": "Confirm the target is normally distributed.",
          "explanation": "Not required; classification targets are not expected to be normal.",
          "category": "incorrect"
        },
        {
          "text": "Confirm the target is independent of all features.",
          "explanation": "If it were independent, learning would be impossible.",
          "category": "incorrect"
        },
        {
          "text": "Assume any column named ‘y’ must be the label.",
          "explanation": "You must confirm labels using the problem definition, not naming conventions.",
          "category": "incorrect"
        },
        {
          "text": "Confirm the target column is ‘the chosen one.’",
          "explanation": "Target selection must come from business/problem context.",
          "category": "ridiculous"
        },
        {
          "text": "Ask the label to introduce itself politely.",
          "explanation": "Use documentation and validation checks, not anthropomorphism.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Dataset understanding & framing"
    },
    {
      "id": "82b5dac0c75a459c58246b438303267a567d3860a428735a02a9e2c02739f5cb",
      "question": "Why is a data dictionary (or feature definition document) useful during EDA?",
      "tags": [
        "data_dictionary",
        "features"
      ],
      "answers": [
        {
          "text": "It clarifies what each feature means, units, allowed values, and how it was collected.",
          "explanation": "This prevents incorrect assumptions and helps identify inconsistencies.",
          "category": "correct"
        },
        {
          "text": "It prevents misinterpretation by documenting semantics and constraints, which helps catch inconsistencies and leakage.",
          "explanation": "EDA is far more reliable when feature meanings are explicit.",
          "category": "correct"
        },
        {
          "text": "It tells you which columns are numeric vs categorical.",
          "explanation": "That’s part of it, but it also captures semantics, units, and constraints.",
          "category": "partially_correct"
        },
        {
          "text": "It helps you remember what columns exist.",
          "explanation": "True, but its main value is meaning and constraints.",
          "category": "partially_correct"
        },
        {
          "text": "It replaces EDA because it already explains the data.",
          "explanation": "Documentation can be incomplete or wrong; EDA validates reality.",
          "category": "incorrect"
        },
        {
          "text": "It’s only useful after you deploy the model.",
          "explanation": "It’s most valuable early to guide correct analysis.",
          "category": "incorrect"
        },
        {
          "text": "It’s unnecessary if you can see the column names.",
          "explanation": "Column names rarely capture units, constraints, and collection context.",
          "category": "incorrect"
        },
        {
          "text": "It’s a dictionary where features look up synonyms.",
          "explanation": "It’s metadata about fields, not language.",
          "category": "ridiculous"
        },
        {
          "text": "It’s where the dataset stores its diary entries.",
          "explanation": "It’s a technical reference, not a journal.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Dataset understanding & framing"
    },
    {
      "id": "b39caa1db3de20f183b81fbc220d470bba8eef3395bef5eda73e32b24448ece4",
      "question": "What are common causes of missing values, and how can the cause influence how you handle them?",
      "tags": [
        "missing_data",
        "imputation"
      ],
      "answers": [
        {
          "text": "Missingness may be MCAR/MAR/MNAR (random, related to other fields, or related to the value itself), and the cause affects whether you impute, model missingness, or collect more data.",
          "explanation": "The missingness mechanism determines bias risk and appropriate handling.",
          "category": "correct"
        },
        {
          "text": "The missingness mechanism matters: you might impute, add a missing-indicator, or avoid biased imputation if missingness is informative.",
          "explanation": "If missingness is informative, treating it as random can be harmful.",
          "category": "correct"
        },
        {
          "text": "Missing values happen from sensor errors or users skipping fields; you can fill with mean/median.",
          "explanation": "Sometimes, but feature type and missingness mechanism often require different strategies.",
          "category": "partially_correct"
        },
        {
          "text": "You can drop missing values if there aren’t many.",
          "explanation": "Dropping can be acceptable, but may introduce bias depending on why values are missing.",
          "category": "partially_correct"
        },
        {
          "text": "Always drop any row with a missing value.",
          "explanation": "This can discard useful signal and introduce bias.",
          "category": "incorrect"
        },
        {
          "text": "Replace missing values with zero because zero is neutral.",
          "explanation": "Zero can be meaningful and can distort distributions.",
          "category": "incorrect"
        },
        {
          "text": "Missing values don’t matter because models ignore them automatically.",
          "explanation": "Many models require explicit handling of missing values.",
          "category": "incorrect"
        },
        {
          "text": "Missing values are the dataset’s way of being mysterious.",
          "explanation": "Missingness is usually due to data collection or pipeline issues.",
          "category": "ridiculous"
        },
        {
          "text": "They’re blank because the data was shy.",
          "explanation": "Missing data is a technical/data-generation issue.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Data quality checks & cleaning signals"
    },
    {
      "id": "90ab803b281594fe3f2beb6eeda2ee4d77b2a234914ac9eace6652e68efed22e",
      "question": "Why can duplicate rows be harmful in model evaluation, and how would you detect them?",
      "tags": [
        "duplicates",
        "data_quality"
      ],
      "answers": [
        {
          "text": "Duplicates can leak identical examples into train and test, inflating metrics; detect by row hashing or key-based duplicate checks.",
          "explanation": "Train/test contamination makes evaluation overly optimistic.",
          "category": "correct"
        },
        {
          "text": "Duplicates reduce effective sample size and can cause train/test contamination; detect via full-row duplicate checks or unique-key constraints.",
          "explanation": "Duplicates bias training and distort validation.",
          "category": "correct"
        },
        {
          "text": "Duplicates waste space; use a ‘drop duplicates’ command.",
          "explanation": "That helps storage, but the bigger risk is evaluation leakage.",
          "category": "partially_correct"
        },
        {
          "text": "They can skew summary stats if repeated a lot.",
          "explanation": "True, but leakage in splits is often the most damaging outcome.",
          "category": "partially_correct"
        },
        {
          "text": "Duplicates are good because they increase sample size.",
          "explanation": "They reduce effective sample size and can bias learning.",
          "category": "incorrect"
        },
        {
          "text": "Duplicates only matter for deep learning, not classical ML.",
          "explanation": "Duplicates can harm any model and any evaluation approach.",
          "category": "incorrect"
        },
        {
          "text": "If you shuffle before splitting, duplicates can’t cause leakage.",
          "explanation": "Shuffling does not prevent identical rows from appearing in both splits.",
          "category": "incorrect"
        },
        {
          "text": "Duplicates are fine if they’re identical twins.",
          "explanation": "Identical observations still distort evaluation.",
          "category": "ridiculous"
        },
        {
          "text": "Just rename the duplicates so they don’t count.",
          "explanation": "Renaming doesn’t change that they’re duplicates.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Data quality checks & cleaning signals"
    },
    {
      "id": "d19b8c8530c22d697cf89463e7891d0475c39f3e3c54dfd7976e6e7c0e323b00",
      "question": "What is a “schema/type mismatch,” and how can it break downstream EDA or modeling?",
      "tags": [
        "schema",
        "datatypes"
      ],
      "answers": [
        {
          "text": "It’s when a column’s stored type doesn’t match its meaning (e.g., numbers as strings, timestamps as ints), causing wrong stats, bad plots, or failed preprocessing.",
          "explanation": "Tools behave differently by dtype; wrong dtypes lead to wrong analyses.",
          "category": "correct"
        },
        {
          "text": "It’s a representation error (e.g., dates-as-strings, numbers-as-text) that leads tools and models to treat values incorrectly.",
          "explanation": "This can silently break pipelines or invalidate feature handling.",
          "category": "correct"
        },
        {
          "text": "It’s when the column is ‘object’ instead of numeric in pandas.",
          "explanation": "That’s a common symptom, but mismatch is about meaning vs representation.",
          "category": "partially_correct"
        },
        {
          "text": "It’s when parsing fails and you get NaNs.",
          "explanation": "Sometimes, but mismatches can also occur without NaNs.",
          "category": "partially_correct"
        },
        {
          "text": "It only matters for visualization; models don’t care.",
          "explanation": "Many models and preprocessors require correct types.",
          "category": "incorrect"
        },
        {
          "text": "Schema mismatch is the same thing as class imbalance.",
          "explanation": "Schema is about types/structure; imbalance is about label frequencies.",
          "category": "incorrect"
        },
        {
          "text": "You can fix type issues by standardizing the column.",
          "explanation": "Standardization assumes valid numeric types; it won’t fix parsing/meaning.",
          "category": "incorrect"
        },
        {
          "text": "Schema mismatch is when the data disagrees with your vibes.",
          "explanation": "It’s a concrete technical mismatch.",
          "category": "ridiculous"
        },
        {
          "text": "It means the schema is jealous of your model.",
          "explanation": "It’s about representation correctness.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Data quality checks & cleaning signals"
    },
    {
      "id": "afc3095d30ce55404106cd416a91373d9628c6c2e5e906f20395653eadbc1454",
      "question": "What is one EDA technique to identify potential data leakage before modeling?",
      "tags": [
        "leakage",
        "validation"
      ],
      "answers": [
        {
          "text": "Look for features that encode the target or future information (post-outcome fields) using feature definitions and suspiciously high correlation with the target.",
          "explanation": "Leakage often shows up as ‘too good to be true’ signals.",
          "category": "correct"
        },
        {
          "text": "Audit features for post-outcome fields and ‘too predictive’ signals that wouldn’t exist at inference time.",
          "explanation": "If a feature wouldn’t be available at prediction time, it can cause leakage.",
          "category": "correct"
        },
        {
          "text": "Remove the target column from the dataset.",
          "explanation": "Necessary, but leakage typically comes from other columns.",
          "category": "partially_correct"
        },
        {
          "text": "Look at correlation with the label.",
          "explanation": "Useful, but leakage can be nonlinear or conditional.",
          "category": "partially_correct"
        },
        {
          "text": "Leakage can’t be detected until after deployment.",
          "explanation": "Many leakage sources are visible during EDA.",
          "category": "incorrect"
        },
        {
          "text": "If correlation is low, leakage is impossible.",
          "explanation": "Leakage can be nonlinear or interaction-based.",
          "category": "incorrect"
        },
        {
          "text": "Leakage only happens when you accidentally include the label column twice.",
          "explanation": "Leakage often comes from proxy or future-derived fields.",
          "category": "incorrect"
        },
        {
          "text": "Leakage is when the CSV file gets wet.",
          "explanation": "Leakage refers to information leakage, not liquid.",
          "category": "ridiculous"
        },
        {
          "text": "Put the dataset in a bucket so it can’t leak.",
          "explanation": "You prevent leakage through feature and pipeline design.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Data quality checks & cleaning signals"
    },
    {
      "id": "2ef062ad3fb1224e5cd62a4687018788f44469588e1a3e8953aeb5a94c806e6b",
      "question": "When would median and IQR be more informative than mean and standard deviation?",
      "tags": [
        "robust_stats",
        "summary"
      ],
      "answers": [
        {
          "text": "When distributions are skewed or have outliers; median and IQR are robust to extreme values.",
          "explanation": "Mean and standard deviation can be dominated by extremes.",
          "category": "correct"
        },
        {
          "text": "When extreme values distort the average; median and IQR better represent typical value and spread.",
          "explanation": "Robust statistics better reflect central tendency under skew/outliers.",
          "category": "correct"
        },
        {
          "text": "When you want typical values.",
          "explanation": "Yes, especially when outliers skew the mean.",
          "category": "partially_correct"
        },
        {
          "text": "When the data looks lopsided.",
          "explanation": "Skew is a common reason to prefer robust summaries.",
          "category": "partially_correct"
        },
        {
          "text": "Always; mean and standard deviation are never useful.",
          "explanation": "Mean/std are useful for roughly symmetric distributions.",
          "category": "incorrect"
        },
        {
          "text": "Only when data is categorical.",
          "explanation": "Median/IQR are for numeric data.",
          "category": "incorrect"
        },
        {
          "text": "Median is better because it uses more data points than the mean.",
          "explanation": "Both use all points; median is robust because it’s rank-based.",
          "category": "incorrect"
        },
        {
          "text": "When the mean is feeling insecure.",
          "explanation": "It’s about statistical robustness.",
          "category": "ridiculous"
        },
        {
          "text": "When you want statistics with a cooler name.",
          "explanation": "Choose based on distribution properties, not aesthetics.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Descriptive statistics & summaries"
    },
    {
      "id": "e8349a5cb6f917d4ff73ae1fd0a8358bbe3c43abc0f8b0a1572159e262d04ae9",
      "question": "Why are group-by summaries (e.g., per class, per region) often more useful than global summaries?",
      "tags": [
        "groupby",
        "aggregation"
      ],
      "answers": [
        {
          "text": "Because overall aggregates can hide segment differences; group-by reveals patterns by class/region/time that affect modeling.",
          "explanation": "Important patterns may only exist within subgroups.",
          "category": "correct"
        },
        {
          "text": "They expose heterogeneity—subgroups may behave differently even when overall averages look fine.",
          "explanation": "This helps avoid being misled by aggregation (e.g., Simpson’s paradox).",
          "category": "correct"
        },
        {
          "text": "Because they produce more tables.",
          "explanation": "More output isn’t the goal; subgroup insight is.",
          "category": "partially_correct"
        },
        {
          "text": "Because you can compare groups side-by-side.",
          "explanation": "Yes, comparison is valuable when groups differ.",
          "category": "partially_correct"
        },
        {
          "text": "Group-bys are only for databases, not EDA.",
          "explanation": "Group-bys are a core EDA technique in any tool.",
          "category": "incorrect"
        },
        {
          "text": "Global summaries are always sufficient.",
          "explanation": "Global summaries can mask subgroup structure.",
          "category": "incorrect"
        },
        {
          "text": "Grouping creates bias, so you shouldn’t do it.",
          "explanation": "Grouping can reveal bias and structure; it doesn’t inherently create it.",
          "category": "incorrect"
        },
        {
          "text": "Because groups like being summarized together.",
          "explanation": "Grouping is for analysis, not social bonding.",
          "category": "ridiculous"
        },
        {
          "text": "Because the data wants to form social clubs.",
          "explanation": "Groups reflect structure, not intent.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Descriptive statistics & summaries"
    },
    {
      "id": "de487813518549e54d2461355e116136f40320732c98400fc13d201b71cebdd4",
      "question": "What does a percentile (e.g., 95th) tell you that the mean does not?",
      "tags": [
        "percentiles",
        "distribution"
      ],
      "answers": [
        {
          "text": "It provides a rank-based threshold such as ‘95% of values are below X,’ revealing tail behavior the mean can hide.",
          "explanation": "Percentiles show spread and extremes directly.",
          "category": "correct"
        },
        {
          "text": "It gives a tail cutoff useful for ‘typical worst-case’ behavior and skewed distributions.",
          "explanation": "Means can hide important tail risk.",
          "category": "correct"
        },
        {
          "text": "It tells you the maximum value.",
          "explanation": "Only the 100th percentile is the maximum; 95th is a threshold.",
          "category": "partially_correct"
        },
        {
          "text": "It tells you the rank position of values.",
          "explanation": "Yes—percentiles are distribution cutoffs by rank.",
          "category": "partially_correct"
        },
        {
          "text": "Percentiles and mean are the same thing.",
          "explanation": "Mean is an average; percentiles are rank cutoffs.",
          "category": "incorrect"
        },
        {
          "text": "Percentiles only matter for normal distributions.",
          "explanation": "Percentiles apply to any distribution.",
          "category": "incorrect"
        },
        {
          "text": "The 95th percentile means 95% of values are exactly that number.",
          "explanation": "It means 95% are at or below that value.",
          "category": "incorrect"
        },
        {
          "text": "It tells you what percent the data likes you.",
          "explanation": "It’s a statistical rank measure.",
          "category": "ridiculous"
        },
        {
          "text": "It’s the dataset’s speed limit.",
          "explanation": "Percentiles are distribution thresholds, not constraints.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Descriptive statistics & summaries"
    },
    {
      "id": "3fc7c039a5b151b41ac4e136719e11be25f9065ce24b2c6cd43d96e556c301a3",
      "question": "What is a practical reason to compute summary stats separately for train vs test splits?",
      "tags": [
        "train_test",
        "dataset_shift"
      ],
      "answers": [
        {
          "text": "To detect distribution differences (shift/leakage) and to ensure preprocessing statistics are learned from train only.",
          "explanation": "Using test stats in preprocessing can leak information.",
          "category": "correct"
        },
        {
          "text": "To spot drift and avoid peeking at test data when fitting scalers or imputers.",
          "explanation": "Train-derived parameters preserve evaluation integrity.",
          "category": "correct"
        },
        {
          "text": "Because it’s good practice to look at both.",
          "explanation": "Yes, but the why is drift detection and leakage avoidance.",
          "category": "partially_correct"
        },
        {
          "text": "To make sure the split worked as expected.",
          "explanation": "True, especially for detecting unintended sampling differences.",
          "category": "partially_correct"
        },
        {
          "text": "Because test statistics should override train statistics.",
          "explanation": "That leaks information and breaks evaluation.",
          "category": "incorrect"
        },
        {
          "text": "You should never look at the test distribution.",
          "explanation": "You can compare distributions without using labels to tune.",
          "category": "incorrect"
        },
        {
          "text": "Train/test stats are always identical if the split is random.",
          "explanation": "They can still differ by chance or due to hidden stratification issues.",
          "category": "incorrect"
        },
        {
          "text": "Because train and test have different personalities.",
          "explanation": "They can differ statistically; that’s what you’re checking.",
          "category": "ridiculous"
        },
        {
          "text": "Because the test set demands special treatment.",
          "explanation": "The goal is integrity, not pampering.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Descriptive statistics & summaries"
    },
    {
      "id": "7bf7e922bbf9f11c733824368cf9fa313366d21f9c6288a89e3bddfad2bdc319",
      "question": "What does strong skew in a numeric feature suggest, and what are common next steps?",
      "tags": [
        "skew",
        "transforms"
      ],
      "answers": [
        {
          "text": "Skew suggests a long tail; consider transforms (log/Box-Cox), robust scaling, or choosing models less sensitive to skew.",
          "explanation": "Skew can violate assumptions and make some losses overly sensitive.",
          "category": "correct"
        },
        {
          "text": "It implies non-symmetry; consider transforming, using robust summaries, or carefully clipping if extremes are errors.",
          "explanation": "Many real-world variables are naturally skewed; don’t assume error.",
          "category": "correct"
        },
        {
          "text": "Skew suggests outliers; remove them.",
          "explanation": "Sometimes, but skew can be legitimate heavy-tail behavior.",
          "category": "partially_correct"
        },
        {
          "text": "Try standardization to make it look nicer.",
          "explanation": "Standardization changes scale but does not remove skew.",
          "category": "partially_correct"
        },
        {
          "text": "Skew means the feature is useless.",
          "explanation": "Skewed features can be highly predictive.",
          "category": "incorrect"
        },
        {
          "text": "Skew only happens when data is wrong.",
          "explanation": "Income, counts, and durations are often naturally skewed.",
          "category": "incorrect"
        },
        {
          "text": "You must always delete skewed features.",
          "explanation": "Transforming or using robust methods is often better than deletion.",
          "category": "incorrect"
        },
        {
          "text": "Skew means the histogram is leaning.",
          "explanation": "It’s a distribution property, not chart gravity.",
          "category": "ridiculous"
        },
        {
          "text": "Rotate the plot 10 degrees to fix skew.",
          "explanation": "You fix skew via transformations, not rotation.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Univariate analysis & distributions"
    },
    {
      "id": "1a466ae34a9caef012522d22ad8e7507c015e9a82bbbc39e1a478a245763743c",
      "question": "How do you interpret a bimodal distribution, and what might it imply about the data?",
      "tags": [
        "bimodal",
        "segments"
      ],
      "answers": [
        {
          "text": "It often implies a mixture of subpopulations or regimes (e.g., two cohorts or processes).",
          "explanation": "Bimodality can indicate hidden groups or different generating mechanisms.",
          "category": "correct"
        },
        {
          "text": "Two peaks commonly indicate hidden grouping, suggesting stratification or an omitted feature.",
          "explanation": "You may need segmentation or additional features to explain the modes.",
          "category": "correct"
        },
        {
          "text": "It means there are two common values.",
          "explanation": "Close—usually it’s two peaks over ranges, not necessarily two exact values.",
          "category": "partially_correct"
        },
        {
          "text": "It means the data might come from two sources.",
          "explanation": "Often true; it can be two populations, systems, or regimes.",
          "category": "partially_correct"
        },
        {
          "text": "It guarantees the feature is binary.",
          "explanation": "Continuous features can be bimodal.",
          "category": "incorrect"
        },
        {
          "text": "It means the feature must be standardized immediately.",
          "explanation": "Standardization doesn’t resolve mixture structure.",
          "category": "incorrect"
        },
        {
          "text": "It proves your labels are wrong.",
          "explanation": "It may indicate segments; labels could still be correct.",
          "category": "incorrect"
        },
        {
          "text": "It means the data has two moods.",
          "explanation": "It indicates mixture structure, not emotions.",
          "category": "ridiculous"
        },
        {
          "text": "It means you need two models because the data is bipolar.",
          "explanation": "You might segment, but bimodality alone doesn’t mandate separate models.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Univariate analysis & distributions"
    },
    {
      "id": "8f363c62cc10cf0eac7c7a15ae5d3537605dfbb9f3cce89ba3ffc29d6957ae9c",
      "question": "For a categorical feature with many unique values, what would you examine first during EDA?",
      "tags": [
        "categorical",
        "cardinality"
      ],
      "answers": [
        {
          "text": "Examine the frequency distribution (top levels vs long tail), rare/missing levels, and whether it looks ID-like.",
          "explanation": "High cardinality can indicate identifiers or a need for special encoding.",
          "category": "correct"
        },
        {
          "text": "Check cardinality and concentration (e.g., a few dominant levels vs many rare levels), then decide on grouping or alternative encodings.",
          "explanation": "One-hot can explode dimensionality and overfit with rare categories.",
          "category": "correct"
        },
        {
          "text": "Convert it to one-hot encoding.",
          "explanation": "May work, but often impractical for extreme cardinality.",
          "category": "partially_correct"
        },
        {
          "text": "Look at the top 10 most frequent categories.",
          "explanation": "Good first step, but also inspect tail/rare categories and ID-likeness.",
          "category": "partially_correct"
        },
        {
          "text": "Treat it as continuous numeric and compute a mean.",
          "explanation": "Categories are not ordered magnitudes.",
          "category": "incorrect"
        },
        {
          "text": "Drop it automatically because it has too many values.",
          "explanation": "It might be predictive; consider better encoding first.",
          "category": "incorrect"
        },
        {
          "text": "Sort categories alphabetically to create an ordering.",
          "explanation": "Alphabetical order does not represent meaningful numeric structure.",
          "category": "incorrect"
        },
        {
          "text": "Alphabetize categories until the model understands them.",
          "explanation": "Sorting labels doesn’t create signal.",
          "category": "ridiculous"
        },
        {
          "text": "Give every category a nickname to reduce cardinality.",
          "explanation": "You reduce cardinality by grouping/encoding, not renaming.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Univariate analysis & distributions"
    },
    {
      "id": "391c70806ef17de9a58b669b45b827d78b2ff26876e5564ff0d2c2c82dac55fa",
      "question": "When might a log transform improve interpretability or model performance?",
      "tags": [
        "log_transform",
        "scaling"
      ],
      "answers": [
        {
          "text": "When values are positive and highly skewed/heavy-tailed; log compresses extremes and can linearize multiplicative relationships.",
          "explanation": "It reduces outlier influence and can stabilize variance.",
          "category": "correct"
        },
        {
          "text": "When variance increases with magnitude (heteroscedasticity) and relationships are more multiplicative than additive.",
          "explanation": "Log can make patterns more linear and easier to model.",
          "category": "correct"
        },
        {
          "text": "When numbers are large.",
          "explanation": "Magnitude alone isn’t the reason; distribution shape and relationship structure matter.",
          "category": "partially_correct"
        },
        {
          "text": "When you want to reduce the impact of extreme values.",
          "explanation": "Often true, assuming values are positive (or you use log1p/shift).",
          "category": "partially_correct"
        },
        {
          "text": "When data includes many zeros and negatives without adjustment.",
          "explanation": "Plain log is undefined for non-positive values; you must adapt the transform.",
          "category": "incorrect"
        },
        {
          "text": "To make categorical variables numeric.",
          "explanation": "Log transforms numeric magnitudes, not categories.",
          "category": "incorrect"
        },
        {
          "text": "Log transform always improves any dataset.",
          "explanation": "It can harm interpretability or fit if the assumptions don’t match.",
          "category": "incorrect"
        },
        {
          "text": "To make the data more ‘logged in.’",
          "explanation": "It’s a mathematical transform, not authentication.",
          "category": "ridiculous"
        },
        {
          "text": "Because logs are natural, like trees.",
          "explanation": "Use transforms for statistical reasons, not wordplay.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Univariate analysis & distributions"
    },
    {
      "id": "2e54eb6c8f76ae9a707b4c655728b17cfaf4e73be72e116b203ced7af9b6cabc",
      "question": "How does the IQR method identify outliers, and what is a limitation of this approach?",
      "tags": [
        "outliers",
        "iqr"
      ],
      "answers": [
        {
          "text": "It flags values below Q1−1.5×IQR or above Q3+1.5×IQR; a limitation is it may over-flag legitimate extremes in heavy-tail or multimodal data.",
          "explanation": "IQR is robust but not context-aware.",
          "category": "correct"
        },
        {
          "text": "It uses quartile-based fences; limitation is it’s distribution-agnostic and can mislabel valid tail points as outliers.",
          "explanation": "Heavy-tailed real data can trigger many flags without being erroneous.",
          "category": "correct"
        },
        {
          "text": "It finds outliers by looking far from the median.",
          "explanation": "Related idea, but the method uses quartiles and IQR specifically.",
          "category": "partially_correct"
        },
        {
          "text": "It uses quartiles to find unusually large or small points.",
          "explanation": "Yes, the fences are derived from Q1, Q3, and IQR.",
          "category": "partially_correct"
        },
        {
          "text": "IQR outliers are always errors to delete.",
          "explanation": "Outliers can be valid rare events.",
          "category": "incorrect"
        },
        {
          "text": "IQR only works for categorical variables.",
          "explanation": "IQR is for numeric variables.",
          "category": "incorrect"
        },
        {
          "text": "IQR detects outliers by maximizing likelihood.",
          "explanation": "That describes likelihood-based methods, not IQR fences.",
          "category": "incorrect"
        },
        {
          "text": "IQR stands for ‘I Quit Reading’ the data.",
          "explanation": "It stands for interquartile range.",
          "category": "ridiculous"
        },
        {
          "text": "It finds outliers by interrogating the data.",
          "explanation": "It uses a deterministic quartile rule.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Outliers & anomalies"
    },
    {
      "id": "96e486b88f6f2de3587da2931e973a6ebbada4afdeb57ea230434c5a2d7524d6",
      "question": "Why is it important to distinguish between “bad data” outliers and “rare but real” outliers?",
      "tags": [
        "anomalies",
        "data_quality"
      ],
      "answers": [
        {
          "text": "Bad-data outliers should be corrected or removed, but rare real events may be the key signal (e.g., fraud) and should be preserved.",
          "explanation": "Treatment depends on whether the value is error or reality.",
          "category": "correct"
        },
        {
          "text": "Because the right action depends on whether it’s a measurement/ETL error or a legitimate rare case the model must learn.",
          "explanation": "Removing valid rare cases can destroy model usefulness.",
          "category": "correct"
        },
        {
          "text": "Because removing outliers changes the mean.",
          "explanation": "True, but the key issue is whether you’re removing signal or corruption.",
          "category": "partially_correct"
        },
        {
          "text": "Because outliers can affect plots a lot.",
          "explanation": "Yes, but modeling correctness is the bigger concern.",
          "category": "partially_correct"
        },
        {
          "text": "Outliers are always noise and should be removed.",
          "explanation": "Many problems depend on rare events.",
          "category": "incorrect"
        },
        {
          "text": "Outliers never matter because models average them out.",
          "explanation": "Outliers can dominate loss and distort fit.",
          "category": "incorrect"
        },
        {
          "text": "If you use trees, you can ignore all anomalies.",
          "explanation": "Trees can be more robust, but anomalies can still matter.",
          "category": "incorrect"
        },
        {
          "text": "Because outliers have feelings and deserve empathy.",
          "explanation": "It’s about correctness and impact.",
          "category": "ridiculous"
        },
        {
          "text": "Because outliers demand a separate union contract.",
          "explanation": "Decisions should be statistical and domain-driven.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Outliers & anomalies"
    },
    {
      "id": "d26a0df9c876f33725c092be2617665c79660f61f10917dd40a8d0c3b65dd5e1",
      "question": "How can outliers disproportionately affect linear models compared to tree-based models?",
      "tags": [
        "outliers",
        "model_sensitivity"
      ],
      "answers": [
        {
          "text": "Linear models fit a global function (often minimizing squared loss), so extreme points can pull coefficients strongly; trees split locally and can isolate outliers.",
          "explanation": "Outliers can dominate global optimization for linear models.",
          "category": "correct"
        },
        {
          "text": "A few extreme points can dominate the fit/gradients in linear regression, while trees can ‘box in’ outliers via splits.",
          "explanation": "Trees can limit the influence of extremes to a small region.",
          "category": "correct"
        },
        {
          "text": "Trees ignore outliers.",
          "explanation": "Not exactly; trees may be less globally sensitive but not immune.",
          "category": "partially_correct"
        },
        {
          "text": "Linear models are sensitive to extreme values.",
          "explanation": "Yes, especially with squared loss.",
          "category": "partially_correct"
        },
        {
          "text": "Linear models are immune to outliers after standardization.",
          "explanation": "Scaling doesn’t remove extreme influence on loss.",
          "category": "incorrect"
        },
        {
          "text": "Trees always perform better because of outliers.",
          "explanation": "Not always; it depends on noise, size, and structure.",
          "category": "incorrect"
        },
        {
          "text": "Outliers only affect neural networks.",
          "explanation": "Outliers can affect most model families.",
          "category": "incorrect"
        },
        {
          "text": "Trees are made of wood, so they’re tougher.",
          "explanation": "Robustness comes from algorithmic structure.",
          "category": "ridiculous"
        },
        {
          "text": "Linear models are allergic to big numbers.",
          "explanation": "They’re sensitive due to objective functions, not allergies.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Outliers & anomalies"
    },
    {
      "id": "d9cfb1ca931d7f4034c2a95213296f0cc4bf7bd9b762bb65562d8aec36f206b2",
      "question": "What’s the difference between correlation and causation, and why does it matter in EDA?",
      "tags": [
        "correlation",
        "causation"
      ],
      "answers": [
        {
          "text": "Correlation is association; causation means changing X produces change in Y. Confusing them leads to wrong conclusions and brittle decisions.",
          "explanation": "Spurious correlations and confounding are common.",
          "category": "correct"
        },
        {
          "text": "Correlation can arise from confounders; assuming causality can lead to unstable models and harmful interventions.",
          "explanation": "EDA should inform hypotheses, not prove causal claims.",
          "category": "correct"
        },
        {
          "text": "Correlation means they move together; causation means one affects the other.",
          "explanation": "Good start, but you must consider confounding and directionality.",
          "category": "partially_correct"
        },
        {
          "text": "Correlation is a relationship; causation is a reason.",
          "explanation": "True, but causation requires stronger evidence than EDA typically provides.",
          "category": "partially_correct"
        },
        {
          "text": "High correlation proves causation.",
          "explanation": "Correlation alone does not establish causality.",
          "category": "incorrect"
        },
        {
          "text": "Causation is irrelevant in ML.",
          "explanation": "It matters for interventions, stability, and policy decisions.",
          "category": "incorrect"
        },
        {
          "text": "If two variables correlate, changing one will always change the other.",
          "explanation": "Correlation may be non-causal or reversed.",
          "category": "incorrect"
        },
        {
          "text": "Causation is when the chart looks causal.",
          "explanation": "Visual appearance doesn’t prove causality.",
          "category": "ridiculous"
        },
        {
          "text": "Causation is when the scatterplot points ‘point’ somewhere.",
          "explanation": "Causality requires experimental or strong quasi-experimental evidence.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Bivariate relationships & association"
    },
    {
      "id": "c5f9f82ec6c1c740c325cb89936250da114099c6ecc060ecad0a3a5b56bb400f",
      "question": "How would you assess the relationship between two continuous variables during EDA?",
      "tags": [
        "scatterplot",
        "continuous"
      ],
      "answers": [
        {
          "text": "Use scatterplots, correlation (Pearson/Spearman), and trend lines; check nonlinearity, heteroscedasticity, and outliers.",
          "explanation": "Different diagnostics reveal different relationship properties.",
          "category": "correct"
        },
        {
          "text": "Plot X vs Y and inspect shape (linear/nonlinear), correlation, and whether variance changes across X.",
          "explanation": "A single metric often misses important structure.",
          "category": "correct"
        },
        {
          "text": "Compute Pearson correlation.",
          "explanation": "Useful for linear association but misses nonlinear patterns.",
          "category": "partially_correct"
        },
        {
          "text": "Make a scatterplot.",
          "explanation": "Great first step; add diagnostics for strength and shape.",
          "category": "partially_correct"
        },
        {
          "text": "Only compare their means.",
          "explanation": "Means don’t describe paired association.",
          "category": "incorrect"
        },
        {
          "text": "If correlation is near zero, they are unrelated in all ways.",
          "explanation": "Nonlinear relationships can have near-zero correlation.",
          "category": "incorrect"
        },
        {
          "text": "You must discretize both variables before you can compare them.",
          "explanation": "Discretization can lose information and is not required.",
          "category": "incorrect"
        },
        {
          "text": "Ask the variables if they’re friends.",
          "explanation": "Use plots and statistics, not anthropomorphism.",
          "category": "ridiculous"
        },
        {
          "text": "Measure relationship by how close their names are alphabetically.",
          "explanation": "Variable names do not indicate statistical association.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Bivariate relationships & association"
    },
    {
      "id": "20b7c80394f2ef3f31c9d43af363f0a7dd694e1a5e2deee032f9bddcd03ae2dd",
      "question": "How would you compare a continuous feature across multiple classes in a classification dataset?",
      "tags": [
        "class_comparison",
        "distributions"
      ],
      "answers": [
        {
          "text": "Use boxplots/violin plots or KDEs per class and compare medians and spread; optionally use ANOVA or Kruskal-Wallis depending on assumptions.",
          "explanation": "You need class-conditional distribution comparisons, not just averages.",
          "category": "correct"
        },
        {
          "text": "Compare class-conditional distributions (overlap, shifts, tails), not only class means.",
          "explanation": "Separability and overlap are key for classification.",
          "category": "correct"
        },
        {
          "text": "Plot the mean of the feature per class.",
          "explanation": "Helpful but hides overlap and distribution shape.",
          "category": "partially_correct"
        },
        {
          "text": "Create separate histograms per class.",
          "explanation": "Good approach; ensure comparable bins/scales and check overlap.",
          "category": "partially_correct"
        },
        {
          "text": "One-hot encode the continuous variable.",
          "explanation": "One-hot is for categorical variables.",
          "category": "incorrect"
        },
        {
          "text": "Ignore the classes; the feature distribution is the same for everyone.",
          "explanation": "Class-conditional differences are often the signal.",
          "category": "incorrect"
        },
        {
          "text": "Sort the classes by the feature and call it ‘comparison’.",
          "explanation": "Sorting doesn’t quantify or visualize distribution differences.",
          "category": "incorrect"
        },
        {
          "text": "Let each class vote on the feature’s value.",
          "explanation": "Use statistical and visual comparisons instead.",
          "category": "ridiculous"
        },
        {
          "text": "Challenge the classes to a duel of medians.",
          "explanation": "Choose methods that reveal overlap and effect size.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Bivariate relationships & association"
    },
    {
      "id": "68e7993db859d2754bc89908aa95e0ee71d6a4dbf5a6f58366c8e778b2d5e91e",
      "question": "What is multicollinearity, and what is a common EDA signal that it exists?",
      "tags": [
        "multicollinearity",
        "correlation"
      ],
      "answers": [
        {
          "text": "It’s strong linear redundancy among predictors; signals include high pairwise correlations or high variance inflation factor (VIF) and unstable coefficients.",
          "explanation": "Multicollinearity hurts interpretability and can destabilize some models.",
          "category": "correct"
        },
        {
          "text": "Predictors are linearly dependent/redundant; you often see correlated clusters in a correlation matrix and coefficient instability.",
          "explanation": "EDA can reveal redundant feature groups.",
          "category": "correct"
        },
        {
          "text": "It’s when features are related to each other.",
          "explanation": "Yes—specifically when predictors are strongly linearly related.",
          "category": "partially_correct"
        },
        {
          "text": "It’s when two columns look similar.",
          "explanation": "Similarity can indicate redundancy, but quantify it with correlation/VIF.",
          "category": "partially_correct"
        },
        {
          "text": "It’s when the target has multiple classes.",
          "explanation": "That’s multiclass classification, not multicollinearity.",
          "category": "incorrect"
        },
        {
          "text": "It only happens if you standardize features.",
          "explanation": "Standardization does not create collinearity.",
          "category": "incorrect"
        },
        {
          "text": "Multicollinearity improves interpretability because features agree.",
          "explanation": "Redundancy can make coefficients unreliable and hard to interpret.",
          "category": "incorrect"
        },
        {
          "text": "It’s when columns are collaborating behind your back.",
          "explanation": "It’s statistical redundancy, not intent.",
          "category": "ridiculous"
        },
        {
          "text": "It’s when your features form a band.",
          "explanation": "It’s a correlation structure, not a music group.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Bivariate relationships & association"
    },
    {
      "id": "11bdede4568a0ed9a6aec6a4ab504413aa66291a7ce9d9a45c35dd2a52e8a4a6",
      "question": "Why can a feature show high correlation with the target in training data but fail in production?",
      "tags": [
        "generalization",
        "leakage"
      ],
      "answers": [
        {
          "text": "Because of leakage, spurious correlation, sampling bias, or dataset shift where the relationship doesn’t hold in production.",
          "explanation": "Training data may not match deployment conditions.",
          "category": "correct"
        },
        {
          "text": "The training correlation may be accidental or time-bound; when the data-generating process changes, the signal disappears.",
          "explanation": "Stability across environments is required for production usefulness.",
          "category": "correct"
        },
        {
          "text": "Because the feature is noisy.",
          "explanation": "Noise can contribute, but shift/leakage are common root causes.",
          "category": "partially_correct"
        },
        {
          "text": "Because the model overfit.",
          "explanation": "Overfitting can make correlations seem more useful than they are.",
          "category": "partially_correct"
        },
        {
          "text": "Because correlation guarantees good performance everywhere.",
          "explanation": "Correlations can be unstable across time/populations.",
          "category": "incorrect"
        },
        {
          "text": "Because production data is always random.",
          "explanation": "Production data follows processes; it’s not arbitrary.",
          "category": "incorrect"
        },
        {
          "text": "Because production uses a different math for correlation.",
          "explanation": "The issue is distribution/process change, not arithmetic.",
          "category": "incorrect"
        },
        {
          "text": "Because production servers don’t like that feature.",
          "explanation": "It’s about data mismatch, not server preference.",
          "category": "ridiculous"
        },
        {
          "text": "Because the feature got stage fright.",
          "explanation": "It’s about generalization and shift, not emotions.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Bivariate relationships & association"
    },
    {
      "id": "7f7696d0aba82bdbb3515de3b0f395a5778103c93c1ee7d59a0a73a37ee810ce",
      "question": "What does a correlation heatmap help you detect that univariate plots cannot?",
      "tags": [
        "heatmap",
        "relationships"
      ],
      "answers": [
        {
          "text": "It reveals relationships among features (redundancy, correlated clusters) that univariate plots can’t show.",
          "explanation": "Univariate plots ignore cross-feature structure.",
          "category": "correct"
        },
        {
          "text": "It exposes cross-feature structure, such as correlated predictor groups that may indicate multicollinearity.",
          "explanation": "This guides feature selection and modeling choices.",
          "category": "correct"
        },
        {
          "text": "It shows which features are big or small.",
          "explanation": "It shows relationships, not magnitude.",
          "category": "partially_correct"
        },
        {
          "text": "It shows how features relate to each other.",
          "explanation": "Yes, specifically in terms of correlation strength and direction.",
          "category": "partially_correct"
        },
        {
          "text": "It proves which feature causes the target.",
          "explanation": "Correlation does not establish causality.",
          "category": "incorrect"
        },
        {
          "text": "It replaces the need for bivariate analysis.",
          "explanation": "It summarizes correlation but can miss nonlinear patterns and distribution details.",
          "category": "incorrect"
        },
        {
          "text": "It works only if all features are normally distributed.",
          "explanation": "Correlation heatmaps can be used more broadly (with care).",
          "category": "incorrect"
        },
        {
          "text": "It detects whether the dataset is hot or cold.",
          "explanation": "Heat is a visual encoding, not temperature.",
          "category": "ridiculous"
        },
        {
          "text": "It tells you which features are angry.",
          "explanation": "It encodes correlation, not emotions.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Multivariate structure & dimensionality intuition"
    },
    {
      "id": "063c14bd87b1eeb5312084cf464226afb782a1b862e0456bde09818ee62fc802",
      "question": "What is the intuition behind PCA as an EDA tool (without focusing on the math)?",
      "tags": [
        "pca",
        "dimensionality"
      ],
      "answers": [
        {
          "text": "PCA finds new axes (components) that capture the most variation, letting you view high-dimensional structure in fewer dimensions.",
          "explanation": "It helps visualize structure and reduce redundancy/noise.",
          "category": "correct"
        },
        {
          "text": "It rotates and compresses the feature space into a few components that summarize major variance patterns.",
          "explanation": "This supports visualization and exploratory dimensionality reduction.",
          "category": "correct"
        },
        {
          "text": "PCA compresses data.",
          "explanation": "Yes, but specifically by capturing variance in fewer components.",
          "category": "partially_correct"
        },
        {
          "text": "PCA makes fewer columns.",
          "explanation": "It creates new columns (components) that summarize patterns.",
          "category": "partially_correct"
        },
        {
          "text": "PCA selects the best original features and deletes the rest.",
          "explanation": "PCA typically creates new features rather than selecting originals.",
          "category": "incorrect"
        },
        {
          "text": "PCA is supervised and uses labels.",
          "explanation": "Standard PCA is unsupervised.",
          "category": "incorrect"
        },
        {
          "text": "PCA guarantees better model accuracy.",
          "explanation": "It can help or hurt depending on the task and signal.",
          "category": "incorrect"
        },
        {
          "text": "PCA stands for ‘Pretty Cool Analysis.’",
          "explanation": "It stands for Principal Component Analysis.",
          "category": "ridiculous"
        },
        {
          "text": "PCA is ‘Plot Changes Automatically.’",
          "explanation": "It’s a dimensionality reduction method.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Multivariate structure & dimensionality intuition"
    },
    {
      "id": "1a17cc242b757ede998c73ba3fa7e9a996b2757d3c681a434d7600bb28e362e4",
      "question": "How can interactions between features appear in multivariate EDA even if pairwise correlations are low?",
      "tags": [
        "interactions",
        "multivariate"
      ],
      "answers": [
        {
          "text": "Two features may jointly predict outcomes (e.g., XOR-like patterns) even if neither has strong linear pairwise correlation.",
          "explanation": "Interactions are conditional/joint effects that correlation can miss.",
          "category": "correct"
        },
        {
          "text": "Conditional relationships can exist where the effect of X on Y depends on Z, so pairwise correlation looks weak.",
          "explanation": "Interactions require multivariate inspection or modeling to reveal.",
          "category": "correct"
        },
        {
          "text": "Because correlation only checks one variable at a time.",
          "explanation": "Right idea—pairwise measures miss joint/conditional structure.",
          "category": "partially_correct"
        },
        {
          "text": "Because the relationship might be nonlinear.",
          "explanation": "Nonlinearity and conditional structure both reduce pairwise correlation usefulness.",
          "category": "partially_correct"
        },
        {
          "text": "Low correlation means interactions are impossible.",
          "explanation": "Interactions can exist without marginal correlation.",
          "category": "incorrect"
        },
        {
          "text": "Interactions only happen in neural networks.",
          "explanation": "Interactions are properties of data, not a single model family.",
          "category": "incorrect"
        },
        {
          "text": "If correlations are low, you should stop EDA.",
          "explanation": "You may need different tools (e.g., conditional plots, models) to detect structure.",
          "category": "incorrect"
        },
        {
          "text": "Because features gossip in groups.",
          "explanation": "It’s about joint predictive structure, not gossip.",
          "category": "ridiculous"
        },
        {
          "text": "Because variables form secret alliances.",
          "explanation": "Interactions are conditional patterns, not intent.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Multivariate structure & dimensionality intuition"
    },
    {
      "id": "b7d5a505715b6e0e15dccf0b6cbd860181b2ec6a9cfda69b2f4b9733698b6818",
      "question": "Why can axis scaling choices make a visualization misleading?",
      "tags": [
        "visualization",
        "scales"
      ],
      "answers": [
        {
          "text": "Scaling can exaggerate or hide differences (e.g., truncated axes or log vs linear), leading to wrong conclusions.",
          "explanation": "Axis choices strongly affect perceived effect size and trends.",
          "category": "correct"
        },
        {
          "text": "Truncation or inappropriate scaling can distort perceived changes and comparisons.",
          "explanation": "Good visualization preserves honest interpretation.",
          "category": "correct"
        },
        {
          "text": "Because it changes the picture.",
          "explanation": "Yes—specifically it changes perception of magnitude and trend.",
          "category": "partially_correct"
        },
        {
          "text": "Because people interpret slopes visually.",
          "explanation": "Slope and spread perception depend on scale.",
          "category": "partially_correct"
        },
        {
          "text": "Axis scaling doesn’t matter if the data is correct.",
          "explanation": "Presentation can mislead even when underlying data is correct.",
          "category": "incorrect"
        },
        {
          "text": "You should always start y-axis at the mean.",
          "explanation": "Arbitrary truncation can misrepresent effect size.",
          "category": "incorrect"
        },
        {
          "text": "A log scale is always dishonest.",
          "explanation": "Log scales can be appropriate for multiplicative or wide-range data.",
          "category": "incorrect"
        },
        {
          "text": "Because axes get jealous of each other.",
          "explanation": "It’s about perception and integrity, not jealousy.",
          "category": "ridiculous"
        },
        {
          "text": "Because the x-axis will feel left out.",
          "explanation": "Axes are design elements, not emotional entities.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Visualization best practices & pitfalls"
    },
    {
      "id": "6bb7196b561250eb84537d8220b839febee444ee22fe98d433d6fea002e45b6b",
      "question": "When is faceting (small multiples) better than plotting everything on one chart?",
      "tags": [
        "faceting",
        "visualization"
      ],
      "answers": [
        {
          "text": "When subgroups overlap or clutter a single plot; faceting reveals patterns per segment (class, region, time) with consistent axes.",
          "explanation": "Faceting improves readability and fair comparisons.",
          "category": "correct"
        },
        {
          "text": "When you need to compare segments using the same scale without overplotting.",
          "explanation": "Small multiples make subgroup differences easier to see.",
          "category": "correct"
        },
        {
          "text": "When you have many categories.",
          "explanation": "Often true because single plots become cluttered.",
          "category": "partially_correct"
        },
        {
          "text": "When the plot is too crowded.",
          "explanation": "Yes—faceting reduces overplotting and confusion.",
          "category": "partially_correct"
        },
        {
          "text": "Never—faceting always hides the overall trend.",
          "explanation": "You can show both overall and faceted views.",
          "category": "incorrect"
        },
        {
          "text": "Faceting is only for time series.",
          "explanation": "Faceting works for many stratification variables.",
          "category": "incorrect"
        },
        {
          "text": "Faceting makes the analysis invalid because it splits the data.",
          "explanation": "It’s a visualization technique that can improve interpretability.",
          "category": "incorrect"
        },
        {
          "text": "When you want the plots to feel less lonely.",
          "explanation": "It’s about clarity, not feelings.",
          "category": "ridiculous"
        },
        {
          "text": "When your chart needs to be cut into slices like pizza.",
          "explanation": "Faceting is for comparison, not culinary aesthetics.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Visualization best practices & pitfalls"
    },
    {
      "id": "142e33d82c2c6b98997930f533b46e17eec4b980b6ae73054b369f01a09015cd",
      "question": "What is one common visualization mistake that can hide class imbalance or subgroup effects?",
      "tags": [
        "bias",
        "stratification"
      ],
      "answers": [
        {
          "text": "Plotting aggregates without stratifying, or using raw counts without proportions, can hide minority patterns.",
          "explanation": "Aggregation can mask subgroup structure and imbalance.",
          "category": "correct"
        },
        {
          "text": "Using a single overall distribution when class-conditional distributions differ, masking minority behavior.",
          "explanation": "You often need class-conditional plots to see the true differences.",
          "category": "correct"
        },
        {
          "text": "Using the wrong chart type.",
          "explanation": "Sometimes, but the frequent root issue is lack of stratification/normalization.",
          "category": "partially_correct"
        },
        {
          "text": "Not labeling axes clearly.",
          "explanation": "That harms interpretation, though imbalance is often hidden by aggregation.",
          "category": "partially_correct"
        },
        {
          "text": "Using a legend.",
          "explanation": "Legends typically help interpretation.",
          "category": "incorrect"
        },
        {
          "text": "Using a bar chart always causes bias.",
          "explanation": "Bar charts can be fine if designed appropriately.",
          "category": "incorrect"
        },
        {
          "text": "Subgroup effects can’t be hidden because plots always show truth.",
          "explanation": "Plot design can absolutely hide key patterns.",
          "category": "incorrect"
        },
        {
          "text": "Making the minority class invisible to motivate it.",
          "explanation": "You should explicitly inspect minority distributions.",
          "category": "ridiculous"
        },
        {
          "text": "Turning the opacity to 0% for the hard parts.",
          "explanation": "Hiding data is the opposite of good EDA.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Visualization best practices & pitfalls"
    },
    {
      "id": "b938805c658c2eb9eea325235d06b1aff31aba64051e9098a5e8cb992ea0e6ef",
      "question": "What is class imbalance, and what EDA outputs would reveal it quickly?",
      "tags": [
        "class_imbalance",
        "classification"
      ],
      "answers": [
        {
          "text": "Class imbalance is when classes have very different frequencies; label counts/proportions and majority-class baseline accuracy reveal it quickly.",
          "explanation": "Imbalance affects training dynamics and evaluation metrics.",
          "category": "correct"
        },
        {
          "text": "A skewed label distribution; a simple bar chart of label counts plus baseline metrics exposes it immediately.",
          "explanation": "High accuracy can be misleading under imbalance.",
          "category": "correct"
        },
        {
          "text": "When one class has more examples than another.",
          "explanation": "Yes—the key is the degree and its impact.",
          "category": "partially_correct"
        },
        {
          "text": "When accuracy looks high but minority recall is low.",
          "explanation": "That can be a symptom of imbalance, though it’s not the definition.",
          "category": "partially_correct"
        },
        {
          "text": "It means the dataset has missing values.",
          "explanation": "Missingness is different from label distribution skew.",
          "category": "incorrect"
        },
        {
          "text": "It only matters for neural networks.",
          "explanation": "It affects many classifiers and metrics.",
          "category": "incorrect"
        },
        {
          "text": "Imbalance is fixed automatically if you normalize the features.",
          "explanation": "Feature scaling does not change class frequencies.",
          "category": "incorrect"
        },
        {
          "text": "When the classes are fighting and one wins.",
          "explanation": "It’s about frequency distribution, not conflict.",
          "category": "ridiculous"
        },
        {
          "text": "When the classes don’t balance their checkbooks.",
          "explanation": "It’s about counts, not finance.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Target-focused EDA & problem-specific diagnostics"
    },
    {
      "id": "de0e61e52f8be4bc2324a7e0d2768fa0fd75b4f4a5267c7093a5403c2a70fe65",
      "question": "How can label noise show up during EDA, and what’s one way to investigate it?",
      "tags": [
        "label_noise",
        "data_quality"
      ],
      "answers": [
        {
          "text": "It can appear as inconsistent feature-target patterns or high-loss outliers; investigate by auditing borderline cases and validating labeling rules/sources.",
          "explanation": "Label errors often manifest as contradictions in otherwise similar examples.",
          "category": "correct"
        },
        {
          "text": "If similar records have different labels or obvious contradictions appear, review suspect examples and cross-check the labeling process.",
          "explanation": "Sampling and audit are practical ways to confirm label quality.",
          "category": "correct"
        },
        {
          "text": "Low accuracy suggests label noise.",
          "explanation": "Possibly, but low accuracy can have many other causes.",
          "category": "partially_correct"
        },
        {
          "text": "The classes overlap a lot in plots.",
          "explanation": "Overlap can indicate noise, but also true difficulty.",
          "category": "partially_correct"
        },
        {
          "text": "Label noise is impossible if labels are integers.",
          "explanation": "Integer encoding doesn’t guarantee correctness.",
          "category": "incorrect"
        },
        {
          "text": "You fix label noise by scaling features.",
          "explanation": "Scaling doesn’t correct incorrect labels.",
          "category": "incorrect"
        },
        {
          "text": "Label noise is solved by adding more epochs.",
          "explanation": "Training longer can overfit noise rather than fix it.",
          "category": "incorrect"
        },
        {
          "text": "Label noise is when labels play loud music.",
          "explanation": "It means incorrect or inconsistent labels.",
          "category": "ridiculous"
        },
        {
          "text": "It’s when the labels have static like a radio.",
          "explanation": "It’s about mislabeling, not sound.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Target-focused EDA & problem-specific diagnostics"
    },
    {
      "id": "d8e1967c4ae06f218c4c68a953cddd70494223ab285efdaf3bc60f47995d2e7c",
      "question": "What is dataset shift, and how can you detect it using EDA between training and new data?",
      "tags": [
        "dataset_shift",
        "monitoring"
      ],
      "answers": [
        {
          "text": "Dataset shift is when feature or label distributions change between training and new data; detect it by comparing summary stats, histograms, and drift metrics.",
          "explanation": "Shift breaks the assumption that train and production are similar.",
          "category": "correct"
        },
        {
          "text": "It’s a change in the data-generating process; detect it via train-vs-new distribution comparisons, often stratified by key segments.",
          "explanation": "Shift can be global or localized to subgroups.",
          "category": "correct"
        },
        {
          "text": "When the data changes over time.",
          "explanation": "Often true, but shift can also come from population or pipeline changes.",
          "category": "partially_correct"
        },
        {
          "text": "When production data looks different than training data.",
          "explanation": "Yes—EDA comparisons make those differences visible.",
          "category": "partially_correct"
        },
        {
          "text": "Dataset shift only happens if you change the model.",
          "explanation": "Shift is about data, not the model.",
          "category": "incorrect"
        },
        {
          "text": "Shift can be ignored if training accuracy is high.",
          "explanation": "High training accuracy doesn’t protect against drift.",
          "category": "incorrect"
        },
        {
          "text": "If you random-split once, shift can never occur again.",
          "explanation": "Production drift can occur long after training.",
          "category": "incorrect"
        },
        {
          "text": "When the dataset physically moves to another folder.",
          "explanation": "It’s statistical drift, not a filesystem action.",
          "category": "ridiculous"
        },
        {
          "text": "When the dataset shifts into a higher gear.",
          "explanation": "Shift refers to distribution changes.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Target-focused EDA & problem-specific diagnostics"
    },
    {
      "id": "c5c7e0b4ea9796e682dbec217a1fb61e5042a552b381b62d9d1bc1f339395c3e",
      "question": "What simple baseline checks can EDA suggest before you train complex models?",
      "tags": [
        "baselines",
        "sanity_checks"
      ],
      "answers": [
        {
          "text": "Use a majority-class baseline, simple linear/logistic models, and sanity-check plots/metrics to confirm learnable signal and catch pipeline issues.",
          "explanation": "Baselines validate feasibility and detect leakage or preprocessing errors early.",
          "category": "correct"
        },
        {
          "text": "Run cheap baselines and sanity checks to validate feasibility and catch leakage/pipeline mistakes early.",
          "explanation": "Baselines provide a meaningful benchmark before investing in complex methods.",
          "category": "correct"
        },
        {
          "text": "Try a random forest first.",
          "explanation": "Could work, but a simpler baseline is often more diagnostic.",
          "category": "partially_correct"
        },
        {
          "text": "Pick a standard algorithm and see what happens.",
          "explanation": "Better than nothing, but you want interpretable, cheap checks first.",
          "category": "partially_correct"
        },
        {
          "text": "Skip baselines—go straight to deep learning.",
          "explanation": "Baselines are faster and often reveal issues sooner.",
          "category": "incorrect"
        },
        {
          "text": "Baselines are only for regression.",
          "explanation": "Baselines are essential for classification too.",
          "category": "incorrect"
        },
        {
          "text": "Baselines are useless once you have enough data.",
          "explanation": "Baselines still help detect errors and quantify improvement.",
          "category": "incorrect"
        },
        {
          "text": "Flip a coin and call it a baseline.",
          "explanation": "Baselines should be meaningful comparators.",
          "category": "ridiculous"
        },
        {
          "text": "Ask the GPU to guess the answer.",
          "explanation": "You need systematic, measurable baselines.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "Target-focused EDA & problem-specific diagnostics"
    },
    {
      "id": "c83c9568209d30432005188201bea6c70ce8d9e35ccb2bab3117aa8c53f169c6",
      "question": "Give an example of an EDA finding that suggests you should encode a feature differently.",
      "tags": [
        "encoding",
        "feature_engineering"
      ],
      "answers": [
        {
          "text": "If a categorical feature has many rare levels, use hashing/target/frequency encoding or group rare categories instead of one-hot.",
          "explanation": "One-hot on high-cardinality features can overfit and create huge sparse matrices.",
          "category": "correct"
        },
        {
          "text": "If a feature behaves like an identifier (near-unique values), avoid one-hot; consider hashing, grouping, or dropping if it’s pure ID noise.",
          "explanation": "ID-like features can cause leakage or memorize training examples.",
          "category": "correct"
        },
        {
          "text": "If it’s categorical, one-hot encode it.",
          "explanation": "Often works, but not ideal for high cardinality or rare-level tails.",
          "category": "partially_correct"
        },
        {
          "text": "Convert categories to integers.",
          "explanation": "This can accidentally impose an artificial ordering.",
          "category": "partially_correct"
        },
        {
          "text": "If a feature is numeric, one-hot encode it.",
          "explanation": "Continuous features generally shouldn’t be one-hot encoded.",
          "category": "incorrect"
        },
        {
          "text": "Encoding choice never matters if you use enough data.",
          "explanation": "Encoding affects bias/variance and model compatibility.",
          "category": "incorrect"
        },
        {
          "text": "Always sort categories alphabetically and map A=1, B=2…",
          "explanation": "This adds meaningless ordinal structure.",
          "category": "incorrect"
        },
        {
          "text": "Encode categories by their favorite color.",
          "explanation": "Encoding must reflect useful structure, not arbitrary attributes.",
          "category": "ridiculous"
        },
        {
          "text": "Assign each category a random emoji and train on that.",
          "explanation": "Random mappings don’t preserve meaningful information.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "EDA outputs → feature engineering & modeling decisions"
    },
    {
      "id": "dcd41c8b5d49f808eaf22e5de217bfa8ffa2f7407ea81797c7693da9291f94b8",
      "question": "Give an example of an EDA finding that suggests you should scale or normalize features.",
      "tags": [
        "normalization",
        "scaling"
      ],
      "answers": [
        {
          "text": "If features are on very different scales (e.g., dollars vs percentages) and you’ll use distance- or gradient-based models, apply scaling/normalization.",
          "explanation": "Many models are sensitive to relative feature scale (k-NN, SVM, linear models, neural nets).",
          "category": "correct"
        },
        {
          "text": "If one feature’s magnitude dominates others (0–1 vs 0–1,000,000), scale to prevent it from overwhelming optimization or distance calculations.",
          "explanation": "Scaling ensures features contribute comparably.",
          "category": "correct"
        },
        {
          "text": "Scale everything because scaling is always good.",
          "explanation": "Not always—tree models often don’t require scaling, and scaling can reduce interpretability.",
          "category": "partially_correct"
        },
        {
          "text": "Scale when you see very large numbers.",
          "explanation": "Magnitude alone isn’t enough; model sensitivity and distribution matter.",
          "category": "partially_correct"
        },
        {
          "text": "Scaling is only for categorical features.",
          "explanation": "Scaling applies to numeric magnitudes.",
          "category": "incorrect"
        },
        {
          "text": "Scaling changes the meaning of the target, so it’s invalid.",
          "explanation": "Scaling features is common; you generally don’t scale labels for classification.",
          "category": "incorrect"
        },
        {
          "text": "Scaling is pointless because models internally rescale everything.",
          "explanation": "Many models do not automatically correct for scale differences.",
          "category": "incorrect"
        },
        {
          "text": "Normalize by chanting ‘unit variance’ three times.",
          "explanation": "Normalization is a defined numerical transformation.",
          "category": "ridiculous"
        },
        {
          "text": "Just divide everything by 7 because it’s a lucky number.",
          "explanation": "Scaling must be principled and consistent.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain2-exploratory-data-analysis",
      "subtopic": "EDA outputs → feature engineering & modeling decisions"
    },
    {
      "id": "e889c8c606260f3442c21e93e59ad7aa0005dad8fe00722c9498f456c4742fc3",
      "question": "How do you translate a business objective into a machine learning problem statement?",
      "tags": [
        "problem_framing",
        "requirements"
      ],
      "answers": [
        {
          "text": "Define the decision, target, available inputs at decision time, constraints, and success metric.",
          "explanation": "A good ML statement connects business value to a specific prediction task, measurable outcomes, and real-world constraints.",
          "category": "correct"
        },
        {
          "text": "Convert the business goal into: prediction task + label definition + features available pre-decision + evaluation metric + operational constraints.",
          "explanation": "This ensures the modeling work matches how the system will be used in practice.",
          "category": "correct"
        },
        {
          "text": "Turn the goal into something you can predict.",
          "explanation": "This is a start, but you also need constraints, the label definition, and how success will be measured.",
          "category": "partially_correct"
        },
        {
          "text": "Write down what you want the model to guess.",
          "explanation": "Useful, but incomplete without defining the decision context and evaluation criteria.",
          "category": "partially_correct"
        },
        {
          "text": "Pick a model first, then decide what business problem it solves.",
          "explanation": "Model choice should follow problem definition, not the other way around.",
          "category": "incorrect"
        },
        {
          "text": "Start training immediately; the problem definition will emerge.",
          "explanation": "Skipping problem framing leads to wrong targets, leakage, and wasted training effort.",
          "category": "incorrect"
        },
        {
          "text": "Ask the CFO to label the data with vibes.",
          "explanation": "Labels must be grounded in measurable business outcomes, not subjective vibes.",
          "category": "ridiculous"
        },
        {
          "text": "Put the objective into a fortune cookie and read the model.",
          "explanation": "Problem framing is a structured requirements activity, not fortune-telling.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.1 Framing business problems as ML problems"
    },
    {
      "id": "58c4118cff7ebf26d451a2e3ca87c8a115e7a6fd6331900ae98dcb702a7a2971",
      "question": "What criteria help you decide whether a problem should be solved with supervised learning vs unsupervised learning?",
      "tags": [
        "learning_type",
        "supervised",
        "unsupervised"
      ],
      "answers": [
        {
          "text": "If you have labeled outcomes and want to predict them, use supervised learning; if you have no labels and want to discover structure (clusters/embeddings/anomalies), use unsupervised learning.",
          "explanation": "The presence of reliable labels is the key differentiator between supervised and unsupervised approaches.",
          "category": "correct"
        },
        {
          "text": "If you have ground-truth targets for training, it’s supervised; if you’re discovering patterns or segments without labels, it’s unsupervised.",
          "explanation": "Unsupervised learning is exploratory and structure-finding rather than direct outcome prediction.",
          "category": "correct"
        },
        {
          "text": "Supervised is for prediction, unsupervised is for grouping.",
          "explanation": "Generally true, but unsupervised can also target anomaly detection and representation learning.",
          "category": "partially_correct"
        },
        {
          "text": "Unsupervised means find clusters.",
          "explanation": "Clustering is one unsupervised task, but not the only one.",
          "category": "partially_correct"
        },
        {
          "text": "Unsupervised is always better because it needs less data.",
          "explanation": "Unsupervised still needs sufficient data and may not solve outcome-driven goals.",
          "category": "incorrect"
        },
        {
          "text": "Supervised only works for images; unsupervised is for everything else.",
          "explanation": "Both paradigms apply across domains like tabular, text, images, and time series.",
          "category": "incorrect"
        },
        {
          "text": "Supervised means your boss watches the training.",
          "explanation": "Supervision refers to labeled targets, not human monitoring.",
          "category": "ridiculous"
        },
        {
          "text": "Unsupervised means the model is homeschooled.",
          "explanation": "Unsupervised means no labels, not a schooling metaphor.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.1 Framing business problems as ML problems"
    },
    {
      "id": "12088deb4ef06b7cdd890f0ff8b69e3c75882f3174fc5dd6380cf4d390680898",
      "question": "What is the difference between a prediction target and a proxy target, and when would you use a proxy?",
      "tags": [
        "target",
        "proxy_label"
      ],
      "answers": [
        {
          "text": "A prediction target is the true outcome you care about; a proxy target is a measurable substitute used when the true target is unavailable, delayed, or expensive.",
          "explanation": "Proxies enable learning when direct labels are missing or too slow to arrive.",
          "category": "correct"
        },
        {
          "text": "Use a proxy when the real outcome is delayed or hard to measure (e.g., churn proxy = no login in 30 days).",
          "explanation": "Good proxies correlate with the true outcome and are available at training time.",
          "category": "correct"
        },
        {
          "text": "A proxy is a stand-in label.",
          "explanation": "Yes, but it should be chosen carefully to avoid training the wrong behavior.",
          "category": "partially_correct"
        },
        {
          "text": "Proxy means approximate target.",
          "explanation": "Correct in spirit, but you still need to validate the proxy’s alignment with business goals.",
          "category": "partially_correct"
        },
        {
          "text": "A proxy target is just noise you add to regularize.",
          "explanation": "A proxy target is a substitute label, not injected noise.",
          "category": "incorrect"
        },
        {
          "text": "A proxy target means you predict a feature instead of the label by default.",
          "explanation": "Proxy targets are intentional substitutes for the true target, not arbitrary feature prediction.",
          "category": "incorrect"
        },
        {
          "text": "A proxy target is when you hire someone else to do the predicting.",
          "explanation": "Proxy refers to a measurable substitute label, not outsourced work.",
          "category": "ridiculous"
        },
        {
          "text": "A proxy target is a label wearing sunglasses.",
          "explanation": "Proxies are about measurability and timing, not costumes.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.1 Framing business problems as ML problems"
    },
    {
      "id": "5ec5d2dcfc8a2e10c559610c97d10451702102c6f51397d3922db367b05d1b9d",
      "question": "How do you define success metrics for an ML project so they align with business outcomes?",
      "tags": [
        "metrics",
        "business_alignment"
      ],
      "answers": [
        {
          "text": "Choose metrics that reflect business cost/benefit of errors (profit lift, cost saved) plus operational constraints (latency, coverage, fairness).",
          "explanation": "The best metric is the one that captures the real value and risk of predictions.",
          "category": "correct"
        },
        {
          "text": "Define success in terms of decision quality (expected value, risk reduced) plus constraints like throughput, stability, and compliance.",
          "explanation": "A model can be statistically good but operationally unusable without constraint-aware metrics.",
          "category": "correct"
        },
        {
          "text": "Pick accuracy (or RMSE) as the main metric.",
          "explanation": "Sometimes reasonable, but often misaligned if costs are asymmetric or classes are imbalanced.",
          "category": "partially_correct"
        },
        {
          "text": "Use model score as the metric.",
          "explanation": "You need a specific measurable objective, not a vague notion of score.",
          "category": "partially_correct"
        },
        {
          "text": "Use whatever metric gives the highest score.",
          "explanation": "This encourages gaming the evaluation rather than optimizing real business outcomes.",
          "category": "incorrect"
        },
        {
          "text": "Only track training loss; business results will follow automatically.",
          "explanation": "Training loss is not a business KPI and may not reflect real-world performance.",
          "category": "incorrect"
        },
        {
          "text": "Success is when the chart looks upward.",
          "explanation": "Visual trends are not reliable definitions of business success.",
          "category": "ridiculous"
        },
        {
          "text": "Success is when the model gets a standing ovation.",
          "explanation": "Success must be defined using measurable outcomes.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.1 Framing business problems as ML problems"
    },
    {
      "id": "bcfd3e39798b276cdabe5787bc1e7859d2aeaf6ce4eaa82f249ca41f96cc7b4e",
      "question": "Why is establishing a baseline model important before investing in complex modeling?",
      "tags": [
        "baseline",
        "benchmarking"
      ],
      "answers": [
        {
          "text": "It sets a minimum bar, validates signal, catches pipeline issues early, and quantifies ROI of added complexity.",
          "explanation": "Baselines prevent over-investing in complexity when simple methods already work—or when nothing works.",
          "category": "correct"
        },
        {
          "text": "Baselines tell you whether the project is feasible and provide a reference to justify extra effort.",
          "explanation": "Without a baseline, you can’t measure true improvement.",
          "category": "correct"
        },
        {
          "text": "It’s a quick model to start with.",
          "explanation": "True, but the main value is benchmarking and risk reduction.",
          "category": "partially_correct"
        },
        {
          "text": "It gives you something to show early.",
          "explanation": "Yes, but it also provides diagnostic information and a reality check.",
          "category": "partially_correct"
        },
        {
          "text": "Baselines are only for beginners and can be skipped.",
          "explanation": "Even experts rely on baselines to validate assumptions.",
          "category": "incorrect"
        },
        {
          "text": "If you plan to use deep learning, baselines are irrelevant.",
          "explanation": "Deep models often fail to beat strong baselines on tabular data or small datasets.",
          "category": "incorrect"
        },
        {
          "text": "Because the baseline model is morally superior.",
          "explanation": "Baselines are a methodological tool, not an ethical statement.",
          "category": "ridiculous"
        },
        {
          "text": "Baseline is required by the Machine Learning Police.",
          "explanation": "No—baselines are a best practice for measurement and sanity checking.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.1 Framing business problems as ML problems"
    },
    {
      "id": "557c0b56870c1a5168168de68588f91bfb203d9f69404edcd7977ebdbf410260",
      "question": "What is the risk of label leakage when defining features and the target in a business context?",
      "tags": [
        "leakage",
        "target_definition"
      ],
      "answers": [
        {
          "text": "Including post-outcome or target-derived information in features can inflate validation performance and fail in production.",
          "explanation": "Leakage gives the model access to information it won’t have at prediction time.",
          "category": "correct"
        },
        {
          "text": "Leakage occurs when a feature encodes future information or the label itself, producing unrealistically high metrics.",
          "explanation": "These models collapse when deployed because the leaked signal disappears.",
          "category": "correct"
        },
        {
          "text": "Leakage makes the model too accurate.",
          "explanation": "It can look accurate in validation, but it’s not real predictive skill.",
          "category": "partially_correct"
        },
        {
          "text": "Leakage makes validation look great.",
          "explanation": "Yes, but the key point is that it breaks real-world performance.",
          "category": "partially_correct"
        },
        {
          "text": "Leakage is fine if cross-validation is used.",
          "explanation": "Cross-validation does not fix leakage; it just spreads it.",
          "category": "incorrect"
        },
        {
          "text": "Leakage is only possible if the label column is duplicated.",
          "explanation": "Leakage often happens via proxy features or future-derived fields.",
          "category": "incorrect"
        },
        {
          "text": "Leakage is when your GPU coolant drips on the dataset.",
          "explanation": "Leakage is about information contamination, not liquids.",
          "category": "ridiculous"
        },
        {
          "text": "The label leaks out of the monitor.",
          "explanation": "Leakage refers to feature/target information flow, not hardware.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.1 Framing business problems as ML problems"
    },
    {
      "id": "db5ba23dec64a865fb2e3cf864d2e345c0ca0082cf26dc7de5e65a7aa8c170b0",
      "question": "How do cost-of-error considerations influence the choice of evaluation metric and decision threshold?",
      "tags": [
        "cost_sensitive",
        "thresholding",
        "metrics"
      ],
      "answers": [
        {
          "text": "Choose metrics and thresholds that minimize expected cost (e.g., emphasize recall when false negatives are costly).",
          "explanation": "Different errors have different real-world consequences, so optimize accordingly.",
          "category": "correct"
        },
        {
          "text": "Set thresholds to meet business constraints (precision ≥ X, recall ≥ Y) or maximize expected profit/value.",
          "explanation": "Thresholding is a decision policy layer tuned to objectives and constraints.",
          "category": "correct"
        },
        {
          "text": "Change the threshold if you want more positives.",
          "explanation": "Correct, but the decision should be guided by cost/benefit tradeoffs.",
          "category": "partially_correct"
        },
        {
          "text": "Use a different metric if accuracy seems unfair.",
          "explanation": "You should select metrics based on costs and priorities, not perceived fairness alone.",
          "category": "partially_correct"
        },
        {
          "text": "Threshold is irrelevant if accuracy is high.",
          "explanation": "A high accuracy model can still be useless if the operating point is wrong.",
          "category": "incorrect"
        },
        {
          "text": "Always use threshold 0.5; it’s standard.",
          "explanation": "0.5 is arbitrary unless it matches costs and class priors.",
          "category": "incorrect"
        },
        {
          "text": "Set the threshold to 0.5 because it’s in the middle, like balance.",
          "explanation": "Balanced does not mean optimal for business costs.",
          "category": "ridiculous"
        },
        {
          "text": "Use the company’s founding year as the threshold.",
          "explanation": "Thresholds are probabilities/scores, not arbitrary numbers.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.1 Framing business problems as ML problems"
    },
    {
      "id": "a0cc2a04e60945cb09a663ddfdf733926da0162b55d27271b3bb7e128c36d06f",
      "question": "What is an example of a business question that is better framed as ranking than classification?",
      "tags": [
        "ranking",
        "problem_framing"
      ],
      "answers": [
        {
          "text": "Prioritizing which leads to contact first by expected value or conversion likelihood is a ranking problem, not simple yes/no classification.",
          "explanation": "Ranking optimizes ordering for limited resources.",
          "category": "correct"
        },
        {
          "text": "Resource allocation problems (fraud review queues, recommendations) are often better solved by ranking than binary labels.",
          "explanation": "You usually care about top-N decisions rather than all-or-nothing classification.",
          "category": "correct"
        },
        {
          "text": "Search results ordering.",
          "explanation": "Yes—ranking is the core of retrieval and search.",
          "category": "partially_correct"
        },
        {
          "text": "Top-N recommendations.",
          "explanation": "Correct—recommendation systems often optimize ranking.",
          "category": "partially_correct"
        },
        {
          "text": "Predicting house prices is a ranking problem.",
          "explanation": "That’s typically regression, though it can be converted to ranking in some setups.",
          "category": "incorrect"
        },
        {
          "text": "Sentiment analysis is always ranking, not classification.",
          "explanation": "Sentiment is commonly classification (or regression for sentiment score).",
          "category": "incorrect"
        },
        {
          "text": "Ranking which spreadsheet tab is the most confident.",
          "explanation": "Ranking problems are about items in a decision queue, not UI tabs.",
          "category": "ridiculous"
        },
        {
          "text": "Rank models by how cool their names sound.",
          "explanation": "Ranking should reflect utility, not aesthetics.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.1 Framing business problems as ML problems"
    },
    {
      "id": "a5c36464c083da5c3341646b05af30c9ea86df03b4bfac119b7b860e18958d01",
      "question": "For tabular data, what tradeoffs typically separate linear models from tree-based models?",
      "tags": [
        "model_selection",
        "linear_models",
        "trees"
      ],
      "answers": [
        {
          "text": "Linear models are simpler and interpretable but need feature engineering for nonlinearities; tree models capture nonlinearities/interactions with less preprocessing but require tuning to control overfitting.",
          "explanation": "Model choice balances interpretability, flexibility, preprocessing, and robustness.",
          "category": "correct"
        },
        {
          "text": "Linear models assume largely additive/linear effects, while trees can model complex splits and interactions but can be less stable without careful validation.",
          "explanation": "Trees often perform better on complex tabular structure; linear models excel when relationships are close to linear or engineered.",
          "category": "correct"
        },
        {
          "text": "Trees are more powerful than linear.",
          "explanation": "Often true for nonlinear tabular patterns, but linear models can be competitive and more interpretable.",
          "category": "partially_correct"
        },
        {
          "text": "Linear is simpler; trees are more flexible.",
          "explanation": "Good summary, though the practical implications include feature engineering and tuning needs.",
          "category": "partially_correct"
        },
        {
          "text": "Linear models always win if you standardize.",
          "explanation": "Scaling helps optimization but doesn’t make linear models capture nonlinearity.",
          "category": "incorrect"
        },
        {
          "text": "Trees can’t model interactions.",
          "explanation": "Trees naturally capture interactions through hierarchical splits.",
          "category": "incorrect"
        },
        {
          "text": "Trees are better because they grow.",
          "explanation": "Model quality depends on data and objectives, not biology metaphors.",
          "category": "ridiculous"
        },
        {
          "text": "Linear models are better because lines are straight and honest.",
          "explanation": "Honesty isn’t a modeling criterion; fit and generalization are.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.2 Model selection & problem–algorithm fit"
    },
    {
      "id": "f874e254314b78e9481ff006ea19fc87262c7d3ed43162a501c6cb00c7924efd",
      "question": "When might logistic regression be preferred over a more complex model for classification?",
      "tags": [
        "logistic_regression",
        "interpretability"
      ],
      "answers": [
        {
          "text": "When interpretability, calibration, speed, or limited data matters—and the decision boundary is roughly linear or can be made linear with features.",
          "explanation": "Logistic regression is a strong baseline that’s explainable and often well-calibrated.",
          "category": "correct"
        },
        {
          "text": "When you need a transparent, stable model and the data doesn’t require complex nonlinear interactions to perform well.",
          "explanation": "Complex models can overfit or be harder to deploy and explain.",
          "category": "correct"
        },
        {
          "text": "When you want something simple.",
          "explanation": "Simplicity is valuable, but the key is whether simplicity is sufficient for the structure of the data.",
          "category": "partially_correct"
        },
        {
          "text": "When you don’t have time to tune.",
          "explanation": "True, but logistics still requires basic validation and regularization.",
          "category": "partially_correct"
        },
        {
          "text": "Only when accuracy doesn’t matter.",
          "explanation": "Logistic regression can be highly accurate for many problems.",
          "category": "incorrect"
        },
        {
          "text": "Logistic regression is obsolete; never use it.",
          "explanation": "It remains widely used due to interpretability and strong baseline performance.",
          "category": "incorrect"
        },
        {
          "text": "When you like the word ‘logistic’.",
          "explanation": "Choice should be based on fit, constraints, and objectives.",
          "category": "ridiculous"
        },
        {
          "text": "Because it sounds like shipping and trucks.",
          "explanation": "Naming doesn’t determine suitability.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.2 Model selection & problem–algorithm fit"
    },
    {
      "id": "4bceb98511693fd54da6a71fec4e4e9e8162fdc5584e503bb0c36cc5c6accd56",
      "question": "What problem characteristics make gradient-boosted trees a strong default choice on tabular data?",
      "tags": [
        "gradient_boosting",
        "tabular"
      ],
      "answers": [
        {
          "text": "They handle nonlinearities, interactions, and mixed feature types and often perform well with limited feature engineering.",
          "explanation": "Boosted trees are a strong general-purpose choice for structured/tabular data.",
          "category": "correct"
        },
        {
          "text": "They iteratively correct errors and can capture complex patterns while remaining effective on modest-sized datasets.",
          "explanation": "The sequential boosting process often yields high accuracy on tabular tasks.",
          "category": "correct"
        },
        {
          "text": "They’re good at accuracy.",
          "explanation": "Yes, but the reason is their ability to capture nonlinear structure and interactions.",
          "category": "partially_correct"
        },
        {
          "text": "They’re popular in Kaggle.",
          "explanation": "True, but popularity reflects performance characteristics, not a guarantee.",
          "category": "partially_correct"
        },
        {
          "text": "They’re always best for images and text.",
          "explanation": "Boosted trees usually excel on tabular data; deep learning often dominates unstructured domains.",
          "category": "incorrect"
        },
        {
          "text": "They require no validation or tuning ever.",
          "explanation": "They still benefit from proper validation, regularization, and hyperparameter tuning.",
          "category": "incorrect"
        },
        {
          "text": "Because boosting makes the trees run faster.",
          "explanation": "Boosting is a learning strategy, not literal speed.",
          "category": "ridiculous"
        },
        {
          "text": "Trees get motivational speeches to improve.",
          "explanation": "They improve by optimizing an objective function.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.2 Model selection & problem–algorithm fit"
    },
    {
      "id": "b5058767a1221f07658f913431c6494038bc8cabf3849cfce501199ccb80bb9c",
      "question": "When would you choose a support vector machine (SVM) over logistic regression?",
      "tags": [
        "svm",
        "model_selection"
      ],
      "answers": [
        {
          "text": "When margin-based separation (especially with kernels) can model complex boundaries on small/medium datasets and the compute cost is manageable.",
          "explanation": "SVMs can perform well when a good kernel captures the boundary structure.",
          "category": "correct"
        },
        {
          "text": "When you suspect a nonlinear separator and want strong margin-based generalization with appropriate kernel and regularization (C).",
          "explanation": "SVMs can be effective with well-chosen kernels and tuning.",
          "category": "correct"
        },
        {
          "text": "When the data isn’t linearly separable.",
          "explanation": "Often true if you use kernels, but you must consider scaling and computational cost.",
          "category": "partially_correct"
        },
        {
          "text": "When you want a maximum margin classifier.",
          "explanation": "Correct intuition; margin maximization is central to SVMs.",
          "category": "partially_correct"
        },
        {
          "text": "When you need probabilities without calibration.",
          "explanation": "SVM outputs are not inherently calibrated probabilities.",
          "category": "incorrect"
        },
        {
          "text": "For huge datasets where kernel methods are too slow.",
          "explanation": "Kernel SVMs often do not scale well to very large datasets.",
          "category": "incorrect"
        },
        {
          "text": "When you want a model that sounds like a military unit.",
          "explanation": "Naming is not a valid selection criterion.",
          "category": "ridiculous"
        },
        {
          "text": "Because SVM looks cool on a slide.",
          "explanation": "Choose models based on performance and constraints.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.2 Model selection & problem–algorithm fit"
    },
    {
      "id": "55825b88215aa0681bc7f4c0633c57070b00de079633a657407aaa18ff4c0083",
      "question": "What is the difference between generative and discriminative classifiers, and when might each be preferred?",
      "tags": [
        "generative",
        "discriminative"
      ],
      "answers": [
        {
          "text": "Generative classifiers model p(x,y) (or p(x|y)p(y)) and can incorporate priors or handle missingness; discriminative classifiers model p(y|x) and often yield stronger predictive performance with enough data.",
          "explanation": "Generative models focus on how data is generated; discriminative models focus directly on boundaries.",
          "category": "correct"
        },
        {
          "text": "Generative models can be helpful in low-data or structured-probabilistic settings; discriminative models typically optimize classification accuracy more directly.",
          "explanation": "Preference depends on data availability, assumptions, and goals.",
          "category": "correct"
        },
        {
          "text": "Generative generates, discriminative classifies.",
          "explanation": "Generative means modeling the joint distribution, not necessarily generating samples in every use case.",
          "category": "partially_correct"
        },
        {
          "text": "Generative models the data distribution; discriminative models the label.",
          "explanation": "Close: discriminative models p(y|x), while generative models p(x|y) and p(y).",
          "category": "partially_correct"
        },
        {
          "text": "Generative always outperforms discriminative.",
          "explanation": "Discriminative models often win when you have sufficient labeled data.",
          "category": "incorrect"
        },
        {
          "text": "Discriminative can’t output probabilities.",
          "explanation": "Many discriminative models output probabilities directly or via calibration.",
          "category": "incorrect"
        },
        {
          "text": "Generative writes poems; discriminative grades them.",
          "explanation": "These are statistical modeling families, not creative arts.",
          "category": "ridiculous"
        },
        {
          "text": "Generative paints; discriminative judges art shows.",
          "explanation": "The distinction is mathematical and probabilistic.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.2 Model selection & problem–algorithm fit"
    },
    {
      "id": "2a77e8ed7fceb9ab77f2eb4f85708327ad624430dea843c5ceb3c7b9e76c4705",
      "question": "When is k-nearest neighbors (k-NN) a poor choice despite being simple to implement?",
      "tags": [
        "knn",
        "scalability"
      ],
      "answers": [
        {
          "text": "In high dimensions, on very large datasets (slow inference), with noisy/irrelevant features, or when distances aren’t meaningful.",
          "explanation": "k-NN relies on distance; scaling, dimensionality, and speed constraints can make it impractical.",
          "category": "correct"
        },
        {
          "text": "When you can’t define a meaningful distance metric or need low-latency prediction at scale.",
          "explanation": "k-NN often shifts cost to inference because it compares against many stored points.",
          "category": "correct"
        },
        {
          "text": "When you have lots of data.",
          "explanation": "Large datasets can make k-NN slow at inference unless you use approximations or indexing.",
          "category": "partially_correct"
        },
        {
          "text": "When features are messy.",
          "explanation": "Yes—distance-based methods can struggle with irrelevant/noisy features.",
          "category": "partially_correct"
        },
        {
          "text": "k-NN is always best because it uses the data directly.",
          "explanation": "It can be brittle and slow, and performance depends heavily on distance quality.",
          "category": "incorrect"
        },
        {
          "text": "k-NN works great even when all features are random noise.",
          "explanation": "If features have no signal, nearest neighbors won’t help.",
          "category": "incorrect"
        },
        {
          "text": "When your neighbors are untrustworthy.",
          "explanation": "Neighbors are data points; the issue is distance and signal.",
          "category": "ridiculous"
        },
        {
          "text": "When your neighborhood has bad Wi-Fi.",
          "explanation": "Network quality is unrelated to algorithm suitability.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.2 Model selection & problem–algorithm fit"
    },
    {
      "id": "680a8ea9c1d40e631a454330b1a873fff75da6ef9b1acba8779113a2e7598356",
      "question": "How do dataset size and feature dimensionality influence whether you choose deep learning vs classical ML?",
      "tags": [
        "deep_learning",
        "dataset_size",
        "dimensionality"
      ],
      "answers": [
        {
          "text": "Deep learning often needs more data and excels at representation learning for unstructured inputs; classical ML can outperform on smaller tabular datasets or when feature engineering is strong.",
          "explanation": "Model families have different data efficiency and inductive biases.",
          "category": "correct"
        },
        {
          "text": "Use deep learning when scale/structure supports it (images/text/audio, very large datasets); boosted trees/linear models often dominate smaller structured datasets.",
          "explanation": "Choice depends on data type, size, and how much feature learning is needed.",
          "category": "correct"
        },
        {
          "text": "Deep learning is better for big data.",
          "explanation": "Often true, but not universally—especially on tabular data.",
          "category": "partially_correct"
        },
        {
          "text": "Classical ML is better for small data.",
          "explanation": "Often true, though deep transfer learning can succeed with small labeled sets in some domains.",
          "category": "partially_correct"
        },
        {
          "text": "Deep learning is always better regardless of data size.",
          "explanation": "Deep models can overfit and may not beat simpler methods on small datasets.",
          "category": "incorrect"
        },
        {
          "text": "Classical ML can’t handle many features.",
          "explanation": "Many classical methods handle high-dimensional sparse data well.",
          "category": "incorrect"
        },
        {
          "text": "Use deep learning if you have deep feelings.",
          "explanation": "Use deep learning based on data structure and scale.",
          "category": "ridiculous"
        },
        {
          "text": "Use classical ML if you like classical music.",
          "explanation": "Preferences should be technical, not musical.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.2 Model selection & problem–algorithm fit"
    },
    {
      "id": "15d644afe7acbe16a3096cd0422250b78453c7ba69ec6daaf26f9662aff45cdf",
      "question": "What factors determine whether you should use multiclass classification vs multiple one-vs-rest binary classifiers?",
      "tags": [
        "multiclass",
        "one_vs_rest"
      ],
      "answers": [
        {
          "text": "Consider algorithm support, class relationships, class imbalance, calibration, and whether you need independent detectors for each class.",
          "explanation": "Some problems need mutually exclusive classes; others need separate detection per label.",
          "category": "correct"
        },
        {
          "text": "Use native multiclass when available and stable; use one-vs-rest when you need asymmetric class handling or independent score thresholds per class.",
          "explanation": "OVR can be useful when classes are imbalanced or not mutually exclusive in practice.",
          "category": "correct"
        },
        {
          "text": "One-vs-rest is just easier.",
          "explanation": "It can be easier conceptually, but may require careful calibration and thresholding.",
          "category": "partially_correct"
        },
        {
          "text": "Multiclass means more than two classes.",
          "explanation": "True, but the modeling choice depends on structure and costs.",
          "category": "partially_correct"
        },
        {
          "text": "One-vs-rest is always identical to multiclass.",
          "explanation": "They can yield different decision boundaries and calibration behavior.",
          "category": "incorrect"
        },
        {
          "text": "Multiclass cannot be evaluated with cross-validation.",
          "explanation": "Multiclass models can be cross-validated like any other.",
          "category": "incorrect"
        },
        {
          "text": "Pick one-vs-rest if you like arguments.",
          "explanation": "Choose based on task structure and model behavior.",
          "category": "ridiculous"
        },
        {
          "text": "Pick multiclass if you like parties.",
          "explanation": "This is not a modeling criterion.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.2 Model selection & problem–algorithm fit"
    },
    {
      "id": "8418f67aa59bed1211f82029e113a9274aab5f26af760df7a1a7f503d2af671f",
      "question": "How do interpretability requirements influence model selection?",
      "tags": [
        "interpretability",
        "model_selection"
      ],
      "answers": [
        {
          "text": "They push you toward transparent models (linear, GAMs, small trees) or explainable-by-design approaches; complex models may require post-hoc explanation.",
          "explanation": "In high-stakes settings, interpretability and auditability can be requirements.",
          "category": "correct"
        },
        {
          "text": "Regulated or high-impact decisions often require models with stable explanations and understandable feature effects.",
          "explanation": "Interpretability can be as important as raw accuracy.",
          "category": "correct"
        },
        {
          "text": "Choose a simpler model.",
          "explanation": "Often correct, but interpretability can also come from constraints, monotonicity, or model-specific explainers.",
          "category": "partially_correct"
        },
        {
          "text": "Use linear models for explainability.",
          "explanation": "Common approach, though other interpretable models exist.",
          "category": "partially_correct"
        },
        {
          "text": "Interpretability doesn’t matter if the model is accurate.",
          "explanation": "Many domains require justification, robustness, and auditability.",
          "category": "incorrect"
        },
        {
          "text": "Interpretability means the model is easy to train.",
          "explanation": "Interpretability refers to understanding predictions, not training difficulty.",
          "category": "incorrect"
        },
        {
          "text": "Interpretability is when you interpret the model’s dreams.",
          "explanation": "Interpretability is about explaining feature effects and decisions.",
          "category": "ridiculous"
        },
        {
          "text": "Ask the model to write an autobiography.",
          "explanation": "Explanations must be grounded in model behavior, not storytelling.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.2 Model selection & problem–algorithm fit"
    },
    {
      "id": "13a3d72165612d61be9f1b432d419037687a9595d9e4270e00b95c101c39cb7a",
      "question": "When does class imbalance meaningfully affect model choice (not just metrics)?",
      "tags": [
        "class_imbalance",
        "model_selection"
      ],
      "answers": [
        {
          "text": "When the algorithm is sensitive to class priors or minority recall is critical; choose models that support class weights, sampling, or specialized objectives.",
          "explanation": "Imbalance changes optimization and decision policy needs.",
          "category": "correct"
        },
        {
          "text": "If the minority class is extremely rare, consider anomaly detection, specialized losses, or methods robust to skew.",
          "explanation": "Some model families handle skew better or allow cost-sensitive learning.",
          "category": "correct"
        },
        {
          "text": "When one class is smaller.",
          "explanation": "Yes, but the impact depends on degree and business cost of errors.",
          "category": "partially_correct"
        },
        {
          "text": "When accuracy seems suspiciously high.",
          "explanation": "High accuracy under imbalance can indicate a trivial majority-class predictor.",
          "category": "partially_correct"
        },
        {
          "text": "Imbalance only changes how you report accuracy.",
          "explanation": "Imbalance can change training dynamics and optimal model design.",
          "category": "incorrect"
        },
        {
          "text": "Imbalance is fixed by feature normalization.",
          "explanation": "Scaling features does not change label distribution.",
          "category": "incorrect"
        },
        {
          "text": "Balance the dataset on a scale.",
          "explanation": "Imbalance is statistical, not physical.",
          "category": "ridiculous"
        },
        {
          "text": "Feed the minority class extra snacks.",
          "explanation": "You address imbalance via sampling, weighting, and evaluation.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.2 Model selection & problem–algorithm fit"
    },
    {
      "id": "1e1cc1fe580163bd2184fcb1b369c5a435a1fffb8086031154dff46c525afb9f",
      "question": "What are ensembles, and why do they often outperform single models?",
      "tags": [
        "ensembles",
        "generalization"
      ],
      "answers": [
        {
          "text": "Ensembles combine multiple learners to reduce variance and/or bias, improving generalization through diversity and aggregation.",
          "explanation": "Multiple imperfect models can average out individual errors.",
          "category": "correct"
        },
        {
          "text": "Aggregating diverse predictors yields more stable performance than relying on a single estimator.",
          "explanation": "Diversity plus aggregation often improves robustness.",
          "category": "correct"
        },
        {
          "text": "They’re multiple models together.",
          "explanation": "Yes, the key is how and why the combination improves performance.",
          "category": "partially_correct"
        },
        {
          "text": "Averaging predictions helps.",
          "explanation": "Often true, especially in bagging and some stacking setups.",
          "category": "partially_correct"
        },
        {
          "text": "Ensembles always overfit because they’re bigger.",
          "explanation": "Many ensembles reduce overfitting by averaging out variance.",
          "category": "incorrect"
        },
        {
          "text": "Ensembles only work for regression.",
          "explanation": "They work for classification and many other tasks.",
          "category": "incorrect"
        },
        {
          "text": "An ensemble is a group of models singing in harmony.",
          "explanation": "It’s a statistical aggregation, not a choir.",
          "category": "ridiculous"
        },
        {
          "text": "A model boy band.",
          "explanation": "It’s an engineering technique, not a music group.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.2 Model selection & problem–algorithm fit"
    },
    {
      "id": "77f3e6be375a4b5faf54d395dbebef9fdc79fabfd7f4a4c09fb4bfddef37b761",
      "question": "What is stacking vs bagging vs boosting, in practical terms?",
      "tags": [
        "ensembles",
        "bagging",
        "boosting",
        "stacking"
      ],
      "answers": [
        {
          "text": "Bagging trains many independent models in parallel and averages them; boosting trains sequentially to correct errors; stacking trains a meta-model to combine base model outputs.",
          "explanation": "They differ in how models are trained and combined.",
          "category": "correct"
        },
        {
          "text": "Bagging reduces variance, boosting often reduces bias by focusing on errors, and stacking learns the best mixture from data.",
          "explanation": "Understanding the mechanism helps choose the right ensemble strategy.",
          "category": "correct"
        },
        {
          "text": "Boosting is like stronger bagging.",
          "explanation": "They both combine models, but boosting is sequential and error-focused.",
          "category": "partially_correct"
        },
        {
          "text": "Bagging is like averaging lots of models.",
          "explanation": "Correct intuition for bagging.",
          "category": "partially_correct"
        },
        {
          "text": "Stacking means training one model on top of the raw data twice.",
          "explanation": "Stacking uses model predictions as inputs to a combiner model.",
          "category": "incorrect"
        },
        {
          "text": "Boosting and bagging are the same thing.",
          "explanation": "They are fundamentally different training strategies.",
          "category": "incorrect"
        },
        {
          "text": "Bagging is for groceries; boosting is for energy drinks.",
          "explanation": "These are ensemble training methods, not shopping habits.",
          "category": "ridiculous"
        },
        {
          "text": "Stacking pancakes; boosting syrup.",
          "explanation": "Fun metaphor, but not a real explanation.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.2 Model selection & problem–algorithm fit"
    },
    {
      "id": "cf4bea82c9639008f97623c0cb0e184b605fa26ab31d4f0f4ea90344c84016f5",
      "question": "When would you prefer a probabilistic model output (calibrated probabilities) over hard class predictions?",
      "tags": [
        "probabilities",
        "calibration",
        "decision_making"
      ],
      "answers": [
        {
          "text": "When decisions are cost-sensitive, require ranking, resource allocation, or expected-value calculations.",
          "explanation": "Probability outputs let you choose operating points and optimize downstream decisions.",
          "category": "correct"
        },
        {
          "text": "When downstream systems need reliable risk scores (triage, pricing, fraud review), calibrated probabilities are essential.",
          "explanation": "Calibration enables trustworthy decision policies.",
          "category": "correct"
        },
        {
          "text": "When you want confidence scores.",
          "explanation": "Yes, but those scores must be calibrated to be decision-useful.",
          "category": "partially_correct"
        },
        {
          "text": "When you need a score to sort items.",
          "explanation": "Ranking often depends on stable probability estimates.",
          "category": "partially_correct"
        },
        {
          "text": "Only when accuracy is low.",
          "explanation": "Even high-accuracy models may require probabilities for business decisions.",
          "category": "incorrect"
        },
        {
          "text": "Probabilities never matter if you can pick a threshold.",
          "explanation": "Threshold selection itself often depends on probability meaning and costs.",
          "category": "incorrect"
        },
        {
          "text": "When your manager likes decimals.",
          "explanation": "Decimals aren’t the reason; decision policies are.",
          "category": "ridiculous"
        },
        {
          "text": "Because probabilities look scientific.",
          "explanation": "You use probabilities because they support correct decisions.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.2 Model selection & problem–algorithm fit"
    },
    {
      "id": "3b05b123d48f14f4030266b4146326422d053016b935db325a12e8a7c50b12a1",
      "question": "What is the relationship between loss function, gradients, and parameter updates during training?",
      "tags": [
        "loss",
        "gradients",
        "optimization"
      ],
      "answers": [
        {
          "text": "The loss measures error; gradients are derivatives of the loss with respect to parameters; the optimizer updates parameters using gradients to reduce loss.",
          "explanation": "Training iteratively moves parameters in directions that reduce the objective.",
          "category": "correct"
        },
        {
          "text": "Training cycles through: compute predictions → compute loss → backpropagate gradients → apply updates (SGD/Adam) to lower loss.",
          "explanation": "This loop is the core of gradient-based learning.",
          "category": "correct"
        },
        {
          "text": "Gradients tell the model how to change.",
          "explanation": "Yes, they provide the direction and magnitude for updates relative to the loss.",
          "category": "partially_correct"
        },
        {
          "text": "Loss guides learning.",
          "explanation": "Correct, but gradients connect loss to parameter updates.",
          "category": "partially_correct"
        },
        {
          "text": "Gradients are the loss values per row.",
          "explanation": "Gradients are derivatives of the loss, not loss values themselves.",
          "category": "incorrect"
        },
        {
          "text": "Gradients are random numbers the optimizer invents.",
          "explanation": "Gradients are computed from the model and loss, not chosen arbitrarily.",
          "category": "incorrect"
        },
        {
          "text": "Gradients are the model’s mood swings.",
          "explanation": "They’re mathematical derivatives.",
          "category": "ridiculous"
        },
        {
          "text": "Gradients are tiny gremlins pushing weights around.",
          "explanation": "They’re computed signals guiding optimization.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.3 Training fundamentals & optimization behavior"
    },
    {
      "id": "3974986d53e997b06a26ac96206aed24ab66457749dd479ec52564dd96ddcf35",
      "question": "How do batch size and learning rate interact to affect convergence and stability?",
      "tags": [
        "batch_size",
        "learning_rate",
        "convergence"
      ],
      "answers": [
        {
          "text": "Large batches reduce gradient noise and may allow larger learning rates but can affect generalization; small batches add noise and may require smaller learning rates for stability.",
          "explanation": "Batch size affects gradient variance; learning rate controls step size.",
          "category": "correct"
        },
        {
          "text": "Batch size changes the variance of the gradient estimate, while learning rate sets update magnitude—together they determine convergence speed and oscillation risk.",
          "explanation": "Poor combinations cause divergence or slow learning.",
          "category": "correct"
        },
        {
          "text": "Big batch is faster; small batch is slower.",
          "explanation": "Often, but the more important issue is stability, noise, and generalization.",
          "category": "partially_correct"
        },
        {
          "text": "Small batch is noisy.",
          "explanation": "Yes—noise can help escape sharp minima but can also destabilize training.",
          "category": "partially_correct"
        },
        {
          "text": "Batch size doesn’t affect convergence.",
          "explanation": "It affects gradient noise and optimization behavior.",
          "category": "incorrect"
        },
        {
          "text": "Learning rate and batch size are unrelated knobs.",
          "explanation": "They interact strongly in practice.",
          "category": "incorrect"
        },
        {
          "text": "Batch size is the number of snacks you eat while training.",
          "explanation": "Batch size refers to samples per gradient update.",
          "category": "ridiculous"
        },
        {
          "text": "Batch size is how many GPUs you pet for luck.",
          "explanation": "It’s a data/optimization setting, not a ritual.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.3 Training fundamentals & optimization behavior"
    },
    {
      "id": "e58b9a0e7170bad9e88d1f61dfc798a50871f546c8c7c907f011a79eafec8727",
      "question": "What is an epoch, and why is “more epochs” not always better?",
      "tags": [
        "epochs",
        "overfitting"
      ],
      "answers": [
        {
          "text": "An epoch is one full pass through the training data; too many epochs can overfit and worsen validation performance.",
          "explanation": "Training longer can memorize noise instead of learning general patterns.",
          "category": "correct"
        },
        {
          "text": "More epochs can keep decreasing training loss while validation stops improving—so you stop based on validation signals.",
          "explanation": "Generalization, not training loss, is the goal.",
          "category": "correct"
        },
        {
          "text": "An epoch is one training loop.",
          "explanation": "More precisely, it’s one pass through the entire training dataset.",
          "category": "partially_correct"
        },
        {
          "text": "One run over the dataset.",
          "explanation": "Yes, that’s the standard definition.",
          "category": "partially_correct"
        },
        {
          "text": "More epochs always improves test accuracy.",
          "explanation": "After a point, more training often increases overfitting.",
          "category": "incorrect"
        },
        {
          "text": "Epochs only matter for deep learning, not other models.",
          "explanation": "Any iterative training process can have epoch-like iterations.",
          "category": "incorrect"
        },
        {
          "text": "Epochs are historical periods of your model’s life.",
          "explanation": "They are training passes, not eras.",
          "category": "ridiculous"
        },
        {
          "text": "Epochs are when the model levels up like a video game.",
          "explanation": "Training progress must be validated, not assumed.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.3 Training fundamentals & optimization behavior"
    },
    {
      "id": "ab0460fff7582a74f4aedf766769ff9874993fdc55fa09982bb8bfa53f5c03bb",
      "question": "What is the difference between training loss and validation loss, and how should you interpret a widening gap?",
      "tags": [
        "train_validation",
        "overfitting",
        "generalization"
      ],
      "answers": [
        {
          "text": "Training loss measures fit on training data; validation loss measures generalization. A widening gap often indicates overfitting or a distribution mismatch.",
          "explanation": "A model can fit training data well while generalizing poorly.",
          "category": "correct"
        },
        {
          "text": "If training improves but validation stalls or worsens, the model is learning training-specific patterns; apply regularization, early stopping, or improve data.",
          "explanation": "The goal is low error on unseen data, not just training data.",
          "category": "correct"
        },
        {
          "text": "The model is learning the training set.",
          "explanation": "Yes, but the key is that it may not generalize.",
          "category": "partially_correct"
        },
        {
          "text": "Validation isn’t improving.",
          "explanation": "True; interpret why and consider interventions.",
          "category": "partially_correct"
        },
        {
          "text": "A widening gap proves the model is unbiased.",
          "explanation": "Bias is not measured by train/validation loss divergence.",
          "category": "incorrect"
        },
        {
          "text": "A widening gap means you should increase model complexity every time.",
          "explanation": "More complexity often worsens overfitting.",
          "category": "incorrect"
        },
        {
          "text": "The losses are drifting apart because they need couples therapy.",
          "explanation": "It indicates a statistical generalization issue, not emotions.",
          "category": "ridiculous"
        },
        {
          "text": "They drift because of lunar tides.",
          "explanation": "It’s caused by data/model fit, not the moon.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.3 Training fundamentals & optimization behavior"
    },
    {
      "id": "da852f424e5d40a206a152f8ca05d0c7c26e18304502e18071f36796d7e9e63e",
      "question": "Why can feature scaling dramatically affect training speed for gradient-based methods?",
      "tags": [
        "feature_scaling",
        "gradient_descent"
      ],
      "answers": [
        {
          "text": "Different feature scales can create an ill-conditioned optimization landscape; scaling balances gradients and improves convergence speed.",
          "explanation": "Scaling reduces zig-zagging and enables more stable step sizes.",
          "category": "correct"
        },
        {
          "text": "Without scaling, one feature can dominate gradient updates, causing slow or unstable optimization; scaling makes updates more uniform.",
          "explanation": "Balanced scales help gradient descent find good directions efficiently.",
          "category": "correct"
        },
        {
          "text": "Scaling makes numbers smaller.",
          "explanation": "It changes magnitude, but the key is conditioning and gradient balance.",
          "category": "partially_correct"
        },
        {
          "text": "Scaling helps the optimizer.",
          "explanation": "Yes, especially for gradient-based optimization.",
          "category": "partially_correct"
        },
        {
          "text": "Scaling only affects tree models.",
          "explanation": "Trees are usually scale-insensitive; gradient-based models are often scale-sensitive.",
          "category": "incorrect"
        },
        {
          "text": "Scaling changes the label, so it ruins training.",
          "explanation": "Feature scaling affects inputs, not labels (for classification).",
          "category": "incorrect"
        },
        {
          "text": "Scaling makes the optimizer feel taller.",
          "explanation": "Scaling affects the geometry of optimization.",
          "category": "ridiculous"
        },
        {
          "text": "Scaling gives gradients better posture.",
          "explanation": "It improves conditioning, not posture.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.3 Training fundamentals & optimization behavior"
    },
    {
      "id": "947283a44e8294a5fc32258f02160f51a5f84c0872fbded44237c4cf7f097341",
      "question": "What is the vanishing/exploding gradient problem, and what are common mitigations?",
      "tags": [
        "vanishing_gradient",
        "exploding_gradient",
        "deep_learning"
      ],
      "answers": [
        {
          "text": "Gradients shrink or grow dramatically through many layers; mitigations include careful initialization, normalization, residual connections, suitable activations, and gradient clipping.",
          "explanation": "Stabilizing gradient flow is critical in deep networks.",
          "category": "correct"
        },
        {
          "text": "Deep networks can produce tiny or huge gradients that stall or diverge training; use batch/layer norm, residuals, ReLU-family activations, and clipping.",
          "explanation": "These techniques help maintain usable gradient magnitudes.",
          "category": "correct"
        },
        {
          "text": "Gradients get too small or too big.",
          "explanation": "Correct definition, but you should also know causes and fixes.",
          "category": "partially_correct"
        },
        {
          "text": "It’s a deep network training problem.",
          "explanation": "Mostly, though instability can appear in other iterative systems too.",
          "category": "partially_correct"
        },
        {
          "text": "It happens only in logistic regression.",
          "explanation": "It’s primarily a deep network issue due to repeated composition.",
          "category": "incorrect"
        },
        {
          "text": "It’s fixed by increasing epochs alone.",
          "explanation": "More epochs won’t fix unstable gradients; you need architectural/optimization changes.",
          "category": "incorrect"
        },
        {
          "text": "Exploding gradients are when your laptop fan reaches orbit.",
          "explanation": "They’re numerical instabilities, not propulsion.",
          "category": "ridiculous"
        },
        {
          "text": "Gradients explode like fireworks.",
          "explanation": "They can blow up numerically, but not literally.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.3 Training fundamentals & optimization behavior"
    },
    {
      "id": "cf0a349a90afbf299e092011c710b9e97a879ce2e475a489ca0d5422ed3d3cdb",
      "question": "Why can random initialization affect training outcomes, and how do you control for it?",
      "tags": [
        "random_seed",
        "initialization",
        "reproducibility"
      ],
      "answers": [
        {
          "text": "Non-convex optimization can converge to different solutions depending on initialization; control with fixed seeds, multiple runs, and stable training procedures.",
          "explanation": "Randomness can change which local minimum you end up in.",
          "category": "correct"
        },
        {
          "text": "Initialization can change early optimization trajectories; assess variability across runs and report mean/variance or select stable configurations.",
          "explanation": "Reproducibility and robustness require managing stochasticity.",
          "category": "correct"
        },
        {
          "text": "Randomness changes results; set a seed.",
          "explanation": "Yes, seeds help reproduce a run, but you may still want multiple runs for robustness.",
          "category": "partially_correct"
        },
        {
          "text": "Set a seed.",
          "explanation": "Good for reproducibility, though not a guarantee of best performance.",
          "category": "partially_correct"
        },
        {
          "text": "Initialization doesn’t matter if you train long enough.",
          "explanation": "Different initializations can converge to different solutions.",
          "category": "incorrect"
        },
        {
          "text": "Seeds guarantee the best possible accuracy.",
          "explanation": "Seeds reproduce results; they don’t guarantee optimality.",
          "category": "incorrect"
        },
        {
          "text": "Random init works best during a full moon.",
          "explanation": "Performance depends on optimization and data, not astrology.",
          "category": "ridiculous"
        },
        {
          "text": "Initialize with horoscope signs.",
          "explanation": "Initialization should be principled and reproducible.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.3 Training fundamentals & optimization behavior"
    },
    {
      "id": "8c0694072c67f86b808493f5420e6f8cfb0ecd29808c64337b9e081311729ee1",
      "question": "What is early stopping, and what signal is it watching for?",
      "tags": [
        "early_stopping",
        "validation_loss"
      ],
      "answers": [
        {
          "text": "Early stopping halts training when validation performance stops improving (often validation loss), preventing overfitting.",
          "explanation": "It uses a held-out signal to stop before memorization.",
          "category": "correct"
        },
        {
          "text": "Monitor validation metrics with patience and stop when improvements plateau or reverse.",
          "explanation": "Patience reduces sensitivity to noise in validation measurements.",
          "category": "correct"
        },
        {
          "text": "Stop when good enough.",
          "explanation": "The key is defining good enough using validation behavior.",
          "category": "partially_correct"
        },
        {
          "text": "Stop when validation stops improving.",
          "explanation": "Yes—usually with a patience window.",
          "category": "partially_correct"
        },
        {
          "text": "Stop when training loss reaches zero.",
          "explanation": "Training loss can drop while validation worsens.",
          "category": "incorrect"
        },
        {
          "text": "Stop after a fixed number of epochs no matter what.",
          "explanation": "Fixed stopping can underfit or overfit; validation-driven stopping is safer.",
          "category": "incorrect"
        },
        {
          "text": "Early stopping is when you unplug the server mid-epoch.",
          "explanation": "It’s a controlled training strategy, not sabotage.",
          "category": "ridiculous"
        },
        {
          "text": "Stop when the model sounds tired.",
          "explanation": "Use metrics, not feelings.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.3 Training fundamentals & optimization behavior"
    },
    {
      "id": "f5292bfdd4e47cd17b259fdcc7d2df2ae5a5a73b7989049518a91e8635c53ab1",
      "question": "How can label noise impact training dynamics and model generalization?",
      "tags": [
        "label_noise",
        "generalization"
      ],
      "answers": [
        {
          "text": "Label noise raises irreducible error, distorts the learning signal, encourages memorization, harms calibration, and reduces generalization.",
          "explanation": "The model may fit contradictions and lose real predictive power.",
          "category": "correct"
        },
        {
          "text": "Noisy labels can cause overfitting to errors and mislead tuning, producing brittle models that fail on clean real-world outcomes.",
          "explanation": "Better labeling or robust methods may be required.",
          "category": "correct"
        },
        {
          "text": "It lowers accuracy.",
          "explanation": "Often true, but it also affects calibration and training stability.",
          "category": "partially_correct"
        },
        {
          "text": "It makes training harder.",
          "explanation": "Yes—because gradients include misleading signals.",
          "category": "partially_correct"
        },
        {
          "text": "Label noise always helps regularize the model.",
          "explanation": "It often harms learning, especially when noise is systematic.",
          "category": "incorrect"
        },
        {
          "text": "It doesn’t matter if you have enough compute.",
          "explanation": "Compute can’t fix incorrect targets.",
          "category": "incorrect"
        },
        {
          "text": "Label noise is when labels make static sounds.",
          "explanation": "It means incorrect or inconsistent labels.",
          "category": "ridiculous"
        },
        {
          "text": "Labels are noisy because they’re yelling.",
          "explanation": "Noise is statistical mismatch, not volume.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.3 Training fundamentals & optimization behavior"
    },
    {
      "id": "21451ae13dc6a382228153cba0a5c980e6afdbed443c21847922fc6b94a4eb89",
      "question": "What is the practical difference between optimizing for accuracy vs optimizing for log loss?",
      "tags": [
        "accuracy",
        "log_loss",
        "objective"
      ],
      "answers": [
        {
          "text": "Accuracy measures thresholded correctness; log loss evaluates probability quality and penalizes overconfident wrong predictions.",
          "explanation": "Log loss encourages calibrated probabilities and better ranking behavior.",
          "category": "correct"
        },
        {
          "text": "Accuracy ignores confidence as long as predictions are correct; log loss cares about confidence and heavily penalizes confident mistakes.",
          "explanation": "Log loss is better when probabilities drive decisions.",
          "category": "correct"
        },
        {
          "text": "Log loss is stricter than accuracy.",
          "explanation": "Often, because it punishes overconfidence, not just mistakes.",
          "category": "partially_correct"
        },
        {
          "text": "Log loss penalizes confident mistakes.",
          "explanation": "Yes—this is a key practical difference.",
          "category": "partially_correct"
        },
        {
          "text": "They are equivalent metrics.",
          "explanation": "They measure different things and can rank models differently.",
          "category": "incorrect"
        },
        {
          "text": "Log loss is only for regression.",
          "explanation": "Log loss is commonly used for probabilistic classification.",
          "category": "incorrect"
        },
        {
          "text": "Log loss is accuracy written in a logbook.",
          "explanation": "It is a distinct probabilistic scoring rule.",
          "category": "ridiculous"
        },
        {
          "text": "Log loss is when you lose your logs.",
          "explanation": "It’s a mathematical objective, not forestry.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.3 Training fundamentals & optimization behavior"
    },
    {
      "id": "b2efeba35e8674bbbb4167829f31cf514a81656d2e2d99122d334d0c63a43fe3",
      "question": "What does “overfitting” mean operationally, and how can you detect it during training?",
      "tags": [
        "overfitting",
        "diagnostics"
      ],
      "answers": [
        {
          "text": "Overfitting is when training performance improves while validation performance degrades, indicating poor generalization; detect via learning curves and cross-validation instability.",
          "explanation": "The model has learned training-specific patterns that don’t transfer.",
          "category": "correct"
        },
        {
          "text": "Detect overfitting when validation loss rises or plateaus while training loss continues to fall.",
          "explanation": "This divergence is a classic operational signal.",
          "category": "correct"
        },
        {
          "text": "It memorizes the training data.",
          "explanation": "That’s the intuition; the operational sign is failure on unseen data.",
          "category": "partially_correct"
        },
        {
          "text": "It does great on train but not test.",
          "explanation": "Yes—generalization is the key issue.",
          "category": "partially_correct"
        },
        {
          "text": "Overfitting means the model is too accurate.",
          "explanation": "High training accuracy can coincide with poor real-world performance.",
          "category": "incorrect"
        },
        {
          "text": "Overfitting only happens with neural networks.",
          "explanation": "Any flexible model can overfit.",
          "category": "incorrect"
        },
        {
          "text": "Overfitting is when the model’s weights gain weight.",
          "explanation": "It’s about generalization error, not physical weight.",
          "category": "ridiculous"
        },
        {
          "text": "Overfitting is when the model overfits its clothes.",
          "explanation": "It’s a statistical phenomenon, not fashion.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "Regularization & generalization control"
    },
    {
      "id": "0d5730e27be5aa96658e3338bf48aa21ec9631b214cbe31e0e84828156dd177f",
      "question": "Compare L1 and L2 regularization: what behavior does each encourage in the learned weights?",
      "tags": [
        "l1",
        "l2",
        "regularization"
      ],
      "answers": [
        {
          "text": "L1 encourages sparsity (many weights become zero), while L2 shrinks weights smoothly and discourages large weights, improving stability.",
          "explanation": "L1 can act like feature selection; L2 reduces sensitivity to noise.",
          "category": "correct"
        },
        {
          "text": "L1 performs implicit feature selection by driving coefficients to zero; L2 keeps all features but reduces extreme coefficient magnitudes.",
          "explanation": "They impose different inductive biases on the solution.",
          "category": "correct"
        },
        {
          "text": "Both make weights smaller.",
          "explanation": "Yes, but L1 tends to zero out many weights, while L2 shrinks them continuously.",
          "category": "partially_correct"
        },
        {
          "text": "Both reduce overfitting.",
          "explanation": "Often true, but their effects on sparsity and stability differ.",
          "category": "partially_correct"
        },
        {
          "text": "L1 makes all weights equal; L2 makes them random.",
          "explanation": "Neither penalty forces equality or randomness.",
          "category": "incorrect"
        },
        {
          "text": "L1 and L2 are the same penalty.",
          "explanation": "They differ in geometry and the solutions they encourage.",
          "category": "incorrect"
        },
        {
          "text": "L1 is the lonely norm; L2 is the too-cool norm.",
          "explanation": "They are mathematical penalties, not personalities.",
          "category": "ridiculous"
        },
        {
          "text": "L1 is one love; L2 is two love.",
          "explanation": "Not a real distinction—focus on sparsity vs shrinkage.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "Regularization & generalization control"
    },
    {
      "id": "53c4d079acaeff5a48ff425ad5b38a1f9e5790bbb56a88fcfc494aab02eb74a2",
      "question": "What is dropout, and why does it help reduce overfitting in neural networks?",
      "tags": [
        "dropout",
        "regularization",
        "neural_networks"
      ],
      "answers": [
        {
          "text": "Dropout randomly zeros activations during training, reducing co-adaptation and encouraging redundant representations, which improves generalization.",
          "explanation": "It prevents reliance on specific pathways and behaves like training many subnetworks.",
          "category": "correct"
        },
        {
          "text": "It acts like an implicit ensemble of subnetworks by stochastically removing units during training.",
          "explanation": "This reduces overfitting by making representations more robust.",
          "category": "correct"
        },
        {
          "text": "It removes neurons so the model can’t memorize.",
          "explanation": "It doesn’t permanently remove neurons; it randomly drops them during training.",
          "category": "partially_correct"
        },
        {
          "text": "It adds randomness during training.",
          "explanation": "Yes—this randomness is what produces regularization.",
          "category": "partially_correct"
        },
        {
          "text": "Dropout is used at inference to increase randomness.",
          "explanation": "Dropout is typically disabled at inference (or handled via scaling).",
          "category": "incorrect"
        },
        {
          "text": "Dropout guarantees higher accuracy always.",
          "explanation": "It can help generalization, but not guaranteed for every dataset/model.",
          "category": "incorrect"
        },
        {
          "text": "Dropout is when your model rage-quits training.",
          "explanation": "It’s a controlled technique, not quitting.",
          "category": "ridiculous"
        },
        {
          "text": "Neurons take coffee breaks.",
          "explanation": "It’s a stochastic regularization method.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "Regularization & generalization control"
    },
    {
      "id": "e62367ade744dce20986bdafbebe03977f660ebf74c71131f9762cc392497fea",
      "question": "How does data augmentation act like a form of regularization?",
      "tags": [
        "data_augmentation",
        "regularization"
      ],
      "answers": [
        {
          "text": "It expands effective training data with label-preserving variations, encouraging invariances and reducing overfitting.",
          "explanation": "Augmentation makes memorization harder and generalization easier.",
          "category": "correct"
        },
        {
          "text": "Augmentation increases sample diversity so the model learns robust patterns rather than memorizing exact examples.",
          "explanation": "This is especially powerful for images/audio/text with valid transformations.",
          "category": "correct"
        },
        {
          "text": "It makes the dataset bigger.",
          "explanation": "Yes, but the point is adding meaningful variability, not just duplication.",
          "category": "partially_correct"
        },
        {
          "text": "Creates more examples.",
          "explanation": "Correct, as long as the transformations preserve labels.",
          "category": "partially_correct"
        },
        {
          "text": "Augmentation guarantees the model becomes unbiased.",
          "explanation": "It may reduce some biases but can also introduce new ones if poorly designed.",
          "category": "incorrect"
        },
        {
          "text": "Augmentation always works for tabular categorical data the same way as images.",
          "explanation": "Tabular augmentation is trickier and must preserve semantics.",
          "category": "incorrect"
        },
        {
          "text": "Compliment the data to boost confidence.",
          "explanation": "Augmentation is algorithmic variation, not praise.",
          "category": "ridiculous"
        },
        {
          "text": "Give the dataset a motivational poster.",
          "explanation": "Regularization comes from statistical diversity, not posters.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "Regularization & generalization control"
    },
    {
      "id": "1622038bcf56d1a31a797751690bb6729bd26869fedbfc9050c41fc23bf849ac",
      "question": "What is the bias–variance tradeoff, and how does model complexity influence it?",
      "tags": [
        "bias_variance",
        "model_complexity"
      ],
      "answers": [
        {
          "text": "More complex models generally reduce bias but increase variance; simpler models increase bias but reduce variance—seek the complexity that minimizes validation error.",
          "explanation": "The best generalization comes from balancing underfitting and overfitting.",
          "category": "correct"
        },
        {
          "text": "As capacity increases, models can fit training data better but may generalize worse unless controlled with regularization and validation.",
          "explanation": "The tradeoff explains why ‘more complex’ isn’t always better.",
          "category": "correct"
        },
        {
          "text": "Complex models overfit; simple models underfit.",
          "explanation": "Good intuition, though the tradeoff is about expected generalization error.",
          "category": "partially_correct"
        },
        {
          "text": "It’s a tradeoff between underfitting and overfitting.",
          "explanation": "Yes—bias relates to underfit, variance relates to overfit.",
          "category": "partially_correct"
        },
        {
          "text": "Bias and variance always decrease together.",
          "explanation": "They often move in opposite directions as complexity changes.",
          "category": "incorrect"
        },
        {
          "text": "More complexity always improves test performance.",
          "explanation": "Complexity can hurt test performance when variance dominates.",
          "category": "incorrect"
        },
        {
          "text": "Bias–variance is a political tradeoff.",
          "explanation": "It’s a statistical tradeoff, not politics.",
          "category": "ridiculous"
        },
        {
          "text": "Bias and variance are siblings fighting.",
          "explanation": "They’re error components, not people.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "Regularization & generalization control"
    },
    {
      "id": "d0557f6464108b5ae7d4fcb90488391435c00da5f611eaa168c2d356f13242f4",
      "question": "What is a hyperparameter, and how is it different from a learned parameter?",
      "tags": [
        "hyperparameters",
        "parameters"
      ],
      "answers": [
        {
          "text": "Hyperparameters are set externally (learning rate, depth, C), while learned parameters are fitted from data (weights, coefficients, splits).",
          "explanation": "Hyperparameters control capacity/learning; parameters are the model values learned during training.",
          "category": "correct"
        },
        {
          "text": "Hyperparameters govern the training procedure or model family; parameters are the resulting values optimized by training.",
          "explanation": "You tune hyperparameters; the optimizer learns parameters.",
          "category": "correct"
        },
        {
          "text": "Hyperparameters are settings.",
          "explanation": "Yes, but they are not learned directly from training gradients.",
          "category": "partially_correct"
        },
        {
          "text": "Parameters are learned weights.",
          "explanation": "Correct for many models; the distinction is who sets them.",
          "category": "partially_correct"
        },
        {
          "text": "Hyperparameters are learned after training finishes.",
          "explanation": "Hyperparameters are typically selected via tuning, not learned as model parameters.",
          "category": "incorrect"
        },
        {
          "text": "Parameters are chosen by the developer.",
          "explanation": "Parameters are generally learned from data.",
          "category": "incorrect"
        },
        {
          "text": "Hyperparameters are parameters with superpowers.",
          "explanation": "They’re simply externally chosen configuration values.",
          "category": "ridiculous"
        },
        {
          "text": "Hyperparameters wear capes.",
          "explanation": "They control learning, but they’re not heroic characters.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.4 Hyperparameter optimization"
    },
    {
      "id": "e19eb1ad93816e0822fb23a551032561c17c9fac2d8e7dfd2d05dae5ed6d1d14",
      "question": "Why do you need a validation strategy (not the test set) for hyperparameter tuning?",
      "tags": [
        "validation",
        "model_selection",
        "leakage"
      ],
      "answers": [
        {
          "text": "Tuning on the test set leaks information and inflates performance; the test set must remain untouched for final evaluation.",
          "explanation": "Otherwise, your final metric is biased and over-optimistic.",
          "category": "correct"
        },
        {
          "text": "Using the test set during tuning turns it into a validation set and contaminates your final report.",
          "explanation": "A clean test set is needed for an unbiased estimate of generalization.",
          "category": "correct"
        },
        {
          "text": "Because the test set is for the end.",
          "explanation": "Yes—the reason is to preserve unbiased evaluation.",
          "category": "partially_correct"
        },
        {
          "text": "Keep test unseen until final report.",
          "explanation": "Correct: don’t use test for model selection.",
          "category": "partially_correct"
        },
        {
          "text": "It’s fine to tune on test if you don’t look too closely.",
          "explanation": "Any tuning on test contaminates the estimate.",
          "category": "incorrect"
        },
        {
          "text": "You should tune on test so it’s optimized for reality.",
          "explanation": "That removes your ability to measure real-world performance honestly.",
          "category": "incorrect"
        },
        {
          "text": "Because the test set will feel used.",
          "explanation": "The issue is statistical contamination, not feelings.",
          "category": "ridiculous"
        },
        {
          "text": "Test set will post about it on social media.",
          "explanation": "You avoid test leakage for scientific validity.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.4 Hyperparameter optimization"
    },
    {
      "id": "4683de478ede4643b0f087b0b12b51834ce2183804ae4579924959673d6c1ab9",
      "question": "Compare grid search and random search: why is random search often more efficient in practice?",
      "tags": [
        "grid_search",
        "random_search",
        "tuning"
      ],
      "answers": [
        {
          "text": "Often only a few hyperparameters matter; random search explores more distinct values in important dimensions and covers space better under a limited budget.",
          "explanation": "Grid search wastes trials on unimportant combinations.",
          "category": "correct"
        },
        {
          "text": "Random search samples broadly and can find good regions faster when performance is sensitive to only a subset of parameters.",
          "explanation": "It’s a better use of a fixed number of evaluations.",
          "category": "correct"
        },
        {
          "text": "Random search tries more combinations.",
          "explanation": "Under the same budget it tries different points; the advantage is better coverage of relevant dimensions.",
          "category": "partially_correct"
        },
        {
          "text": "Grid is systematic; random is flexible.",
          "explanation": "Yes—random can be more efficient when the space is high-dimensional.",
          "category": "partially_correct"
        },
        {
          "text": "Grid search is always faster because it’s systematic.",
          "explanation": "Systematic doesn’t mean efficient; it can be wasteful.",
          "category": "incorrect"
        },
        {
          "text": "Random search can’t be reproduced.",
          "explanation": "It can be reproduced by fixing the random seed.",
          "category": "incorrect"
        },
        {
          "text": "Random search works because the universe likes chaos.",
          "explanation": "Its advantage is statistical coverage, not cosmic preference.",
          "category": "ridiculous"
        },
        {
          "text": "Dice-based optimization is quantum.",
          "explanation": "It’s just randomized sampling.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.4 Hyperparameter optimization"
    },
    {
      "id": "fdced844b8a6b8ada024d0e17799a566142c2cc3930c3ca4ea18e9071b6d726b",
      "question": "What is Bayesian optimization for hyperparameters, and what intuition explains why it can be effective?",
      "tags": [
        "bayesian_optimization",
        "tuning"
      ],
      "answers": [
        {
          "text": "It builds a surrogate model of performance vs hyperparameters and chooses new trials via an acquisition rule that balances exploration and exploitation.",
          "explanation": "It can find good regions with fewer expensive evaluations.",
          "category": "correct"
        },
        {
          "text": "It learns which regions are promising and samples intelligently rather than blindly.",
          "explanation": "The surrogate guides search toward likely improvements.",
          "category": "correct"
        },
        {
          "text": "It uses probability to pick the next settings.",
          "explanation": "Yes—through a probabilistic model of the objective.",
          "category": "partially_correct"
        },
        {
          "text": "It builds a model of the tuning landscape.",
          "explanation": "Correct: the landscape model guides sampling.",
          "category": "partially_correct"
        },
        {
          "text": "It guarantees the global optimum every time.",
          "explanation": "It improves efficiency but does not guarantee a global optimum.",
          "category": "incorrect"
        },
        {
          "text": "It only works if the objective is convex.",
          "explanation": "It can be used on non-convex objectives; it’s still heuristic.",
          "category": "incorrect"
        },
        {
          "text": "It asks Bayes personally which hyperparameters he prefers.",
          "explanation": "Bayesian methods refer to probabilistic modeling, not a person.",
          "category": "ridiculous"
        },
        {
          "text": "Summon Bayes with a ritual.",
          "explanation": "It’s a method, not magic.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.4 Hyperparameter optimization"
    },
    {
      "id": "c9ee45d8bdc5b35bd5a2fee36d988b1c58f0e335a8cba15f3add19c747f2dce7",
      "question": "What is a common failure mode of hyperparameter tuning when data leakage exists?",
      "tags": [
        "leakage",
        "tuning",
        "evaluation"
      ],
      "answers": [
        {
          "text": "Hyperparameters optimize leaked signal, producing unrealistically high validation scores that collapse in production.",
          "explanation": "Tuning amplifies leakage by selecting configurations that exploit it.",
          "category": "correct"
        },
        {
          "text": "Tuning overfits to artifacts that won’t exist at inference time, yielding a “best” model that’s not real.",
          "explanation": "Leakage invalidates the model selection process.",
          "category": "correct"
        },
        {
          "text": "The tuned model looks great but fails later.",
          "explanation": "Yes—because the validation estimate was contaminated.",
          "category": "partially_correct"
        },
        {
          "text": "Scores are unrealistically high.",
          "explanation": "A common symptom of leakage.",
          "category": "partially_correct"
        },
        {
          "text": "Leakage only affects accuracy, not tuning.",
          "explanation": "Leakage affects any selection criterion used in tuning.",
          "category": "incorrect"
        },
        {
          "text": "Leakage is fixed by adding more trials.",
          "explanation": "More trials can worsen leakage exploitation.",
          "category": "incorrect"
        },
        {
          "text": "The hyperparameters escape the lab.",
          "explanation": "The problem is evaluation contamination.",
          "category": "ridiculous"
        },
        {
          "text": "The model becomes a wizard on the test set only.",
          "explanation": "It appears brilliant due to leakage, not real skill.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.4 Hyperparameter optimization"
    },
    {
      "id": "b351664dfd1aba6fed6d25f7118ae1ca332df285de763ecb053c99abeeb18903",
      "question": "How can you tune a decision threshold separately from tuning model hyperparameters, and why would you?",
      "tags": [
        "thresholding",
        "cost_sensitive",
        "tuning"
      ],
      "answers": [
        {
          "text": "Train the model normally, then choose a threshold on validation data to meet a cost/precision/recall constraint without changing model parameters.",
          "explanation": "Thresholding is a decision policy that can be optimized after training.",
          "category": "correct"
        },
        {
          "text": "Tune the threshold to align predictions with business objectives (e.g., maximize expected value or satisfy precision constraints).",
          "explanation": "The best threshold depends on costs and class priors, not just the model.",
          "category": "correct"
        },
        {
          "text": "Move the threshold to get more positives.",
          "explanation": "Yes, but do it using an objective like cost or required precision/recall.",
          "category": "partially_correct"
        },
        {
          "text": "Adjust threshold based on precision/recall needs.",
          "explanation": "Correct—this is the primary reason to separate threshold tuning.",
          "category": "partially_correct"
        },
        {
          "text": "Threshold is a hyperparameter learned by gradient descent.",
          "explanation": "Threshold is usually set externally after training.",
          "category": "incorrect"
        },
        {
          "text": "Threshold must always be 0.5 by definition.",
          "explanation": "0.5 is arbitrary unless it matches costs and priors.",
          "category": "incorrect"
        },
        {
          "text": "Set threshold to 0.99 to be extra confident.",
          "explanation": "That may destroy recall; threshold must match objectives.",
          "category": "ridiculous"
        },
        {
          "text": "Set the threshold to your GPA.",
          "explanation": "Thresholds are chosen by validation objectives, not personal numbers.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.4 Hyperparameter optimization"
    },
    {
      "id": "adc49b25a749d44c8606ab402ee986de71b537ab88b8936e1dac1ed9a34c1566",
      "question": "Why might the “best” hyperparameters on one dataset slice fail on another (and how do you guard against that)?",
      "tags": [
        "generalization",
        "robustness",
        "cross_validation"
      ],
      "answers": [
        {
          "text": "Different distributions/subgroups can have different optimal settings; guard with cross-validation, stratified splits, and evaluation across key segments.",
          "explanation": "Robust tuning requires testing stability across folds and populations.",
          "category": "correct"
        },
        {
          "text": "Validate across multiple folds and important slices (time, geography, customer segments) to avoid over-optimizing for a narrow subset.",
          "explanation": "Robustness is as important as peak score on one slice.",
          "category": "correct"
        },
        {
          "text": "Because the data changed.",
          "explanation": "Yes—distribution differences drive hyperparameter sensitivity.",
          "category": "partially_correct"
        },
        {
          "text": "Different users behave differently.",
          "explanation": "Correct: heterogeneity can change optimal settings.",
          "category": "partially_correct"
        },
        {
          "text": "If hyperparameters are optimal once, they are optimal forever.",
          "explanation": "Optimal settings depend on data distribution and objectives.",
          "category": "incorrect"
        },
        {
          "text": "Just pick the biggest model to cover all slices.",
          "explanation": "Bigger models can overfit and worsen robustness.",
          "category": "incorrect"
        },
        {
          "text": "Because the hyperparameters got bored.",
          "explanation": "It’s distribution shift and heterogeneity, not boredom.",
          "category": "ridiculous"
        },
        {
          "text": "Hyperparameters are seasonal.",
          "explanation": "Performance changes with data conditions, not seasons.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.4 Hyperparameter optimization"
    },
    {
      "id": "c35a540cc2c27a0aff68e049a2dc7b88a31ea19dcccb11318392b25918009e0c",
      "question": "What is the purpose of a confusion matrix, and what does it reveal that accuracy can hide?",
      "tags": [
        "confusion_matrix",
        "classification_metrics"
      ],
      "answers": [
        {
          "text": "It breaks outcomes into TP/FP/FN/TN (or per-class), showing which error types dominate and what accuracy hides.",
          "explanation": "Accuracy can be high even when false negatives or false positives are unacceptable.",
          "category": "correct"
        },
        {
          "text": "It decomposes mistakes by class and direction, helping diagnose where the model fails and how to adjust thresholds.",
          "explanation": "Different error types often have different business costs.",
          "category": "correct"
        },
        {
          "text": "It shows correct vs incorrect counts.",
          "explanation": "It’s more specific: it shows which kinds of errors occur.",
          "category": "partially_correct"
        },
        {
          "text": "It shows misclassifications.",
          "explanation": "Yes, and also the distribution of correct predictions by class.",
          "category": "partially_correct"
        },
        {
          "text": "It’s only for regression.",
          "explanation": "Confusion matrices are primarily for classification.",
          "category": "incorrect"
        },
        {
          "text": "It replaces all other metrics.",
          "explanation": "It’s diagnostic, but you often need precision/recall/AUC and others too.",
          "category": "incorrect"
        },
        {
          "text": "It confuses the model into behaving.",
          "explanation": "It’s a reporting tool, not a behavioral hack.",
          "category": "ridiculous"
        },
        {
          "text": "The matrix makes you confused on purpose.",
          "explanation": "It aims to clarify error types.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.5 Evaluate machine learning models"
    },
    {
      "id": "70fe7657274e2809fcfbe31774b0ab0cb77a5ac5bc8d42d8762aa095fbe1817d",
      "question": "When should you prefer precision/recall (or F1) over accuracy for classification evaluation?",
      "tags": [
        "precision",
        "recall",
        "f1",
        "class_imbalance"
      ],
      "answers": [
        {
          "text": "When classes are imbalanced or error costs differ; accuracy can hide poor minority-class performance.",
          "explanation": "Precision and recall reflect false positive/false negative tradeoffs directly.",
          "category": "correct"
        },
        {
          "text": "When positives are rare or expensive, optimize metrics aligned with the real objective (e.g., recall for safety, precision for costly reviews).",
          "explanation": "Different applications prioritize different error types.",
          "category": "correct"
        },
        {
          "text": "When you care about false positives or false negatives.",
          "explanation": "Yes—this is exactly what precision and recall measure.",
          "category": "partially_correct"
        },
        {
          "text": "When the dataset is imbalanced.",
          "explanation": "Often true, since accuracy can become misleading.",
          "category": "partially_correct"
        },
        {
          "text": "Always use accuracy; it’s the best metric.",
          "explanation": "Accuracy can be misleading under imbalance or asymmetric costs.",
          "category": "incorrect"
        },
        {
          "text": "Precision/recall only matter for multiclass.",
          "explanation": "They matter for binary classification and multiclass (macro/micro averaging).",
          "category": "incorrect"
        },
        {
          "text": "Use F1 when you like the number 1.",
          "explanation": "Choose F1 when you need a balance between precision and recall.",
          "category": "ridiculous"
        },
        {
          "text": "Use recall because you have a good memory.",
          "explanation": "Recall measures sensitivity, not human memory.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.5 Evaluate machine learning models"
    },
    {
      "id": "e71c1a24fd6fa28d5e909d1f5f2457f8d8de0fa7563f1a638236be5db85a3bba",
      "question": "What is ROC-AUC measuring, and when can it be misleading?",
      "tags": [
        "roc_auc",
        "metrics"
      ],
      "answers": [
        {
          "text": "It measures ranking quality across thresholds (TPR vs FPR); it can mislead with extreme imbalance or when precision on positives is the priority.",
          "explanation": "ROC-AUC can look high even when positive precision is poor.",
          "category": "correct"
        },
        {
          "text": "ROC-AUC summarizes the tradeoff across thresholds, but may not reflect utility in rare-event detection where PR-AUC is more informative.",
          "explanation": "Choose metrics aligned to what matters operationally.",
          "category": "correct"
        },
        {
          "text": "It measures overall classifier performance.",
          "explanation": "It measures ranking, not a specific operating point.",
          "category": "partially_correct"
        },
        {
          "text": "It summarizes performance across thresholds.",
          "explanation": "Yes—across thresholds, not at one fixed threshold.",
          "category": "partially_correct"
        },
        {
          "text": "ROC-AUC is the same as accuracy at a 0.5 threshold.",
          "explanation": "ROC-AUC is threshold-independent; accuracy is threshold-dependent.",
          "category": "incorrect"
        },
        {
          "text": "ROC-AUC is only for regression.",
          "explanation": "It’s primarily used for classification.",
          "category": "incorrect"
        },
        {
          "text": "It measures how fast your model can draw a curve.",
          "explanation": "It measures ranking discrimination, not speed.",
          "category": "ridiculous"
        },
        {
          "text": "ROC is a rock band; AUC is their album.",
          "explanation": "ROC-AUC is a statistical metric.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.5 Evaluate machine learning models"
    },
    {
      "id": "731ddf985b23b018e70a2325cfdae73a6eae8e5a30590cf47199352753d8f978",
      "question": "What is PR-AUC, and why is it often preferred for highly imbalanced datasets?",
      "tags": [
        "pr_auc",
        "class_imbalance",
        "metrics"
      ],
      "answers": [
        {
          "text": "PR-AUC is the area under the precision–recall curve; it focuses on positive-class performance and is more informative when positives are rare.",
          "explanation": "It highlights the false-positive burden and retrieval quality for rare events.",
          "category": "correct"
        },
        {
          "text": "PR-AUC is sensitive to changes in false positives and better reflects usefulness when the positive class is scarce.",
          "explanation": "In rare-event settings, precision is often critical.",
          "category": "correct"
        },
        {
          "text": "It’s like ROC-AUC but for precision and recall.",
          "explanation": "Yes—PR-AUC summarizes the precision/recall tradeoff across thresholds.",
          "category": "partially_correct"
        },
        {
          "text": "Better for rare positives.",
          "explanation": "Correct; it’s often more aligned with rare-event detection needs.",
          "category": "partially_correct"
        },
        {
          "text": "PR-AUC is only for multiclass problems.",
          "explanation": "It’s commonly used in binary imbalanced classification.",
          "category": "incorrect"
        },
        {
          "text": "PR-AUC ignores false positives.",
          "explanation": "Precision directly depends on false positives.",
          "category": "incorrect"
        },
        {
          "text": "PR-AUC stands for Pretty Reliable AUC.",
          "explanation": "It stands for Precision-Recall AUC.",
          "category": "ridiculous"
        },
        {
          "text": "PR means Public Relations AUC.",
          "explanation": "PR refers to precision and recall.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.5 Evaluate machine learning models"
    },
    {
      "id": "ccdefb876ce8d05db5d5e81d6a3feb5b7c42ece03059fe25682f31574449dbb4",
      "question": "What is cross-validation, and what problem does it solve compared to a single train/validation split?",
      "tags": [
        "cross_validation",
        "model_selection"
      ],
      "answers": [
        {
          "text": "Cross-validation trains and validates across multiple folds to reduce variance in performance estimates and use data more efficiently.",
          "explanation": "It prevents over-reliance on a single lucky/unlucky split.",
          "category": "correct"
        },
        {
          "text": "It averages results across several train/validation partitions, producing a more stable estimate of generalization.",
          "explanation": "This supports more reliable model selection and tuning.",
          "category": "correct"
        },
        {
          "text": "Split the data multiple times.",
          "explanation": "Yes—systematically into folds with repeated evaluation.",
          "category": "partially_correct"
        },
        {
          "text": "Train on different subsets.",
          "explanation": "Correct, and evaluate on the held-out fold each time.",
          "category": "partially_correct"
        },
        {
          "text": "It replaces the need for a test set.",
          "explanation": "You still need a final untouched test set for unbiased evaluation.",
          "category": "incorrect"
        },
        {
          "text": "It’s always better than having more data.",
          "explanation": "More data can be more valuable; CV just estimates performance more reliably.",
          "category": "incorrect"
        },
        {
          "text": "Models validate each other’s feelings.",
          "explanation": "It’s a statistical evaluation protocol.",
          "category": "ridiculous"
        },
        {
          "text": "Models exchange approval stamps.",
          "explanation": "Folds provide repeated validation, not stamps.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.5 Evaluate machine learning models"
    },
    {
      "id": "e743cf5d13d606a54c5fa7dc32a264394a39d16271d4bc6389b6ffbfcd8d7af1",
      "question": "How do you compare two models fairly when their optimal thresholds differ?",
      "tags": [
        "thresholding",
        "model_comparison",
        "metrics"
      ],
      "answers": [
        {
          "text": "Use threshold-independent metrics (AUC, log loss) and/or tune thresholds on validation under the same objective, then compare on a held-out test set consistently.",
          "explanation": "Fair comparison requires the same decision objective and clean evaluation.",
          "category": "correct"
        },
        {
          "text": "Fix a business goal (e.g., precision ≥ X) and compare recall (or expected cost) at that operating point for both models.",
          "explanation": "Compare models at the same operational constraint, not arbitrary thresholds.",
          "category": "correct"
        },
        {
          "text": "Pick the threshold that makes each look best.",
          "explanation": "That is not a fair comparison unless both optimize the same objective and are tested on held-out data.",
          "category": "partially_correct"
        },
        {
          "text": "Compare at 0.5 and hope it’s fair.",
          "explanation": "0.5 is arbitrary and may not reflect costs or priors.",
          "category": "partially_correct"
        },
        {
          "text": "Use 0.5 for both because fairness means identical thresholds.",
          "explanation": "Fairness means comparable objectives, not identical thresholds.",
          "category": "incorrect"
        },
        {
          "text": "Compare training metrics only; thresholds don’t matter.",
          "explanation": "Training metrics are biased; thresholds strongly affect error tradeoffs.",
          "category": "incorrect"
        },
        {
          "text": "Let the models fight and declare the survivor the winner.",
          "explanation": "Use consistent metrics and objectives, not combat.",
          "category": "ridiculous"
        },
        {
          "text": "Rock-paper-scissors for thresholds.",
          "explanation": "Thresholds should be chosen based on validation objectives.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.5 Evaluate machine learning models"
    },
    {
      "id": "59fa9bf550a2d2d7911fea110e7134c810c21b700b9295f687d3ed6b8da15295",
      "question": "What is calibration (probability calibration), and why can a well-calibrated model matter in real decisions?",
      "tags": [
        "calibration",
        "probabilities",
        "decision_making"
      ],
      "answers": [
        {
          "text": "Calibration means predicted probabilities match observed frequencies; it matters for risk decisions, ranking, and expected-value optimization.",
          "explanation": "Well-calibrated probabilities enable correct decision thresholds and resource allocation.",
          "category": "correct"
        },
        {
          "text": "A calibrated 0.8 should mean ~80% chance in reality, supporting triage, pricing, and cost-sensitive policies.",
          "explanation": "Poor calibration can cause overconfident or underconfident decisions.",
          "category": "correct"
        },
        {
          "text": "It means the confidence scores are trustworthy.",
          "explanation": "Yes—trustworthy in the sense that probability corresponds to real frequency.",
          "category": "partially_correct"
        },
        {
          "text": "Probabilities reflect reality better.",
          "explanation": "Correct—calibration aligns predicted and observed outcomes.",
          "category": "partially_correct"
        },
        {
          "text": "Calibration only matters for accuracy.",
          "explanation": "Calibration matters most when probabilities drive decisions, not just class labels.",
          "category": "incorrect"
        },
        {
          "text": "Calibration guarantees higher accuracy.",
          "explanation": "Calibration improves probability quality; it may not change accuracy.",
          "category": "incorrect"
        },
        {
          "text": "Calibration is when you tune the model with a guitar tuner.",
          "explanation": "Calibration is statistical alignment of predicted probabilities and outcomes.",
          "category": "ridiculous"
        },
        {
          "text": "Calibrate using a measuring tape.",
          "explanation": "You calibrate probabilities using validation data and calibration methods.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain3-modeling",
      "subtopic": "3.5 Evaluate machine learning models"
    },
    {
      "id": "1fa8972bfd54272d12b22be6c810058138b1130441e080d207fdcd0cc10b6f82",
      "question": "What are the main stages in an AWS SageMaker model lifecycle from training to production deployment?",
      "tags": [
        "sagemaker",
        "mlops",
        "lifecycle",
        "deployment"
      ],
      "answers": [
        {
          "text": "Prepare data → train → evaluate → register/version artifact → deploy (endpoint/batch) → monitor → iterate/retrain.",
          "explanation": "A production lifecycle includes evaluation, controlled release, and monitoring—not just training.",
          "category": "correct"
        },
        {
          "text": "Data prep & validation → training job → model evaluation/approval gate → package/register → deployment strategy (canary/blue-green) → monitoring & retraining loop.",
          "explanation": "Operational ML requires gates, versioning, and feedback loops for continuous improvement.",
          "category": "correct"
        },
        {
          "text": "Train a model, then deploy it.",
          "explanation": "This skips critical steps like evaluation, registration/versioning, monitoring, and rollback planning.",
          "category": "partially_correct"
        },
        {
          "text": "Build the model and put it behind an endpoint.",
          "explanation": "Deployment is only one stage; production requires validation and ongoing operations.",
          "category": "partially_correct"
        },
        {
          "text": "Deploy first, then train in production traffic.",
          "explanation": "Training must be validated offline before exposing production traffic to a new model.",
          "category": "incorrect"
        },
        {
          "text": "Skip evaluation; production traffic is the evaluation.",
          "explanation": "This risks harmful outcomes and makes failures harder to diagnose and roll back.",
          "category": "incorrect"
        },
        {
          "text": "Let the model “marinate” in S3 until it becomes production-ready.",
          "explanation": "Models require validation, controlled deployment, and monitoring—not waiting.",
          "category": "ridiculous"
        },
        {
          "text": "Print the weights and mail them to production.",
          "explanation": "Deployment requires packaging artifacts and infrastructure, not physical mail.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.4 Deploy and operationalize ML solutions (MLOps)"
    },
    {
      "id": "2b8750b2bfd823f21ada2e5fd3a0525e9fa3a2525d127f4b95b0c15069092615",
      "question": "What’s the difference between real-time inference, asynchronous inference, and batch transform, and when would you choose each?",
      "tags": [
        "inference",
        "realtime",
        "async_inference",
        "batch_transform"
      ],
      "answers": [
        {
          "text": "Real-time inference serves low-latency requests per call; async inference supports longer-running requests with decoupled response; batch transform scores large datasets offline.",
          "explanation": "Choose based on latency needs, payload size, throughput patterns, and user interactivity.",
          "category": "correct"
        },
        {
          "text": "Real-time for interactive APIs; async for long-running or large payloads with results delivered later; batch for scheduled/backfill scoring jobs.",
          "explanation": "Each mode optimizes a different operational constraint.",
          "category": "correct"
        },
        {
          "text": "Real-time is fast; batch is for big jobs.",
          "explanation": "Correct idea, but async inference is a distinct middle ground for long-running requests.",
          "category": "partially_correct"
        },
        {
          "text": "Async is for slower requests.",
          "explanation": "Yes, but the key distinction is decoupled responses and queue/storage-based patterns.",
          "category": "partially_correct"
        },
        {
          "text": "Async inference is the same as real-time, just with a different name.",
          "explanation": "Async changes the request/response pattern and accommodates longer processing times.",
          "category": "incorrect"
        },
        {
          "text": "Batch transform is for training, not inference.",
          "explanation": "Batch transform is an inference mode for offline scoring.",
          "category": "incorrect"
        },
        {
          "text": "Batch transform is when you transform the batch size hyperparameter.",
          "explanation": "Batch transform refers to offline inference, not training hyperparameters.",
          "category": "ridiculous"
        },
        {
          "text": "Async means you predict later without ever returning results.",
          "explanation": "Async still returns results—just not synchronously.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.4 Deploy and operationalize ML solutions (MLOps)"
    },
    {
      "id": "074783996c736ce3a48cd86e4a3284985aaa403b0a09a6520431d421a6931dac",
      "question": "What is model drift vs data drift, and how would you detect and respond to each in production?",
      "tags": [
        "drift",
        "model_drift",
        "data_drift",
        "monitoring"
      ],
      "answers": [
        {
          "text": "Data drift is a shift in input/feature distributions; model drift is a change in performance or the input→label relationship. Detect with feature/prediction monitoring and (when labels arrive) outcome metrics; respond with investigation, retraining, or rollback.",
          "explanation": "Drift detection combines statistical monitoring with performance verification when ground truth becomes available.",
          "category": "correct"
        },
        {
          "text": "Data drift is covariate shift; model drift is degradation of the learned mapping. Use alerting, evaluation gates, retraining, and safe deployment rollbacks.",
          "explanation": "You need both detection (signals) and response (process) to manage drift safely.",
          "category": "correct"
        },
        {
          "text": "Drift means the model gets worse over time; retrain it.",
          "explanation": "Retraining may help, but you should identify whether inputs shifted or the relationship changed.",
          "category": "partially_correct"
        },
        {
          "text": "Drift is when production data changes; update the model.",
          "explanation": "Data drift is one form; model drift can occur even without obvious feature shifts.",
          "category": "partially_correct"
        },
        {
          "text": "Drift only happens when your endpoint is under-provisioned.",
          "explanation": "Drift is a data/model phenomenon, not a capacity issue.",
          "category": "incorrect"
        },
        {
          "text": "Drift can be ignored if latency is low.",
          "explanation": "Low latency doesn’t imply correct predictions or stable performance.",
          "category": "incorrect"
        },
        {
          "text": "Drift is when the model starts believing conspiracy theories.",
          "explanation": "Drift refers to statistical changes affecting model performance.",
          "category": "ridiculous"
        },
        {
          "text": "Drift is when your model wanders off the internet.",
          "explanation": "Drift is about distributions and performance, not connectivity.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.4 Deploy and operationalize ML solutions (MLOps)"
    },
    {
      "id": "b0c81e130c937bcdc686841e6af1e6b0c711ada03202dda23c8e0a4af7311c89",
      "question": "What is the purpose of a model registry (or versioning system), and what must you track for reproducibility?",
      "tags": [
        "model_registry",
        "versioning",
        "reproducibility",
        "lineage"
      ],
      "answers": [
        {
          "text": "A registry tracks model versions and lifecycle state; for reproducibility you must track the artifact, training code/container, hyperparameters, data version/lineage, feature definitions, and evaluation metrics.",
          "explanation": "Reproducing a model requires the full training recipe, not just the weights.",
          "category": "correct"
        },
        {
          "text": "Track the full recipe: dataset snapshot, preprocessing version, environment (libraries/container), evaluation reports, approvals, and deployment linkage.",
          "explanation": "Lineage enables auditability, rollback, and consistent retraining.",
          "category": "correct"
        },
        {
          "text": "Store the model files with a version number.",
          "explanation": "Helpful, but incomplete without data lineage, code, and environment details.",
          "category": "partially_correct"
        },
        {
          "text": "Keep a folder of models named by date.",
          "explanation": "Manual naming is error-prone and doesn’t guarantee full reproducibility.",
          "category": "partially_correct"
        },
        {
          "text": "You only need to track the final accuracy score.",
          "explanation": "Accuracy alone doesn’t allow you to recreate the model or diagnose failures.",
          "category": "incorrect"
        },
        {
          "text": "You only need the endpoint name.",
          "explanation": "Endpoints don’t encode the training recipe and may change over time.",
          "category": "incorrect"
        },
        {
          "text": "Name models after pets and you’ll always remember which is which.",
          "explanation": "Naming doesn’t replace traceable lineage and version metadata.",
          "category": "ridiculous"
        },
        {
          "text": "Version by the number of coffee cups consumed.",
          "explanation": "Versioning must be deterministic and audit-friendly.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.4 Deploy and operationalize ML solutions (MLOps)"
    },
    {
      "id": "c3db816d297747421671f951a8802435fe32d9a5327582ee2d2a64b9db131184",
      "question": "Why is canary or blue/green deployment useful for ML models, and how does it reduce risk?",
      "tags": [
        "deployment_strategy",
        "canary",
        "blue_green",
        "risk_mitigation"
      ],
      "answers": [
        {
          "text": "It limits blast radius: canary routes a small percentage of traffic to the new model; blue/green switches between two environments, enabling quick rollback if issues appear.",
          "explanation": "Controlled exposure reduces the impact of regressions and supports rapid recovery.",
          "category": "correct"
        },
        {
          "text": "It validates online metrics and system behavior under real traffic while preserving a known-good fallback.",
          "explanation": "Online behavior can differ from offline evaluation, so safe rollout patterns matter.",
          "category": "correct"
        },
        {
          "text": "It reduces risk by rolling out gradually.",
          "explanation": "Yes—gradual rollout is the key safety benefit.",
          "category": "partially_correct"
        },
        {
          "text": "It helps you roll back quickly if issues appear.",
          "explanation": "Rollback speed is crucial when production metrics degrade.",
          "category": "partially_correct"
        },
        {
          "text": "It prevents drift automatically.",
          "explanation": "Deployment strategy doesn’t stop drift; monitoring and retraining address drift.",
          "category": "incorrect"
        },
        {
          "text": "It guarantees the new model is better.",
          "explanation": "It reduces risk, but you still need online validation.",
          "category": "incorrect"
        },
        {
          "text": "Blue models are calmer; green models are eco-friendly.",
          "explanation": "The colors refer to environments, not emotions.",
          "category": "ridiculous"
        },
        {
          "text": "Canary works because birds detect bad models.",
          "explanation": "Canary deployment is a traffic-routing strategy.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.4 Deploy and operationalize ML solutions (MLOps)"
    },
    {
      "id": "c3053ef9c81ae7592ad7cbfc90c73367ee14a3318bc06be0ede17ccb7d9b3382",
      "question": "What monitoring signals would you set up for an ML endpoint beyond latency/CPU?",
      "tags": [
        "monitoring",
        "observability",
        "data_quality",
        "prediction_quality"
      ],
      "answers": [
        {
          "text": "Monitor prediction distribution shifts, feature distribution/stat summaries, missing/invalid inputs, confidence/calibration signals, and segment-level error rates; add outcome metrics when labels arrive.",
          "explanation": "Model quality monitoring requires data and prediction signals, not just infrastructure health.",
          "category": "correct"
        },
        {
          "text": "Add schema/range checks, drift scores, skew checks, request failure rates, and business KPI proxies tied to predictions.",
          "explanation": "Operational monitoring combines technical health with ML-specific behavior.",
          "category": "correct"
        },
        {
          "text": "Monitor errors and latency.",
          "explanation": "Useful, but you also need data/prediction behavior monitoring to detect drift and skew.",
          "category": "partially_correct"
        },
        {
          "text": "Monitor endpoint health and logs.",
          "explanation": "Good baseline, but not sufficient to detect quality degradation.",
          "category": "partially_correct"
        },
        {
          "text": "Only monitor GPU utilization; everything else is noise.",
          "explanation": "GPU usage doesn’t reveal prediction validity or data drift.",
          "category": "incorrect"
        },
        {
          "text": "Monitoring is unnecessary if tests passed.",
          "explanation": "Production data can change and models can drift after deployment.",
          "category": "incorrect"
        },
        {
          "text": "Monitor whether the model feels confident based on its horoscope.",
          "explanation": "Confidence must be measured with calibration/statistics, not astrology.",
          "category": "ridiculous"
        },
        {
          "text": "Ask the model how it feels every hour.",
          "explanation": "Monitoring must be objective and measurable.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.4 Deploy and operationalize ML solutions (MLOps)"
    },
    {
      "id": "c7e2f480fc62d412da666b021bf8b14283a297cecf1a1850f655ffe90c100d9d",
      "question": "How do you design a production pipeline to support retraining on a schedule or trigger (e.g., drift threshold)?",
      "tags": [
        "retraining",
        "pipelines",
        "automation",
        "triggers"
      ],
      "answers": [
        {
          "text": "Automate ingestion and validation, training, evaluation gates, model registration, and controlled deployment; trigger via schedule or drift/quality alarms.",
          "explanation": "Retraining must be governed by validation gates and safe release patterns.",
          "category": "correct"
        },
        {
          "text": "Implement CI/CD for models: data checks → train → validate → approve → deploy → monitor → retrain trigger with guardrails.",
          "explanation": "MLOps pipelines make retraining repeatable, auditable, and safe.",
          "category": "correct"
        },
        {
          "text": "Run training weekly and redeploy.",
          "explanation": "Scheduling helps, but you need validation/approval and safe rollout to prevent regressions.",
          "category": "partially_correct"
        },
        {
          "text": "Retrain whenever performance drops.",
          "explanation": "Good trigger idea, but you need a defined detection mechanism and deployment controls.",
          "category": "partially_correct"
        },
        {
          "text": "Retrain on every single request to stay fresh.",
          "explanation": "That’s expensive and unstable; retraining should be controlled and validated.",
          "category": "incorrect"
        },
        {
          "text": "Retrain but never evaluate—just deploy.",
          "explanation": "Without evaluation gates, you can deploy worse models.",
          "category": "incorrect"
        },
        {
          "text": "Retrain whenever someone says the model seems off in Slack.",
          "explanation": "Retraining should be driven by measurable signals, not anecdotes.",
          "category": "ridiculous"
        },
        {
          "text": "Retrain when Mercury is in retrograde.",
          "explanation": "Use statistical triggers and business metrics, not astrology.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.4 Deploy and operationalize ML solutions (MLOps)"
    },
    {
      "id": "da9304293116a1671b62158c311c0f9aaaa15f446a55b04b832db120346a5b22",
      "question": "What are common causes of training–serving skew, and how can you prevent it?",
      "tags": [
        "training_serving_skew",
        "feature_engineering",
        "pipelines",
        "consistency"
      ],
      "answers": [
        {
          "text": "Skew comes from different feature logic, sources, time windows, preprocessing, or leakage between training and inference. Prevent with shared pipelines/feature stores and strict 'available at prediction time' checks.",
          "explanation": "Consistency between offline and online feature computation prevents skew.",
          "category": "correct"
        },
        {
          "text": "Skew often arises when offline and online transforms diverge; prevent by reusing the same feature definitions and validating parity.",
          "explanation": "Feature parity tests and shared code reduce mismatch.",
          "category": "correct"
        },
        {
          "text": "Training data differs from production data.",
          "explanation": "Yes, but the key issue is mismatched transformations and availability at inference time.",
          "category": "partially_correct"
        },
        {
          "text": "Production looks different than the dataset.",
          "explanation": "True, but you should detect whether it’s drift, skew, or leakage.",
          "category": "partially_correct"
        },
        {
          "text": "Skew is unavoidable, so don’t bother.",
          "explanation": "Many forms of skew are preventable with engineering controls.",
          "category": "incorrect"
        },
        {
          "text": "Skew is fixed by increasing instance size.",
          "explanation": "Compute capacity doesn’t fix feature mismatch.",
          "category": "incorrect"
        },
        {
          "text": "Skew happens when the model sits at an angle on the GPU.",
          "explanation": "Skew is a data/pipeline inconsistency, not hardware orientation.",
          "category": "ridiculous"
        },
        {
          "text": "Skew is a GPU tilt setting.",
          "explanation": "Skew refers to train/serve mismatch.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.4 Deploy and operationalize ML solutions (MLOps)"
    },
    {
      "id": "f8df098e73ba844bd99bac7cfabb9d6f00ee370fabbf380231f7b9b1ec8b3d40",
      "question": "How do you validate that a newly deployed model is actually better in production (not just offline)?",
      "tags": [
        "online_evaluation",
        "ab_testing",
        "shadow_testing",
        "model_validation"
      ],
      "answers": [
        {
          "text": "Use shadow testing, canary, or A/B tests with online KPIs and guardrails; evaluate latency/cost and segment impacts under the same traffic.",
          "explanation": "Online performance can differ due to distribution shifts and system constraints.",
          "category": "correct"
        },
        {
          "text": "Run controlled experiments with rollback thresholds and verify business impact and safety metrics, not only offline scores.",
          "explanation": "Production success is measured by real outcomes and operational constraints.",
          "category": "correct"
        },
        {
          "text": "Check that validation accuracy is higher before deploying.",
          "explanation": "Offline gains are necessary but not sufficient; you still need online verification.",
          "category": "partially_correct"
        },
        {
          "text": "Compare offline AUC and ship the winner.",
          "explanation": "You must confirm online behavior and guardrails.",
          "category": "partially_correct"
        },
        {
          "text": "If offline AUC is higher, it’s automatically better in production.",
          "explanation": "Production conditions can differ; offline metrics can mislead.",
          "category": "incorrect"
        },
        {
          "text": "If the training loss is lower, it’s better in production.",
          "explanation": "Training loss does not measure generalization or business impact.",
          "category": "incorrect"
        },
        {
          "text": "If the dashboard line is greener, it’s better.",
          "explanation": "Color doesn’t equal improvement; use defined metrics.",
          "category": "ridiculous"
        },
        {
          "text": "If the model name sounds stronger, it wins.",
          "explanation": "Model quality must be demonstrated with evidence.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.4 Deploy and operationalize ML solutions (MLOps)"
    },
    {
      "id": "28f682f246cb513a680136de83c7d96a1f798c4a6d9779571901043b9d541076",
      "question": "What’s the difference between scaling training vs scaling inference, and what constraints differ between them?",
      "tags": [
        "scaling",
        "training",
        "inference",
        "performance"
      ],
      "answers": [
        {
          "text": "Training scaling focuses on throughput/parallelism (distributed compute and IO), time-to-train, and cost; inference scaling focuses on latency, concurrency, autoscaling behavior, and steady-state cost per request.",
          "explanation": "Training is often batch-oriented; inference is user-facing and latency-constrained.",
          "category": "correct"
        },
        {
          "text": "Training is bursty and can tolerate delays; inference must meet p95/p99 latency and availability targets while controlling cost.",
          "explanation": "The operational constraints differ fundamentally between offline training and online serving.",
          "category": "correct"
        },
        {
          "text": "Training is heavy compute; inference is serving.",
          "explanation": "True, but the deeper difference is latency and concurrency constraints vs throughput and time-to-train.",
          "category": "partially_correct"
        },
        {
          "text": "Training uses clusters; inference uses endpoints.",
          "explanation": "Often true in practice, but both can be distributed depending on architecture.",
          "category": "partially_correct"
        },
        {
          "text": "They scale the same way: just add more GPUs.",
          "explanation": "Inference scaling is often limited by latency and concurrency patterns, not only raw compute.",
          "category": "incorrect"
        },
        {
          "text": "Scaling is only about network bandwidth.",
          "explanation": "Compute, memory, model size, and request patterns are often bigger factors.",
          "category": "incorrect"
        },
        {
          "text": "Inference scaling is when you scale the model’s ego.",
          "explanation": "Scaling refers to system capacity and performance.",
          "category": "ridiculous"
        },
        {
          "text": "Scale training by shouting go faster at the instance.",
          "explanation": "Scaling requires architectural and resource changes, not shouting.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.1 Performance, availability, scalability, resiliency, fault tolerance"
    },
    {
      "id": "0e1b8f816712b5d61421260b48d35cb688faf7665d6a401771e0c60957dc5c79",
      "question": "How do you think about availability and fault tolerance for a real-time ML endpoint?",
      "tags": [
        "availability",
        "fault_tolerance",
        "realtime_inference",
        "reliability"
      ],
      "answers": [
        {
          "text": "Define SLOs and design redundancy: multiple instances/AZs, health checks, autoscaling, retries with backoff, fallback behavior, and fast rollback to a known-good model.",
          "explanation": "Real-time endpoints must be resilient to failures and safe to roll back.",
          "category": "correct"
        },
        {
          "text": "Design for failure using circuit breakers, graceful degradation, and rollback-ready versioning so outages don’t become catastrophes.",
          "explanation": "Reliability comes from redundancy plus controlled failure handling.",
          "category": "correct"
        },
        {
          "text": "Use autoscaling and monitoring.",
          "explanation": "Helpful, but availability also needs redundancy, rollback, and fallback logic.",
          "category": "partially_correct"
        },
        {
          "text": "Put it behind a load balancer.",
          "explanation": "Load balancing helps distribute traffic, but you still need health checks and multi-AZ resilience.",
          "category": "partially_correct"
        },
        {
          "text": "One big instance is more reliable than many small ones.",
          "explanation": "A single instance is a single point of failure.",
          "category": "incorrect"
        },
        {
          "text": "High availability means never deploy updates.",
          "explanation": "You still need updates—use safe deployment strategies instead.",
          "category": "incorrect"
        },
        {
          "text": "Deploy it on a yacht with satellite internet.",
          "explanation": "Reliability is engineered via redundancy and control planes.",
          "category": "ridiculous"
        },
        {
          "text": "Put the server in a bunker and it can’t fail.",
          "explanation": "Hardware still fails; design for failure.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.1 Performance, availability, scalability, resiliency, fault tolerance"
    },
    {
      "id": "bbee6baf6ec61fc8823d1a4a155287bfd6db747d2c34e32053d266e4beac2a23",
      "question": "What are typical bottlenecks for inference throughput (compute, I/O, model size), and how do you address them?",
      "tags": [
        "throughput",
        "latency",
        "bottlenecks",
        "optimization"
      ],
      "answers": [
        {
          "text": "Compute-bound: optimize/quantize or use better instances; IO-bound: improve serialization and batching; model-size/memory: distill/prune; runtime overhead: use optimized runtimes.",
          "explanation": "Bottleneck analysis guides the right optimization technique.",
          "category": "correct"
        },
        {
          "text": "Improve throughput with model optimization, batching/async patterns, right-sizing, and reducing preprocessing overhead.",
          "explanation": "Different constraints require different fixes.",
          "category": "correct"
        },
        {
          "text": "Use a bigger instance and you’ll be fine.",
          "explanation": "Sometimes helps, but cost-efficient solutions often require optimization and batching.",
          "category": "partially_correct"
        },
        {
          "text": "Add more replicas.",
          "explanation": "Horizontal scaling helps, but doesn’t fix per-request inefficiencies.",
          "category": "partially_correct"
        },
        {
          "text": "Throughput is only controlled by network bandwidth.",
          "explanation": "Compute, memory, and runtime overhead often dominate.",
          "category": "incorrect"
        },
        {
          "text": "Bigger models always increase throughput.",
          "explanation": "Bigger models often reduce throughput due to more computation.",
          "category": "incorrect"
        },
        {
          "text": "Tape a turbocharger to the GPU.",
          "explanation": "You need software/hardware optimization, not mechanical hacks.",
          "category": "ridiculous"
        },
        {
          "text": "Download more RAM.",
          "explanation": "RAM is physical; you must choose instances or optimize memory use.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.1 Performance, availability, scalability, resiliency, fault tolerance"
    },
    {
      "id": "a5c9e663a5efca1f53107edb066cc3d37c5689a32950c24038cad8518fac1257",
      "question": "What is an SLA/SLO in the context of ML services, and what metrics commonly define them?",
      "tags": [
        "slo",
        "sla",
        "latency",
        "availability",
        "metrics"
      ],
      "answers": [
        {
          "text": "SLOs are internal reliability targets (availability %, p99 latency, error rate, freshness); SLAs are contractual guarantees. ML may also track quality/drift/outcome metrics when labels exist.",
          "explanation": "SLOs guide engineering; SLAs define external commitments.",
          "category": "correct"
        },
        {
          "text": "SLA is what you promise externally; SLOs are what you engineer toward (latency percentiles, uptime, failures, cost ceilings).",
          "explanation": "ML systems add quality and drift signals beyond typical service metrics.",
          "category": "correct"
        },
        {
          "text": "SLA is a promise; SLO is a goal.",
          "explanation": "Correct—SLOs are targets; SLAs are contractual.",
          "category": "partially_correct"
        },
        {
          "text": "SLO is a target; SLA is a contract.",
          "explanation": "Yes, and both should map to measurable metrics.",
          "category": "partially_correct"
        },
        {
          "text": "SLA means the model is state of the art.",
          "explanation": "SLA is a reliability commitment, not a quality ranking.",
          "category": "incorrect"
        },
        {
          "text": "SLO is the same as accuracy.",
          "explanation": "SLOs are operational targets; accuracy is a model quality metric.",
          "category": "incorrect"
        },
        {
          "text": "SLO stands for SageMaker Loves Optimization.",
          "explanation": "SLO refers to service-level objectives.",
          "category": "ridiculous"
        },
        {
          "text": "SLA stands for Super Legendary Accuracy.",
          "explanation": "SLA refers to service-level agreements.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.1 Performance, availability, scalability, resiliency, fault tolerance"
    },
    {
      "id": "37db7cdfdcf547b71985c6506485100cdd07232acbd92e6eff25f53e6ebe54b6",
      "question": "How can you design an ML system to degrade gracefully if downstream dependencies fail (e.g., fallback logic)?",
      "tags": [
        "resilience",
        "graceful_degradation",
        "fallback",
        "dependency_management"
      ],
      "answers": [
        {
          "text": "Use cached results, rules-based fallback, safe defaults, circuit breakers, async queues, and partial-feature fallback instead of full outage.",
          "explanation": "Graceful degradation preserves service continuity and safety.",
          "category": "correct"
        },
        {
          "text": "Apply resilience patterns: timeouts, retries with limits, bulkheads, fallback models/heuristics, and degraded-mode feature subsets.",
          "explanation": "Limit blast radius and avoid cascading failures.",
          "category": "correct"
        },
        {
          "text": "Return a default value when something breaks.",
          "explanation": "Defaults can help, but should be safe and aligned with business risk.",
          "category": "partially_correct"
        },
        {
          "text": "Use a fallback response.",
          "explanation": "Yes—fallbacks should be designed and tested, not improvised.",
          "category": "partially_correct"
        },
        {
          "text": "If any dependency fails, crash the whole service immediately.",
          "explanation": "This causes avoidable outages and cascading failures.",
          "category": "incorrect"
        },
        {
          "text": "Retry forever until it works.",
          "explanation": "Infinite retries can amplify failures and exhaust resources.",
          "category": "incorrect"
        },
        {
          "text": "If the database is down, ask users to guess the prediction themselves.",
          "explanation": "Graceful degradation should be engineered, not offloaded to users.",
          "category": "ridiculous"
        },
        {
          "text": "Have the endpoint apologize in haiku.",
          "explanation": "Poetry doesn’t restore service reliability.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.1 Performance, availability, scalability, resiliency, fault tolerance"
    },
    {
      "id": "6e052c18b1d7fb3e23649caaffbad83b79b0f78f5229c0b8eac4bd3c317bce79",
      "question": "What are the risks of “autoscaling everything” for ML inference, and what guardrails would you add?",
      "tags": [
        "autoscaling",
        "cost_control",
        "stability",
        "guardrails"
      ],
      "answers": [
        {
          "text": "Risks include cost spikes, cold starts, oscillation/thrashing, quota limits, and cascading failures; add min/max capacity, cooldowns, alarms, load shedding, and batching/queues.",
          "explanation": "Autoscaling must be bounded and monitored to remain stable and cost-effective.",
          "category": "correct"
        },
        {
          "text": "Add budgets, scaling policies tied to meaningful signals, and backpressure mechanisms to prevent runaway scaling and instability.",
          "explanation": "Guardrails keep scaling predictable and safe.",
          "category": "correct"
        },
        {
          "text": "It can get expensive.",
          "explanation": "Yes—cost and instability are common risks.",
          "category": "partially_correct"
        },
        {
          "text": "Cold starts can be an issue.",
          "explanation": "Correct; scaling from zero or low capacity can violate latency SLOs.",
          "category": "partially_correct"
        },
        {
          "text": "Autoscaling removes the need for capacity planning.",
          "explanation": "You still need limits, quotas, and expected-load planning.",
          "category": "incorrect"
        },
        {
          "text": "Autoscaling guarantees stability.",
          "explanation": "Poor policies can cause oscillation and outages.",
          "category": "incorrect"
        },
        {
          "text": "Autoscale until AWS sends you a thank-you note.",
          "explanation": "Unbounded scaling can bankrupt your budget.",
          "category": "ridiculous"
        },
        {
          "text": "Autoscale until you reach enlightenment.",
          "explanation": "Scaling should be engineered with explicit goals and limits.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.1 Performance, availability, scalability, resiliency, fault tolerance"
    },
    {
      "id": "54d2d763afb03df09855f60b6c9ba6c2fcbad7bcad52ae176d0785d516216a99",
      "question": "Given a requirement to classify text sentiment at scale, what AWS service options exist and what are the tradeoffs?",
      "tags": [
        "aws_ai_services",
        "comprehend",
        "nlp",
        "service_selection"
      ],
      "answers": [
        {
          "text": "Amazon Comprehend provides managed sentiment analysis; alternatively, SageMaker can host a built-in/JumpStart/custom NLP model. Tradeoffs are speed-to-value and simplicity vs customization, control, and potentially different cost/latency profiles.",
          "explanation": "Managed services ship fast; custom models provide domain adaptation and deeper control.",
          "category": "correct"
        },
        {
          "text": "Use a managed AI service when it’s 'good enough' and quick; choose custom training/hosting when domain language, custom labels, or strict constraints require it.",
          "explanation": "This is a classic buy-vs-build decision with ML-specific tradeoffs.",
          "category": "correct"
        },
        {
          "text": "Use Comprehend for sentiment.",
          "explanation": "Often correct, but you should consider whether it meets domain-specific requirements.",
          "category": "partially_correct"
        },
        {
          "text": "Use SageMaker if you want to host your own model.",
          "explanation": "True, though the decision should be based on requirements and tradeoffs.",
          "category": "partially_correct"
        },
        {
          "text": "Use Polly; it’s for text.",
          "explanation": "Polly is text-to-speech, not sentiment classification.",
          "category": "incorrect"
        },
        {
          "text": "Use S3 Select; it can classify text.",
          "explanation": "S3 Select queries data; it does not provide ML inference.",
          "category": "incorrect"
        },
        {
          "text": "Use Rekognition—sentiment is basically a face, right?",
          "explanation": "Rekognition is for images/video, not text sentiment.",
          "category": "ridiculous"
        },
        {
          "text": "Use CloudWatch because it has logs.",
          "explanation": "CloudWatch is for monitoring, not sentiment analysis.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.2 Choose and implement the right AWS ML services/features"
    },
    {
      "id": "0a22cbac2c6e1a17af9179940b652aff16404537f400aef31d952962837cca90",
      "question": "When would you use a SageMaker built-in algorithm instead of bringing your own container/model?",
      "tags": [
        "sagemaker",
        "built_in_algorithms",
        "byoc",
        "tradeoffs"
      ],
      "answers": [
        {
          "text": "Use built-in algorithms when they match your problem and you want faster setup, proven training patterns, and reduced ops; use BYOC when you need custom architectures, preprocessing, or dependencies.",
          "explanation": "Built-ins reduce engineering burden; BYOC provides flexibility and differentiation.",
          "category": "correct"
        },
        {
          "text": "Built-ins are ideal for common patterns and quick delivery; BYOC is for unusual data formats, specialized frameworks, or unique requirements.",
          "explanation": "The choice depends on control needs vs speed-to-ship.",
          "category": "correct"
        },
        {
          "text": "Use built-in algorithms to save time.",
          "explanation": "Yes—if the built-in fits the task and requirements.",
          "category": "partially_correct"
        },
        {
          "text": "BYOC for flexibility.",
          "explanation": "True, though flexibility comes with more responsibility and maintenance.",
          "category": "partially_correct"
        },
        {
          "text": "Built-in algorithms can’t be deployed to endpoints.",
          "explanation": "They can be deployed; deployment support is a major feature.",
          "category": "incorrect"
        },
        {
          "text": "BYOC is not allowed in SageMaker.",
          "explanation": "SageMaker supports custom containers.",
          "category": "incorrect"
        },
        {
          "text": "Use built-in only if your boss likes the word built-in.",
          "explanation": "Choose based on requirements and constraints.",
          "category": "ridiculous"
        },
        {
          "text": "BYOC means Bring Your Own Coffee.",
          "explanation": "BYOC refers to containers/models, not coffee.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.2 Choose and implement the right AWS ML services/features"
    },
    {
      "id": "91c96ede3f472ee91de21658b622eeac59af57777463dd070c01ad4c6937bf83",
      "question": "What factors determine whether you should use a built-in algorithm vs AutoML vs a custom training script?",
      "tags": [
        "automl",
        "built_in_algorithms",
        "custom_training",
        "model_choice"
      ],
      "answers": [
        {
          "text": "Consider required control, time-to-ship, data type, explainability, performance needs, compliance, and ops complexity: AutoML for quick strong baseline, built-in for supported patterns, custom for unique needs.",
          "explanation": "These options trade off speed and simplicity against flexibility and specialization.",
          "category": "correct"
        },
        {
          "text": "Choose based on constraints: maintainability and fast delivery vs maximum customization and specialized architectures.",
          "explanation": "Custom approaches require more MLOps and engineering discipline.",
          "category": "correct"
        },
        {
          "text": "AutoML is easiest; custom is hardest.",
          "explanation": "Often true, but the key is whether AutoML meets your constraints and performance needs.",
          "category": "partially_correct"
        },
        {
          "text": "Built-in is in the middle.",
          "explanation": "A useful heuristic, though specifics depend on the task.",
          "category": "partially_correct"
        },
        {
          "text": "Always choose AutoML because it guarantees the best model.",
          "explanation": "AutoML is helpful, but not guaranteed best, and may not meet all constraints.",
          "category": "incorrect"
        },
        {
          "text": "Custom scripts can’t be productionized.",
          "explanation": "Custom training can be productionized with proper packaging and MLOps.",
          "category": "incorrect"
        },
        {
          "text": "Pick based on which option has the coolest UI.",
          "explanation": "UI is not a valid reason to select a modeling approach.",
          "category": "ridiculous"
        },
        {
          "text": "Pick whichever has the longest name.",
          "explanation": "Choose based on requirements and outcomes.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.2 Choose and implement the right AWS ML services/features"
    },
    {
      "id": "86101ad84ba9adca77f5e6bd52c4e698aa2afb6697b8c617a629ef122dae9882",
      "question": "How do you decide between using an AWS AI service (e.g., Comprehend, Transcribe) vs training a custom model?",
      "tags": [
        "managed_services",
        "custom_model",
        "buy_vs_build",
        "requirements"
      ],
      "answers": [
        {
          "text": "Use a managed AI service if it meets accuracy/latency/compliance needs and you want fast delivery; train custom if the domain is specialized, labels are unique, or you need full control and differentiated performance.",
          "explanation": "Managed services optimize time-to-value; custom models optimize fit and control.",
          "category": "correct"
        },
        {
          "text": "Buy when it’s good enough and cheaper to operate; build when requirements, data, or IP needs demand it.",
          "explanation": "The decision depends on constraints, not ideology.",
          "category": "correct"
        },
        {
          "text": "Use managed services to save engineering time.",
          "explanation": "Often true, but validate whether it meets accuracy and compliance needs.",
          "category": "partially_correct"
        },
        {
          "text": "Custom gives you more control.",
          "explanation": "Yes—at the cost of more engineering and operations work.",
          "category": "partially_correct"
        },
        {
          "text": "Custom models are always cheaper than managed services.",
          "explanation": "Custom models can have substantial engineering and ops costs.",
          "category": "incorrect"
        },
        {
          "text": "If the dataset has more than 1,000 rows, you must go custom.",
          "explanation": "Dataset size alone is not a valid decision rule.",
          "category": "incorrect"
        },
        {
          "text": "Flip a coin; AWS will understand.",
          "explanation": "Service selection should be requirement-driven.",
          "category": "ridiculous"
        },
        {
          "text": "Custom only if you have a hero’s journey.",
          "explanation": "Choose based on constraints, not narratives.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.2 Choose and implement the right AWS ML services/features"
    },
    {
      "id": "245777e8b71a736ad67487c6172fbdf5cca6e931cce275866a58d8fcac4afbeb",
      "question": "What are the practical implications of choosing a specific algorithm family for deployment (e.g., model size, latency sensitivity)?",
      "tags": [
        "algorithm_selection",
        "deployment_constraints",
        "latency",
        "model_size"
      ],
      "answers": [
        {
          "text": "Model size/complexity influences instance type, accelerators, batching/quantization/async; latency sensitivity determines real-time vs async vs batch; memory footprint drives cost and scaling behavior.",
          "explanation": "Deployment feasibility is often constrained by latency, throughput, and cost.",
          "category": "correct"
        },
        {
          "text": "Some model families are CPU-friendly while others require GPUs/Inferentia; choices affect cold start times, throughput, and infrastructure cost.",
          "explanation": "Algorithm selection impacts operations as much as accuracy.",
          "category": "correct"
        },
        {
          "text": "Larger models are slower.",
          "explanation": "Often true, though optimizations like quantization can mitigate.",
          "category": "partially_correct"
        },
        {
          "text": "Bigger models cost more to host.",
          "explanation": "Generally true due to compute and memory requirements.",
          "category": "partially_correct"
        },
        {
          "text": "Model choice doesn’t affect deployment—only instance type does.",
          "explanation": "Model properties drive instance requirements and latency behavior.",
          "category": "incorrect"
        },
        {
          "text": "Any model can meet any latency with enough retries.",
          "explanation": "Retries don’t reduce compute time and can worsen latency.",
          "category": "incorrect"
        },
        {
          "text": "If the model is tree-based, deploy it in a forest.",
          "explanation": "Deployment depends on compute and system design, not metaphors.",
          "category": "ridiculous"
        },
        {
          "text": "If it’s neural, deploy it in a brain.",
          "explanation": "Neural networks still run on compute infrastructure.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.2 Choose and implement the right AWS ML services/features"
    },
    {
      "id": "f1c2c26a14cbc1e74300b09fcef6c0726563800cd60497c3927f745eb0d5c616",
      "question": "If you have limited labeled data, what AWS-friendly approaches can help (e.g., transfer learning, managed services, weak supervision patterns)?",
      "tags": [
        "limited_data",
        "transfer_learning",
        "managed_services",
        "data_strategy"
      ],
      "answers": [
        {
          "text": "Use transfer learning/JumpStart fine-tuning, managed AI services, weak labeling strategies, active learning loops, and data-centric improvements with strong baselines and careful evaluation.",
          "explanation": "Pretrained models and human-in-the-loop strategies increase label efficiency.",
          "category": "correct"
        },
        {
          "text": "Leverage pretrained models, augmentation, semi-supervised approaches, and targeted labeling to maximize value from scarce labels.",
          "explanation": "The goal is to reduce labeling needs while preserving reliable evaluation.",
          "category": "correct"
        },
        {
          "text": "Do transfer learning.",
          "explanation": "Good first step, but you should also consider managed services and data-centric work.",
          "category": "partially_correct"
        },
        {
          "text": "Use a pretrained model.",
          "explanation": "Correct—pretrained models often boost performance with minimal labels.",
          "category": "partially_correct"
        },
        {
          "text": "Just train a massive deep net from scratch; labels don’t matter.",
          "explanation": "Labels drive supervised learning; training from scratch often fails with limited data.",
          "category": "incorrect"
        },
        {
          "text": "Skip evaluation; small data means metrics are useless.",
          "explanation": "You still need careful evaluation; small data increases uncertainty, not irrelevance.",
          "category": "incorrect"
        },
        {
          "text": "Label the data telepathically.",
          "explanation": "Labeling requires actual human or programmatic supervision.",
          "category": "ridiculous"
        },
        {
          "text": "Ask the model to label itself and trust it.",
          "explanation": "Self-labeling without validation can amplify errors.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.2 Choose and implement the right AWS ML services/features"
    },
    {
      "id": "4a84d3126626c59e73c44737611f7ef781c7d15cadf4911fc728a7a93214995a",
      "question": "What IAM principles should you apply when granting access to training data in S3 and deploying endpoints?",
      "tags": [
        "iam",
        "least_privilege",
        "s3",
        "access_control"
      ],
      "answers": [
        {
          "text": "Apply least privilege with role-based access, separate roles for training vs inference, scoped S3 permissions (prefix-level), short-lived credentials, and auditing/logging.",
          "explanation": "Minimize blast radius and enforce accountable access.",
          "category": "correct"
        },
        {
          "text": "Use IAM roles for SageMaker jobs/endpoints, restrict S3 paths, add policy conditions where appropriate, and log via CloudTrail.",
          "explanation": "Roles and scoped policies reduce risk compared to broad permissions.",
          "category": "correct"
        },
        {
          "text": "Give the training job permission to read S3.",
          "explanation": "Yes, but permissions should be scoped and separated by role and resource.",
          "category": "partially_correct"
        },
        {
          "text": "Use roles instead of embedding keys.",
          "explanation": "Correct—avoid long-lived credentials in code.",
          "category": "partially_correct"
        },
        {
          "text": "Use admin permissions so nothing breaks.",
          "explanation": "Overbroad permissions increase risk and violate least-privilege best practices.",
          "category": "incorrect"
        },
        {
          "text": "Share root keys with the team for convenience.",
          "explanation": "Root keys are extremely dangerous; use roles and temporary credentials.",
          "category": "incorrect"
        },
        {
          "text": "Share your root keys in the README for convenience.",
          "explanation": "This is a severe security breach.",
          "category": "ridiculous"
        },
        {
          "text": "Post credentials in Slack so everyone can move fast.",
          "explanation": "Credentials must be protected and managed securely.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.3 AWS security practices for ML solutions"
    },
    {
      "id": "c03690e6d65b0ec2dc4873c8c041e61f5abcc7e86e2bd2ce1bc7fb88ba23cc19",
      "question": "How do encryption-at-rest and encryption-in-transit apply to ML data pipelines on AWS (and where do you configure them)?",
      "tags": [
        "encryption",
        "kms",
        "in_transit",
        "at_rest"
      ],
      "answers": [
        {
          "text": "Encryption at rest uses SSE-KMS for S3, encrypted EBS volumes, and encrypted artifacts; encryption in transit uses TLS/HTTPS and private networking (VPC endpoints). Configure via KMS keys, bucket policies, and service/endpoint settings.",
          "explanation": "You secure stored data and data in motion with complementary controls.",
          "category": "correct"
        },
        {
          "text": "Use KMS-managed keys across storage and enforce TLS via policies; prefer private connectivity to reduce exposure.",
          "explanation": "Policy enforcement prevents accidental plaintext exposure.",
          "category": "correct"
        },
        {
          "text": "Enable KMS and HTTPS.",
          "explanation": "Good shorthand, but you should know which services and settings apply where.",
          "category": "partially_correct"
        },
        {
          "text": "Turn on encryption in S3 and use TLS.",
          "explanation": "Correct, but pipelines may also involve EBS, logs, and artifacts that need encryption.",
          "category": "partially_correct"
        },
        {
          "text": "Encryption is only needed for production, not training.",
          "explanation": "Training data can be sensitive and must be protected too.",
          "category": "incorrect"
        },
        {
          "text": "Encryption in transit is unnecessary inside AWS.",
          "explanation": "Defense-in-depth often requires TLS even within cloud networks.",
          "category": "incorrect"
        },
        {
          "text": "Zip the dataset with a password and call it military-grade.",
          "explanation": "Use AWS-native encryption and key management with proper policies.",
          "category": "ridiculous"
        },
        {
          "text": "Rename the bucket to encrypted-data and you’re done.",
          "explanation": "Names don’t provide encryption.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.3 AWS security practices for ML solutions"
    },
    {
      "id": "1d5d99c9d89b7bf06669a1e830873e7622b9d5122c4a1609a0ffdea3acce90d1",
      "question": "What is network isolation (VPC/VPC endpoints/private subnets) for ML workloads, and when is it required?",
      "tags": [
        "vpc",
        "network_isolation",
        "private_endpoints",
        "security"
      ],
      "answers": [
        {
          "text": "Network isolation keeps traffic off the public internet using private subnets and VPC endpoints/PrivateLink; it’s used for sensitive/compliance workloads and to prevent data exfiltration.",
          "explanation": "Private networking reduces exposure and supports stricter security boundaries.",
          "category": "correct"
        },
        {
          "text": "Use VPC-only access patterns with restricted egress when you need strong data boundary controls.",
          "explanation": "Isolation is often required for regulated data and strict governance.",
          "category": "correct"
        },
        {
          "text": "Use a VPC when security matters.",
          "explanation": "Yes, though the key is private connectivity and controlled egress.",
          "category": "partially_correct"
        },
        {
          "text": "Put workloads in private subnets.",
          "explanation": "That helps, but you also need endpoints, routing, and policies.",
          "category": "partially_correct"
        },
        {
          "text": "VPC isolation is only for EC2, not SageMaker.",
          "explanation": "SageMaker supports VPC configuration for training and endpoints.",
          "category": "incorrect"
        },
        {
          "text": "Network isolation means no networking at all.",
          "explanation": "It means restricted/private networking, not zero connectivity.",
          "category": "incorrect"
        },
        {
          "text": "Network isolation means putting the server in a soundproof room.",
          "explanation": "Isolation is about network routing and access control.",
          "category": "ridiculous"
        },
        {
          "text": "Wrap the ethernet cable in tinfoil.",
          "explanation": "Use VPC design and policies, not improvised shielding.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.3 AWS security practices for ML solutions"
    },
    {
      "id": "27020cffff2c9dc13b3630aa61237c964324b905739fb720a725f944ae461421",
      "question": "What are the key risks of exposing an inference endpoint publicly, and what mitigations would you implement?",
      "tags": [
        "endpoint_security",
        "authn_authz",
        "ddos",
        "rate_limiting"
      ],
      "answers": [
        {
          "text": "Risks include unauthorized access, abuse/DoS, data leakage, injection attacks, and model extraction. Mitigate with authn/authz, private endpoints, WAF/rate limits, input validation, logging/alerts, and least-privilege policies.",
          "explanation": "Public endpoints must be defended like any internet-facing API—with extra ML-specific risks.",
          "category": "correct"
        },
        {
          "text": "Use API gateways, signed requests, network restrictions, throttling, anomaly detection, and strong observability to detect and prevent abuse.",
          "explanation": "Defense-in-depth reduces both security and operational risk.",
          "category": "correct"
        },
        {
          "text": "Add authentication and rate limiting.",
          "explanation": "Good start; you should also validate inputs and restrict network exposure.",
          "category": "partially_correct"
        },
        {
          "text": "Put it behind an API gateway with throttles.",
          "explanation": "Useful, but combine with WAF, logging, and least-privilege access.",
          "category": "partially_correct"
        },
        {
          "text": "Public endpoints are safe if the model is accurate.",
          "explanation": "Accuracy doesn’t prevent misuse or attacks.",
          "category": "incorrect"
        },
        {
          "text": "Security isn’t needed if data isn’t secret.",
          "explanation": "Abuse, extraction, and operational disruption can occur even without sensitive data.",
          "category": "incorrect"
        },
        {
          "text": "Protect it by naming the endpoint definitely-not-public.",
          "explanation": "Security requires actual access controls, not naming tricks.",
          "category": "ridiculous"
        },
        {
          "text": "Hide the URL in a long password.",
          "explanation": "Security through obscurity is insufficient; use proper authentication and network controls.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain4-ml-implementation-and-operations",
      "subtopic": "4.3 AWS security practices for ML solutions"
    },
    {
      "id": "2802d327bc0d264cfee178e143ca7e53175685e35f96b3ed466b86f4219b8fe3",
      "question": "You need a fast baseline for tabular binary classification with millions of rows. Which SageMaker built-in algorithm is a strong first choice and why?",
      "tags": [
        "algorithm_selection",
        "tabular",
        "binary_classification",
        "baseline"
      ],
      "answers": [
        {
          "text": "XGBoost is a strong first baseline for tabular data because boosted trees handle nonlinearities, mixed feature types, missingness, and interactions with little feature engineering.",
          "explanation": "Boosted trees commonly provide strong tabular baselines quickly with limited feature engineering.",
          "category": "correct"
        },
        {
          "text": "Start with XGBoost (or an AutoML tabular ensemble if allowed) because it tends to be a high-performing, practical baseline on large structured datasets.",
          "explanation": "It’s often a strong, pragmatic baseline before investing in heavier custom modeling.",
          "category": "correct"
        },
        {
          "text": "Use Linear Learner because it’s fast.",
          "explanation": "Linear models are fast, but may underfit nonlinearities and interactions common in tabular data.",
          "category": "partially_correct"
        },
        {
          "text": "Try a simple linear/logistic model first for speed.",
          "explanation": "Good for a quick check, but not always the strongest baseline for complex tabular patterns.",
          "category": "partially_correct"
        },
        {
          "text": "Use LDA topic modeling because it’s good at classification.",
          "explanation": "LDA is for unsupervised topic discovery in text, not supervised tabular classification.",
          "category": "incorrect"
        },
        {
          "text": "Use semantic segmentation because it’s more advanced.",
          "explanation": "Semantic segmentation is a computer-vision task, not a tabular classification algorithm.",
          "category": "incorrect"
        },
        {
          "text": "Use PCA because fewer dimensions always means higher accuracy.",
          "explanation": "Dimensionality reduction can help, but it does not guarantee higher accuracy.",
          "category": "ridiculous"
        },
        {
          "text": "Just increase the GPU size until the model becomes accurate.",
          "explanation": "More compute doesn't fix model mismatch or data/feature issues.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Problem → algorithm selection cheat-sheet (decisioning + tradeoffs)"
    },
    {
      "id": "2f5d49bac3973bee60bcbfd0174c498b676a006568cdeda410bff7facd3f4180",
      "question": "You have sparse, high-dimensional features (e.g., one-hot IDs) and want to model feature interactions for CTR prediction. Which built-in algorithm best fits?",
      "tags": [
        "algorithm_selection",
        "sparse_data",
        "ctr",
        "factorization_machines"
      ],
      "answers": [
        {
          "text": "Factorization Machines—built for sparse, high-dimensional inputs and efficient pairwise feature interactions via latent factors.",
          "explanation": "FMs model interactions without enumerating all cross-features, which is ideal for sparse one-hot inputs.",
          "category": "correct"
        },
        {
          "text": "Use Factorization Machines for CTR/recsys with one-hot IDs because it models interactions without exploding parameters.",
          "explanation": "Low-rank factorization keeps parameter counts manageable and generalizes well.",
          "category": "correct"
        },
        {
          "text": "Use XGBoost; it can learn interactions.",
          "explanation": "Boosted trees can learn interactions, but FMs are purpose-built for sparse interaction-heavy problems.",
          "category": "partially_correct"
        },
        {
          "text": "Use a linear model with lots of engineered interaction features.",
          "explanation": "This can work but often becomes unwieldy; FMs learn interactions more efficiently.",
          "category": "partially_correct"
        },
        {
          "text": "Use DeepAR because CTR is a time series.",
          "explanation": "CTR prediction is typically a sparse supervised learning problem, not primarily a forecasting task.",
          "category": "incorrect"
        },
        {
          "text": "Use K-Means because clicks cluster naturally.",
          "explanation": "Clustering is unsupervised and doesn’t directly solve supervised CTR prediction.",
          "category": "incorrect"
        },
        {
          "text": "Use Image Classification—ads are basically pictures of clicks.",
          "explanation": "CTR prediction is usually tabular/sparse, not an image classification problem.",
          "category": "ridiculous"
        },
        {
          "text": "Sort features alphabetically to create interactions.",
          "explanation": "Feature ordering doesn’t create meaningful interaction modeling.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Problem → algorithm selection cheat-sheet (decisioning + tradeoffs)"
    },
    {
      "id": "5cad28df708735efb4d730086914efd112709bffee5f0e93326617c86ca778a2",
      "question": "You need unsupervised topic discovery across a large corpus of documents. Which built-in algorithms are designed for this and how do they differ?",
      "tags": [
        "algorithm_selection",
        "nlp",
        "topic_modeling",
        "lda",
        "ntm"
      ],
      "answers": [
        {
          "text": "LDA (probabilistic generative topic mixture model) and NTM (Neural Topic Model using neural/variational inference).",
          "explanation": "Both are topic models, but they differ in modeling assumptions and inference approach.",
          "category": "correct"
        },
        {
          "text": "LDA is Bayesian topic modeling with explicit distributions; NTM uses neural inference to learn topic structure and can behave differently at scale.",
          "explanation": "LDA uses classical probabilistic structure; NTM uses neural latent-variable modeling.",
          "category": "correct"
        },
        {
          "text": "Use LDA for topic modeling.",
          "explanation": "Correct for classical topic discovery, but NTM is also a built-in option for topic modeling.",
          "category": "partially_correct"
        },
        {
          "text": "Use NTM for topic modeling.",
          "explanation": "Correct, though LDA is also a standard choice; selection depends on tradeoffs and data scale.",
          "category": "partially_correct"
        },
        {
          "text": "Use BlazingText embeddings—embeddings are the same thing as topics.",
          "explanation": "Embeddings represent tokens/documents in vector space; topics are latent mixture components with different semantics.",
          "category": "incorrect"
        },
        {
          "text": "Use Random Cut Forest to cut documents into topics.",
          "explanation": "RCF is for anomaly detection, not topic modeling.",
          "category": "incorrect"
        },
        {
          "text": "Use PCA to discover principal topics.",
          "explanation": "PCA reduces numeric dimensionality; it doesn’t directly produce interpretable topic mixtures.",
          "category": "ridiculous"
        },
        {
          "text": "Count vowels per document and call them topics.",
          "explanation": "This doesn’t discover semantic structure.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Problem → algorithm selection cheat-sheet (decisioning + tradeoffs)"
    },
    {
      "id": "ff20ef33dc313fe729eb51fee6e9127f0181bafacaf12c766caa15953448dc37",
      "question": "You need to determine whether the task is image classification, object detection, or semantic segmentation. What question about the business need best distinguishes them?",
      "tags": [
        "problem_framing",
        "computer_vision",
        "task_selection",
        "labels"
      ],
      "answers": [
        {
          "text": "Do we need what is in the image, where it is (boxes), or the exact pixels (mask)?",
          "explanation": "Classification answers 'what', detection answers 'what + where', segmentation answers 'which pixels'.",
          "category": "correct"
        },
        {
          "text": "Is the output one label per image, bounding boxes per object, or a pixel-level label map?",
          "explanation": "The required output format drives the task type and labeling strategy.",
          "category": "correct"
        },
        {
          "text": "Ask whether you need labels or not.",
          "explanation": "You need labels in all three cases; the difference is the granularity of the labels.",
          "category": "partially_correct"
        },
        {
          "text": "Ask if you need to locate objects.",
          "explanation": "This distinguishes classification vs detection/segmentation, but not detection vs segmentation.",
          "category": "partially_correct"
        },
        {
          "text": "Ask what GPU you have; that determines the task type.",
          "explanation": "Hardware constraints matter later; task type is defined by the business requirement and output.",
          "category": "incorrect"
        },
        {
          "text": "Ask how large the dataset is; that decides detection vs segmentation.",
          "explanation": "Dataset size affects feasibility but does not define the task.",
          "category": "incorrect"
        },
        {
          "text": "Ask if the image feels like a dog—that’s segmentation.",
          "explanation": "Task definition must be tied to required outputs, not subjective feelings.",
          "category": "ridiculous"
        },
        {
          "text": "If it’s colorful, it’s object detection.",
          "explanation": "Color has nothing to do with task type.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Problem → algorithm selection cheat-sheet (decisioning + tradeoffs)"
    },
    {
      "id": "1b5d10e37c8d878b6e18ae92d93864079074ce8f2008f618839a4bdc284db597",
      "question": "What is Linear Learner used for, and what regularization options does it commonly support?",
      "tags": [
        "linear_learner",
        "tabular",
        "classification",
        "regression",
        "regularization"
      ],
      "answers": [
        {
          "text": "Linear Learner is for classification and regression (e.g., logistic/linear); it commonly supports L1/L2 and elastic-net-style regularization.",
          "explanation": "Regularization helps control overfitting and can encourage sparsity.",
          "category": "correct"
        },
        {
          "text": "It’s a scalable linear model; regularization (L1/L2) helps control overfitting and can encourage sparsity.",
          "explanation": "Linear models are strong baselines when the relationship is close to linear and interpretability matters.",
          "category": "correct"
        },
        {
          "text": "It’s for regression.",
          "explanation": "Yes, but it also supports classification (e.g., logistic).",
          "category": "partially_correct"
        },
        {
          "text": "It’s for classification.",
          "explanation": "Yes, but it also supports regression.",
          "category": "partially_correct"
        },
        {
          "text": "It’s for topic modeling.",
          "explanation": "Topic modeling is handled by LDA/NTM, not Linear Learner.",
          "category": "incorrect"
        },
        {
          "text": "It’s only for time series forecasting.",
          "explanation": "Forecasting is a separate problem class (e.g., DeepAR).",
          "category": "incorrect"
        },
        {
          "text": "It learns lines, so it’s only for geometry homework.",
          "explanation": "It’s a practical ML algorithm, not a geometry tutor.",
          "category": "ridiculous"
        },
        {
          "text": "Regularization means you run it at regular times.",
          "explanation": "Regularization is a penalty term to control model complexity.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Supervised tabular: classification / regression / ranking (core workhorse set)"
    },
    {
      "id": "06714558c521426134bc2731cf940012cdc886cc3cfb0693b1feb101dcb4b44f",
      "question": "When would you prefer XGBoost over a linear model for tabular prediction problems?",
      "tags": [
        "xgboost",
        "tabular",
        "model_selection",
        "nonlinear"
      ],
      "answers": [
        {
          "text": "When the target depends on nonlinearities and feature interactions that a linear model can’t capture well.",
          "explanation": "Boosted trees capture complex decision boundaries without manual interaction features.",
          "category": "correct"
        },
        {
          "text": "When you want strong tabular accuracy without heavy manual feature engineering (and you can afford tuning/compute).",
          "explanation": "XGBoost often has a higher performance ceiling but needs careful tuning.",
          "category": "correct"
        },
        {
          "text": "When you want higher accuracy.",
          "explanation": "Often true, but it’s specifically about nonlinear structure and interactions.",
          "category": "partially_correct"
        },
        {
          "text": "When the data is messy and not perfectly linear.",
          "explanation": "Yes, but you still should validate with proper evaluation and tuning.",
          "category": "partially_correct"
        },
        {
          "text": "When your data is only text.",
          "explanation": "Text is usually handled with NLP models/services, not classic tabular boosting directly.",
          "category": "incorrect"
        },
        {
          "text": "When you don’t have labels.",
          "explanation": "XGBoost is supervised; it requires labels.",
          "category": "incorrect"
        },
        {
          "text": "When you prefer the letter X.",
          "explanation": "Model choice must be requirement- and data-driven.",
          "category": "ridiculous"
        },
        {
          "text": "When you want the model to look scarier.",
          "explanation": "Naming doesn’t change suitability.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Supervised tabular: classification / regression / ranking (core workhorse set)"
    },
    {
      "id": "7a58b7639936fd9c78ce804cb4c5e5bb628a20bc8060636ca958d8ada06f3a3b",
      "question": "What practical advantage do tree-boosting algorithms like XGBoost have for nonlinear interactions in tabular data?",
      "tags": [
        "boosted_trees",
        "feature_interactions",
        "tabular",
        "nonlinear"
      ],
      "answers": [
        {
          "text": "Trees create nonlinear splits; boosting combines many trees to model complex patterns and interactions naturally.",
          "explanation": "They capture interactions without explicit cross-feature engineering.",
          "category": "correct"
        },
        {
          "text": "They often reduce the need to hand-engineer interaction terms compared with linear models.",
          "explanation": "This speeds iteration and often improves baseline performance.",
          "category": "correct"
        },
        {
          "text": "They’re better because they’re more complex.",
          "explanation": "Complexity helps only when it matches real structure and is controlled to avoid overfitting.",
          "category": "partially_correct"
        },
        {
          "text": "They can capture more patterns than a straight line.",
          "explanation": "Yes—especially nonlinearities and interactions.",
          "category": "partially_correct"
        },
        {
          "text": "They only work if features are normally distributed.",
          "explanation": "Boosted trees do not require normality assumptions.",
          "category": "incorrect"
        },
        {
          "text": "They require all features to be one-hot encoded.",
          "explanation": "They can handle numeric and encoded categorical features; one-hot is not always required.",
          "category": "incorrect"
        },
        {
          "text": "They’re better because trees are made of wood.",
          "explanation": "This is not how ML models work.",
          "category": "ridiculous"
        },
        {
          "text": "Boosting means the model drinks an energy drink.",
          "explanation": "Boosting is an ensemble training method.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Supervised tabular: classification / regression / ranking (core workhorse set)"
    },
    {
      "id": "b9c8ac6fce7fbdbbf9e01b41fed016d69cbeedb122fc3bcfdb00bdb544e6f081",
      "question": "You have a tabular dataset with many categorical variables. Which built-in option is highlighted as efficient for categorical data, and why?",
      "tags": [
        "tabular",
        "categorical_features",
        "algorithm_selection",
        "efficiency"
      ],
      "answers": [
        {
          "text": "CatBoost—optimized for categorical variables and often strong and efficient on category-heavy tabular data.",
          "explanation": "It uses strategies tailored to categorical handling and can reduce manual encoding burden.",
          "category": "correct"
        },
        {
          "text": "CatBoost is commonly chosen when categories dominate and you want good performance with less manual encoding work.",
          "explanation": "It’s designed to handle categorical-heavy tabular settings effectively.",
          "category": "correct"
        },
        {
          "text": "Use TabTransformer because it’s deep learning.",
          "explanation": "It can work well, but CatBoost is specifically known for categorical efficiency in many tabular cases.",
          "category": "partially_correct"
        },
        {
          "text": "Use XGBoost and one-hot everything.",
          "explanation": "Works, but may be less efficient and can blow up feature dimensionality.",
          "category": "partially_correct"
        },
        {
          "text": "Use PCA because it turns categories into numbers.",
          "explanation": "PCA operates on numeric vectors; it does not solve categorical handling by itself.",
          "category": "incorrect"
        },
        {
          "text": "Use LDA because categories are like topics.",
          "explanation": "LDA is for text topic modeling, not categorical tabular prediction.",
          "category": "incorrect"
        },
        {
          "text": "Rename categories to A/B/C and any model becomes categorical-friendly.",
          "explanation": "Renaming labels doesn’t create meaningful encodings.",
          "category": "ridiculous"
        },
        {
          "text": "Sort categories by length to encode them.",
          "explanation": "This is not a valid feature encoding strategy.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Supervised tabular: classification / regression / ranking (core workhorse set)"
    },
    {
      "id": "3a2bfd0f2debb1f59edad4e44b45b645fea14bf6de7cad129816c03964f177cc",
      "question": "What is the key idea behind TabTransformer, and when might it outperform simpler tabular baselines?",
      "tags": [
        "tabtransformer",
        "deep_learning",
        "tabular",
        "categorical_embeddings"
      ],
      "answers": [
        {
          "text": "Embed categorical features and apply Transformer attention to learn interactions; combine with numeric features for prediction.",
          "explanation": "Attention can capture complex categorical interactions beyond simple encodings.",
          "category": "correct"
        },
        {
          "text": "It can outperform simpler baselines when categorical interactions are complex and you have enough data to benefit from deep tabular modeling.",
          "explanation": "Deep tabular models often need more data and careful regularization.",
          "category": "correct"
        },
        {
          "text": "It’s a neural net for tables.",
          "explanation": "Yes, but the core idea is attention over embedded categorical features.",
          "category": "partially_correct"
        },
        {
          "text": "It uses attention on tabular data.",
          "explanation": "Correct, especially for modeling interactions among categorical tokens.",
          "category": "partially_correct"
        },
        {
          "text": "It’s only for time series forecasting.",
          "explanation": "TabTransformer targets tabular prediction problems, not forecasting.",
          "category": "incorrect"
        },
        {
          "text": "It’s a topic model for tables.",
          "explanation": "Topic modeling is an NLP task; TabTransformer is supervised tabular modeling.",
          "category": "incorrect"
        },
        {
          "text": "It transforms your table into a chair.",
          "explanation": "Not applicable.",
          "category": "ridiculous"
        },
        {
          "text": "It’s a spreadsheet macro that predicts the future.",
          "explanation": "It’s a machine learning architecture, not a spreadsheet feature.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Supervised tabular: classification / regression / ranking (core workhorse set)"
    },
    {
      "id": "c1590cb6257c454446139efffb416323907bb08d032c84e461fb2ab23e7431aa",
      "question": "In what kinds of situations would an AutoML tabular approach (e.g., AutoGluon-Tabular) be a good default starting point?",
      "tags": [
        "automl",
        "autogluon_tabular",
        "tabular",
        "baseline"
      ],
      "answers": [
        {
          "text": "When you want a strong baseline fast with automated model selection, tuning, and ensembling/stacking.",
          "explanation": "AutoML helps benchmark feasibility early and often yields strong first-pass results.",
          "category": "correct"
        },
        {
          "text": "Early in a project: use AutoML to set a performance benchmark and learn what matters before manual optimization.",
          "explanation": "It accelerates iteration and informs whether custom work is worth it.",
          "category": "correct"
        },
        {
          "text": "When you don’t know what model to pick.",
          "explanation": "True, but the value is in automated selection/tuning and a strong benchmark, not just indecision.",
          "category": "partially_correct"
        },
        {
          "text": "When you want decent results with minimal effort.",
          "explanation": "Often true, though you still need good evaluation and data preparation.",
          "category": "partially_correct"
        },
        {
          "text": "When you must guarantee the best model possible.",
          "explanation": "AutoML can be strong but doesn’t guarantee a global optimum for all problems.",
          "category": "incorrect"
        },
        {
          "text": "Only when you have zero labeled data.",
          "explanation": "Supervised AutoML typically requires labels.",
          "category": "incorrect"
        },
        {
          "text": "When you want the model to do your taxes too.",
          "explanation": "Not related.",
          "category": "ridiculous"
        },
        {
          "text": "When you want the computer to feel appreciated.",
          "explanation": "Not a technical criterion.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Supervised tabular: classification / regression / ranking (core workhorse set)"
    },
    {
      "id": "c81ede70ae49e7eb5aaffe71dfe3c55473dbb5009aa9037e84a6fd38a18edf14",
      "question": "Why can hyperparameter tuning be especially important for gradient-boosted trees (e.g., XGBoost-style models)?",
      "tags": [
        "hyperparameter_tuning",
        "xgboost",
        "boosted_trees",
        "generalization"
      ],
      "answers": [
        {
          "text": "They’re sensitive to depth, learning rate, number of trees, subsampling, and regularization—tuning controls bias/variance and overfitting.",
          "explanation": "Small changes can materially alter generalization and stability.",
          "category": "correct"
        },
        {
          "text": "Good tuning balances model capacity and generalization (often with early stopping and validation monitoring).",
          "explanation": "Early stopping and validation-based selection reduce overfitting risk.",
          "category": "correct"
        },
        {
          "text": "Because tuning improves performance.",
          "explanation": "Yes, but the key is controlling bias/variance and overfitting.",
          "category": "partially_correct"
        },
        {
          "text": "Because defaults aren’t always optimal.",
          "explanation": "Correct—data characteristics vary widely across tabular problems.",
          "category": "partially_correct"
        },
        {
          "text": "XGBoost won’t run unless you tune at least 20 parameters.",
          "explanation": "It runs with defaults; tuning improves outcomes, not feasibility.",
          "category": "incorrect"
        },
        {
          "text": "Hyperparameters don’t matter for boosted trees.",
          "explanation": "They matter a lot; poor settings can cause under/overfitting.",
          "category": "incorrect"
        },
        {
          "text": "Because they’re called hyper and need attention.",
          "explanation": "Names are irrelevant; sensitivity is the real reason.",
          "category": "ridiculous"
        },
        {
          "text": "Because the model gets jealous otherwise.",
          "explanation": "Models don’t have emotions.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Supervised tabular: classification / regression / ranking (core workhorse set)"
    },
    {
      "id": "cd1030705ee3bb872ad06799594dc6a587f675dbe7ec5f9b99333779eaa8e63c",
      "question": "What is the risk of using a very flexible model (like boosted trees or deep tabular nets) on small tabular datasets, and what’s your first mitigation?",
      "tags": [
        "overfitting",
        "tabular",
        "model_complexity",
        "regularization"
      ],
      "answers": [
        {
          "text": "Risk: overfitting; mitigate first with cross-validation + regularization/early stopping, and reduce model complexity.",
          "explanation": "Validation-based control prevents memorization and unstable generalization.",
          "category": "correct"
        },
        {
          "text": "Use validation-based early stopping and simplify features/model; prefer strong baselines and cautious tuning.",
          "explanation": "Smaller datasets demand stronger guardrails and simpler hypotheses.",
          "category": "correct"
        },
        {
          "text": "They may be too slow; use a faster instance.",
          "explanation": "Compute can help runtime, but doesn’t address generalization risk.",
          "category": "partially_correct"
        },
        {
          "text": "They might memorize; use some regularization.",
          "explanation": "Right direction; include proper validation to confirm improvements.",
          "category": "partially_correct"
        },
        {
          "text": "Small datasets can’t overfit.",
          "explanation": "Small datasets are often more prone to overfitting.",
          "category": "incorrect"
        },
        {
          "text": "Overfitting is solved by adding more epochs.",
          "explanation": "More training can worsen overfitting; use early stopping and regularization instead.",
          "category": "incorrect"
        },
        {
          "text": "Overfitting is impossible if you believe hard enough.",
          "explanation": "Overfitting is a statistical phenomenon.",
          "category": "ridiculous"
        },
        {
          "text": "Just rename the columns to confuse the model.",
          "explanation": "Renaming features doesn’t change information content.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Supervised tabular: classification / regression / ranking (core workhorse set)"
    },
    {
      "id": "eda571e87df2dea86a524a17c2592ccefc71429c864f679d3f455984a8bc5388",
      "question": "If you must choose one model family to produce a strong tabular baseline quickly, what tradeoffs separate Linear Learner vs XGBoost/boosted trees?",
      "tags": [
        "model_comparison",
        "linear_learner",
        "xgboost",
        "tabular",
        "tradeoffs"
      ],
      "answers": [
        {
          "text": "Linear Learner: fast, simple, interpretable but can underfit nonlinear patterns; Boosted trees: typically stronger on complex tabular data but need tuning/compute and can overfit.",
          "explanation": "Choose based on complexity of relationships, need for interpretability, and operational constraints.",
          "category": "correct"
        },
        {
          "text": "Choose linear for speed/interpretability and well-behaved linear signals; choose boosted trees for nonlinear interactions and higher ceiling.",
          "explanation": "Boosted trees often win on accuracy; linear models win on simplicity and explainability.",
          "category": "correct"
        },
        {
          "text": "Linear is simple; XGBoost is complex.",
          "explanation": "True, but the practical tradeoffs are interpretability, tuning, and generalization behavior.",
          "category": "partially_correct"
        },
        {
          "text": "Linear is faster; XGBoost is often more accurate.",
          "explanation": "Commonly true; verify with validation and tuning.",
          "category": "partially_correct"
        },
        {
          "text": "Linear Learner always beats XGBoost if you normalize features.",
          "explanation": "Normalization doesn’t make nonlinear interactions disappear.",
          "category": "incorrect"
        },
        {
          "text": "Boosted trees are always worse than linear models.",
          "explanation": "Boosted trees frequently outperform linear models on complex tabular problems.",
          "category": "incorrect"
        },
        {
          "text": "Choose the one with the cooler logo.",
          "explanation": "Not a technical basis for selection.",
          "category": "ridiculous"
        },
        {
          "text": "Choose whichever starts with the same letter as your name.",
          "explanation": "Not a valid criterion.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Supervised tabular: classification / regression / ranking (core workhorse set)"
    },
    {
      "id": "22010babd978d7bc2150512b080828c85805420340d8b8ac1af2cdba820f2936",
      "question": "What makes DeepAR Forecasting different from plain regression on time index for time-series prediction?",
      "tags": [
        "deepar",
        "time_series",
        "forecasting",
        "probabilistic"
      ],
      "answers": [
        {
          "text": "DeepAR is a sequence model that learns temporal dependencies and covariate effects; it produces probabilistic forecasts.",
          "explanation": "It models sequence structure and uncertainty, not just time as a single feature.",
          "category": "correct"
        },
        {
          "text": "It’s a global neural forecaster trained across series, not a hand-built regression on time.",
          "explanation": "Training across related series lets it share learned patterns.",
          "category": "correct"
        },
        {
          "text": "DeepAR is a neural network for time series.",
          "explanation": "Yes, but its key value is sequence modeling plus probabilistic forecasting.",
          "category": "partially_correct"
        },
        {
          "text": "It’s more advanced than linear regression.",
          "explanation": "True, but the specific difference is sequence modeling and uncertainty output.",
          "category": "partially_correct"
        },
        {
          "text": "DeepAR is just linear regression with a fancy name.",
          "explanation": "It is a neural sequence model, not a simple linear regression.",
          "category": "incorrect"
        },
        {
          "text": "DeepAR is for text embeddings.",
          "explanation": "Embeddings are an NLP concept; DeepAR targets forecasting.",
          "category": "incorrect"
        },
        {
          "text": "DeepAR forecasts by tarot cards, but deeply.",
          "explanation": "Forecasting is data-driven, not mystical.",
          "category": "ridiculous"
        },
        {
          "text": "It predicts by asking the calendar nicely.",
          "explanation": "The model learns from historical patterns and covariates.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Time-series forecasting (DeepAR and probabilistic forecasting)"
    },
    {
      "id": "b839474be57d867c9306edaf7ea8c3a42579fd9d5d8175554df9c14815587e72",
      "question": "DeepAR is described as a probabilistic forecaster. What does that mean operationally for the outputs you get?",
      "tags": [
        "deepar",
        "forecast_distribution",
        "uncertainty",
        "prediction_intervals"
      ],
      "answers": [
        {
          "text": "Outputs a distribution/quantiles so you get prediction intervals and uncertainty, not only a point estimate.",
          "explanation": "This supports risk-aware decisions like safety stock or capacity buffers.",
          "category": "correct"
        },
        {
          "text": "You can act on risk using confidence bands and tail probabilities (e.g., quantile forecasts).",
          "explanation": "Probabilistic outputs make uncertainty explicit and operationally usable.",
          "category": "correct"
        },
        {
          "text": "It gives a range instead of one number.",
          "explanation": "Yes—commonly via quantiles/intervals derived from a predictive distribution.",
          "category": "partially_correct"
        },
        {
          "text": "It provides uncertainty.",
          "explanation": "Correct—uncertainty is part of the output, not just a byproduct.",
          "category": "partially_correct"
        },
        {
          "text": "Probabilistic means it guesses randomly.",
          "explanation": "It estimates uncertainty, but forecasts are still learned and structured.",
          "category": "incorrect"
        },
        {
          "text": "It outputs only one number but calls it probabilistic.",
          "explanation": "Probabilistic forecasting explicitly provides uncertainty information.",
          "category": "incorrect"
        },
        {
          "text": "It outputs the probability that the future will exist.",
          "explanation": "Not meaningful.",
          "category": "ridiculous"
        },
        {
          "text": "It predicts the odds of rain in your spreadsheet.",
          "explanation": "DeepAR forecasts your target series; weather requires different data/models.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Time-series forecasting (DeepAR and probabilistic forecasting)"
    },
    {
      "id": "c5c28ede8e9734a79fdbf599705a2caf67b296d0c5be1cfbc00584563086ed67",
      "question": "Why is DeepAR especially useful when you have many related time series (e.g., many products or SKUs)?",
      "tags": [
        "deepar",
        "global_model",
        "time_series",
        "many_series"
      ],
      "answers": [
        {
          "text": "It learns a shared/global model across series, letting sparse series benefit from common patterns (trend/seasonality).",
          "explanation": "Pooling data improves learning for low-volume series.",
          "category": "correct"
        },
        {
          "text": "Pooling information improves accuracy for low-volume series compared to training one model per series.",
          "explanation": "A global model can generalize shared behaviors across entities.",
          "category": "correct"
        },
        {
          "text": "It can handle lots of time series.",
          "explanation": "Yes, but the key advantage is knowledge sharing across series.",
          "category": "partially_correct"
        },
        {
          "text": "It scales to many SKUs.",
          "explanation": "True, and it can leverage common structure across them.",
          "category": "partially_correct"
        },
        {
          "text": "It requires exactly one time series.",
          "explanation": "DeepAR is designed to work well with many related series.",
          "category": "incorrect"
        },
        {
          "text": "It only works when every SKU is identical.",
          "explanation": "They can differ; the model learns shared and individual patterns.",
          "category": "incorrect"
        },
        {
          "text": "It predicts deep-sea fish populations only.",
          "explanation": "Name-based joke; not relevant.",
          "category": "ridiculous"
        },
        {
          "text": "It needs one model per timestamp.",
          "explanation": "Timestamps are inputs, not separate models.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Time-series forecasting (DeepAR and probabilistic forecasting)"
    },
    {
      "id": "7bf078f5ece78c299989dc69a68eab62e8c07eac9929fcb3134bcafe674f10ff",
      "question": "What input data structure choices matter most when preparing data for a DeepAR-style forecasting model?",
      "tags": [
        "time_series",
        "data_preparation",
        "covariates",
        "series_id"
      ],
      "answers": [
        {
          "text": "Consistent time granularity, correct series IDs, aligned timestamps, missing-value handling, and meaningful covariates (known-future vs observed).",
          "explanation": "Alignment and covariate design are critical to avoid leakage and instability.",
          "category": "correct"
        },
        {
          "text": "Clean, aligned sequences per entity plus proper covariate design are more important than fancy model tweaks.",
          "explanation": "Bad structure and leakage can ruin forecast quality.",
          "category": "correct"
        },
        {
          "text": "Make sure timestamps are sorted.",
          "explanation": "Sorting helps, but alignment and IDs/covariates are also crucial.",
          "category": "partially_correct"
        },
        {
          "text": "Use a consistent time interval.",
          "explanation": "Correct—consistent granularity is foundational.",
          "category": "partially_correct"
        },
        {
          "text": "Shuffle time steps to avoid bias.",
          "explanation": "Shuffling breaks temporal dependencies, which are essential in forecasting.",
          "category": "incorrect"
        },
        {
          "text": "Drop the timestamps and keep only the values.",
          "explanation": "Timestamps define sequence ordering and seasonality structure.",
          "category": "incorrect"
        },
        {
          "text": "Use random timestamps; the model will learn time.",
          "explanation": "Random timestamps destroy meaningful time structure.",
          "category": "ridiculous"
        },
        {
          "text": "Write dates in emojis for better generalization.",
          "explanation": "Encoding style won’t fix structural issues.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Time-series forecasting (DeepAR and probabilistic forecasting)"
    },
    {
      "id": "17bcb686f9d9a8dd90c00e69a5f441eb456362b74783cb7a9bc40a7decb4db8a",
      "question": "In a forecasting system, what is a common failure mode when seasonal patterns shift over time, and how would you monitor it?",
      "tags": [
        "time_series",
        "seasonality",
        "drift",
        "monitoring"
      ],
      "answers": [
        {
          "text": "Concept drift: seasonal effects change (promos, behavior shifts) causing systematic residual patterns; monitor rolling error, bias by segment, and residual distribution shifts.",
          "explanation": "Persistent bias indicates drift and the need to investigate/retrain.",
          "category": "correct"
        },
        {
          "text": "Track forecast error over time and feature/residual drift; trigger investigation/retraining when persistent bias appears.",
          "explanation": "Monitoring both accuracy and drift signals helps catch regime changes early.",
          "category": "correct"
        },
        {
          "text": "Seasonality changes so accuracy drops; retrain.",
          "explanation": "Retraining helps, but monitoring should detect when/where drift occurs and quantify impact.",
          "category": "partially_correct"
        },
        {
          "text": "Monitor MAPE/RMSE and retrain if it worsens.",
          "explanation": "Good start; also examine residual patterns and segments for systematic drift.",
          "category": "partially_correct"
        },
        {
          "text": "Seasonality never changes, so monitoring isn’t needed.",
          "explanation": "Seasonality can and does change in real systems.",
          "category": "incorrect"
        },
        {
          "text": "If seasonality shifts, just increase model depth.",
          "explanation": "More complexity may not fix regime change; you need updated data and proper monitoring.",
          "category": "incorrect"
        },
        {
          "text": "Rename months until it fits.",
          "explanation": "Renaming does not change underlying demand patterns.",
          "category": "ridiculous"
        },
        {
          "text": "If winter looks like summer, swap the labels.",
          "explanation": "This is not a valid monitoring or correction approach.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Time-series forecasting (DeepAR and probabilistic forecasting)"
    },
    {
      "id": "1819d6f8054c5b8a3c2524134ed425de5e0b3083e64b5eb387ddf602a7ff97c9",
      "question": "What is the core modeling idea behind Factorization Machines, and why are they a good fit for sparse recommendation/CTR problems?",
      "tags": [
        "factorization_machines",
        "recommendation",
        "ctr",
        "sparse_data"
      ],
      "answers": [
        {
          "text": "Linear model plus efficient pairwise interactions via low-dimensional latent vectors—great for sparse one-hot IDs.",
          "explanation": "FMs capture interactions without exploding feature crosses.",
          "category": "correct"
        },
        {
          "text": "FMs learn embeddings per feature and use them to estimate interactions without enumerating all cross-features.",
          "explanation": "This keeps parameter count manageable and improves generalization on sparse data.",
          "category": "correct"
        },
        {
          "text": "They’re good for recommendations.",
          "explanation": "True, but the reason is efficient interaction modeling on sparse features.",
          "category": "partially_correct"
        },
        {
          "text": "They work well on sparse data.",
          "explanation": "Yes—especially when interactions matter and one-hot features dominate.",
          "category": "partially_correct"
        },
        {
          "text": "They only work on images.",
          "explanation": "FMs target tabular/sparse supervised learning, not images.",
          "category": "incorrect"
        },
        {
          "text": "They require dense continuous features only.",
          "explanation": "They’re specifically popular for sparse one-hot settings.",
          "category": "incorrect"
        },
        {
          "text": "They factorize machines—like breaking a laptop into primes.",
          "explanation": "Factorization refers to low-rank decomposition of interaction structure.",
          "category": "ridiculous"
        },
        {
          "text": "They recommend items by reading your mind.",
          "explanation": "They learn from data, not telepathy.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Recommenders + sparse / high-dimensional data (Factorization Machines)"
    },
    {
      "id": "cdcc25d1118055469cd09a0fa5e57cd6fadf00c8d895d5a6861c2a5484ff9157",
      "question": "What does low-rank factorization mean in Factorization Machines, and what benefit does it provide?",
      "tags": [
        "factorization_machines",
        "low_rank",
        "embeddings",
        "generalization"
      ],
      "answers": [
        {
          "text": "Represent interaction matrix with low-dimensional embeddings (rank k), cutting parameters and improving generalization.",
          "explanation": "Low-rank structure makes interaction learning feasible and less prone to overfitting.",
          "category": "correct"
        },
        {
          "text": "Low-rank means a compact latent space for interactions—efficient training/inference on sparse data.",
          "explanation": "It avoids explicitly storing all pairwise interaction weights.",
          "category": "correct"
        },
        {
          "text": "It compresses the model.",
          "explanation": "Yes—by reducing parameters for interactions.",
          "category": "partially_correct"
        },
        {
          "text": "It reduces parameters.",
          "explanation": "Correct; fewer parameters improves feasibility and often generalization.",
          "category": "partially_correct"
        },
        {
          "text": "Low-rank means low quality.",
          "explanation": "Low-rank is a structural assumption, not a quality judgment.",
          "category": "incorrect"
        },
        {
          "text": "Low-rank means fewer training examples.",
          "explanation": "Rank refers to parameter structure, not dataset size.",
          "category": "incorrect"
        },
        {
          "text": "Low-rank means it’s ranked #9999 on Kaggle.",
          "explanation": "Rank here is linear-algebra rank.",
          "category": "ridiculous"
        },
        {
          "text": "It means your model is a private in the army.",
          "explanation": "Not applicable.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Recommenders + sparse / high-dimensional data (Factorization Machines)"
    },
    {
      "id": "aa46ecb96669c5f9a386286fdcf53bb2bb458cfbad44f53c0c2cbf762d5a8b8f",
      "question": "In recommendation settings, what is a common cold-start issue, and why can Factorization Machines help?",
      "tags": [
        "recommendation",
        "cold_start",
        "factorization_machines",
        "sparse_features"
      ],
      "answers": [
        {
          "text": "Cold-start: new users/items lack history; FMs can use side/context features (attributes) so predictions are possible even with sparse interactions.",
          "explanation": "Side features activate learned latent factors even when user-item history is limited.",
          "category": "correct"
        },
        {
          "text": "Because features beyond user-item IDs (device, category, region) still activate learned latent factors.",
          "explanation": "This helps generalize to new entities using their attributes.",
          "category": "correct"
        },
        {
          "text": "Cold-start means you have no clicks; you need more data.",
          "explanation": "True, but using side features and shared factors can still provide workable predictions.",
          "category": "partially_correct"
        },
        {
          "text": "Use metadata features to help with cold-start.",
          "explanation": "Yes—FMs naturally incorporate metadata/side features.",
          "category": "partially_correct"
        },
        {
          "text": "Cold-start is fixed by increasing k in k-means.",
          "explanation": "Clustering doesn’t solve cold-start for supervised recommendation directly.",
          "category": "incorrect"
        },
        {
          "text": "Cold-start disappears if you normalize features.",
          "explanation": "Normalization doesn’t create historical interactions.",
          "category": "incorrect"
        },
        {
          "text": "Use a space heater—no more cold-start.",
          "explanation": "Cold-start refers to missing interaction history, not temperature.",
          "category": "ridiculous"
        },
        {
          "text": "Rename new users as old users.",
          "explanation": "This would corrupt identity and data integrity.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Recommenders + sparse / high-dimensional data (Factorization Machines)"
    },
    {
      "id": "5da5f7de76c5252a68ca645218797c39105a55d8c952120fd5de87bb1d602067",
      "question": "What is BlazingText used for, and what two major task types does it support?",
      "tags": [
        "blazingtext",
        "nlp",
        "embeddings",
        "text_classification"
      ],
      "answers": [
        {
          "text": "For word embeddings (Word2Vec-style) and text classification (fast supervised classification).",
          "explanation": "BlazingText supports both representation learning and supervised text labeling.",
          "category": "correct"
        },
        {
          "text": "It supports learning embeddings and using them directly (or training supervised classifiers) for NLP tasks.",
          "explanation": "Embeddings can feed downstream tasks; supervised mode trains a classifier.",
          "category": "correct"
        },
        {
          "text": "It’s for NLP.",
          "explanation": "Yes, but the key is embeddings and supervised text classification.",
          "category": "partially_correct"
        },
        {
          "text": "It builds embeddings.",
          "explanation": "Correct, and it also supports supervised text classification.",
          "category": "partially_correct"
        },
        {
          "text": "It’s for image classification.",
          "explanation": "BlazingText is an NLP algorithm.",
          "category": "incorrect"
        },
        {
          "text": "It clusters images into topics.",
          "explanation": "Topic modeling is LDA/NTM, and images aren’t its domain.",
          "category": "incorrect"
        },
        {
          "text": "It generates flaming hot takes on social media.",
          "explanation": "Not its function.",
          "category": "ridiculous"
        },
        {
          "text": "It literally sets your text on fire for speed.",
          "explanation": "It’s an algorithm name, not a physical process.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "NLP / text: embeddings, classification, seq2seq, topic modeling"
    },
    {
      "id": "1f15f604239d28a333f484b678439c078cf4ecde278aaf4eaaedc8b1697fe4e1",
      "question": "Why are word embeddings useful for downstream NLP tasks like similarity search or classification?",
      "tags": [
        "embeddings",
        "nlp",
        "similarity",
        "representation_learning"
      ],
      "answers": [
        {
          "text": "They map tokens into vectors where semantic similarity corresponds to closeness, enabling nearest-neighbor search and better downstream models.",
          "explanation": "Vector geometry can represent meaning and similarity efficiently.",
          "category": "correct"
        },
        {
          "text": "Dense representations capture meaning and reduce sparsity versus one-hot vectors, improving generalization.",
          "explanation": "Embeddings provide compact, informative features for ML models.",
          "category": "correct"
        },
        {
          "text": "Embeddings turn words into numbers.",
          "explanation": "Yes, but the key is that the numbers preserve semantic relationships.",
          "category": "partially_correct"
        },
        {
          "text": "Embeddings help models understand similarity.",
          "explanation": "Correct—similar words tend to be close in embedding space.",
          "category": "partially_correct"
        },
        {
          "text": "Embeddings remove meaning so models don’t get confused.",
          "explanation": "Embeddings aim to preserve semantic meaning, not remove it.",
          "category": "incorrect"
        },
        {
          "text": "Embeddings are just compression with no semantics.",
          "explanation": "They are learned to encode semantic structure.",
          "category": "incorrect"
        },
        {
          "text": "Embeddings store every word inside a tiny USB drive.",
          "explanation": "Embeddings are numeric vectors, not storage devices.",
          "category": "ridiculous"
        },
        {
          "text": "Embeddings are stickers you put on sentences.",
          "explanation": "Not applicable.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "NLP / text: embeddings, classification, seq2seq, topic modeling"
    },
    {
      "id": "876971f1b0b306146b8f3ab3491493cb663815f2bb364ae6fea9488d41dc5f9b",
      "question": "What is a sequence-to-sequence (seq2seq) model, and what kinds of NLP problems is it typically used to solve?",
      "tags": [
        "seq2seq",
        "nlp",
        "machine_translation",
        "summarization"
      ],
      "answers": [
        {
          "text": "Encoder–decoder mapping input sequence → output sequence (translation, summarization, transformation/generation).",
          "explanation": "Seq2seq handles variable-length input/output and structured generation.",
          "category": "correct"
        },
        {
          "text": "Useful when output length differs from input and structure matters (paraphrase, normalization, extraction-style generation).",
          "explanation": "It models conditional sequence generation.",
          "category": "correct"
        },
        {
          "text": "It converts one text into another.",
          "explanation": "Yes—seq2seq is a general sequence transformation framework.",
          "category": "partially_correct"
        },
        {
          "text": "It’s used for translation.",
          "explanation": "Translation is a core seq2seq use case, along with summarization and more.",
          "category": "partially_correct"
        },
        {
          "text": "It’s only for clustering documents.",
          "explanation": "Clustering is unsupervised; seq2seq is a supervised generation/transformation setup.",
          "category": "incorrect"
        },
        {
          "text": "It’s a regression model for time series.",
          "explanation": "Seq2seq can be adapted broadly, but in this context it refers to NLP sequence modeling.",
          "category": "incorrect"
        },
        {
          "text": "It turns sentences into snakes and back again.",
          "explanation": "Not applicable.",
          "category": "ridiculous"
        },
        {
          "text": "It’s a model that only counts sequences.",
          "explanation": "It transforms/generates sequences, not just counts them.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "NLP / text: embeddings, classification, seq2seq, topic modeling"
    },
    {
      "id": "d54da27070b53d8c4b5a1be4c04d2e8b3b7436f5166f000e99281263532b3889",
      "question": "Compare LDA vs Neural Topic Model (NTM): what’s a key conceptual difference in how they represent topics?",
      "tags": [
        "lda",
        "ntm",
        "topic_modeling",
        "nlp"
      ],
      "answers": [
        {
          "text": "LDA is a probabilistic generative mixture model; NTM uses neural (often variational) inference to learn topic representations.",
          "explanation": "They differ in assumptions and inference mechanisms.",
          "category": "correct"
        },
        {
          "text": "LDA has explicit Dirichlet topic distributions; NTM learns topics via neural latent variables and amortized inference.",
          "explanation": "NTM replaces classical inference with learned neural inference.",
          "category": "correct"
        },
        {
          "text": "LDA is classical; NTM is neural.",
          "explanation": "Correct, but the deeper distinction is probabilistic mixture modeling vs neural latent-variable inference.",
          "category": "partially_correct"
        },
        {
          "text": "They both find topics, but NTM uses deep learning.",
          "explanation": "Yes—though both aim to represent documents as mixtures over latent topics.",
          "category": "partially_correct"
        },
        {
          "text": "NTM is supervised and requires topic labels.",
          "explanation": "Topic modeling is typically unsupervised.",
          "category": "incorrect"
        },
        {
          "text": "LDA is only for images.",
          "explanation": "LDA is a text topic model.",
          "category": "incorrect"
        },
        {
          "text": "LDA uses Legal Document Analysis; NTM uses Newspaper Topic Magic.",
          "explanation": "These are not the algorithm meanings.",
          "category": "ridiculous"
        },
        {
          "text": "Topics are discovered by alphabetical order.",
          "explanation": "Topic discovery is statistical, not alphabetical.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "NLP / text: embeddings, classification, seq2seq, topic modeling"
    },
    {
      "id": "000c0cd046613b11e9917ddbdaefc2d3737c7b5c720e90f55dbe241dc3e2aa53",
      "question": "When would you choose Text Classification – TensorFlow (transfer learning) instead of training a text classifier from scratch?",
      "tags": [
        "text_classification",
        "transfer_learning",
        "tensorflow",
        "limited_data"
      ],
      "answers": [
        {
          "text": "Choose transfer learning with limited labels or when you want strong performance fast by fine-tuning a pretrained model.",
          "explanation": "Pretrained language representations reduce data needs and speed convergence.",
          "category": "correct"
        },
        {
          "text": "Train from scratch mainly when you have massive domain-specific data or highly specialized constraints that pretrained models can’t meet.",
          "explanation": "Scratch training is costly and typically needs lots of data to outperform fine-tuning.",
          "category": "correct"
        },
        {
          "text": "Use transfer learning when you want better accuracy.",
          "explanation": "Often true, but especially when labels are limited or time-to-value matters.",
          "category": "partially_correct"
        },
        {
          "text": "Use transfer learning when you have less data.",
          "explanation": "A common and correct motivation.",
          "category": "partially_correct"
        },
        {
          "text": "Always train from scratch; pretrained models never help.",
          "explanation": "Pretrained models often provide strong gains, especially with limited labels.",
          "category": "incorrect"
        },
        {
          "text": "Transfer learning only works for images, not text.",
          "explanation": "Transfer learning is widely used in NLP (e.g., fine-tuning pretrained language models).",
          "category": "incorrect"
        },
        {
          "text": "Train from scratch to build character and discipline.",
          "explanation": "Choose based on requirements and constraints, not motivation.",
          "category": "ridiculous"
        },
        {
          "text": "Use transfer learning because it’s lazy learning.",
          "explanation": "Transfer learning is an effective engineering strategy, not laziness.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "NLP / text: embeddings, classification, seq2seq, topic modeling"
    },
    {
      "id": "af5dfa1b3648002aef01d8f42a07940d4bf6f13d98d921eb48528dacabe9ff91",
      "question": "What is the key difference between Image Classification and Object Detection from a labeling and output perspective?",
      "tags": [
        "computer_vision",
        "image_classification",
        "object_detection",
        "labels"
      ],
      "answers": [
        {
          "text": "Classification: label(s) for the whole image; Detection: multiple objects with boxes + class + confidence.",
          "explanation": "Detection localizes instances, not just overall image category.",
          "category": "correct"
        },
        {
          "text": "Detection localizes and classifies instances; classification does not localize.",
          "explanation": "Output granularity (global vs per-instance) defines the task.",
          "category": "correct"
        },
        {
          "text": "Detection finds objects; classification labels images.",
          "explanation": "Correct—detection includes localization.",
          "category": "partially_correct"
        },
        {
          "text": "Classification is one label; detection is many.",
          "explanation": "Often true, though classification can be multi-label; key is localization.",
          "category": "partially_correct"
        },
        {
          "text": "Classification requires bounding boxes; detection doesn’t.",
          "explanation": "It’s the opposite: detection requires boxes; classification usually does not.",
          "category": "incorrect"
        },
        {
          "text": "Detection outputs only a single label.",
          "explanation": "Detection outputs multiple instances with coordinates and labels.",
          "category": "incorrect"
        },
        {
          "text": "Classification is when you classify the camera, not the picture.",
          "explanation": "Not how vision tasks are defined.",
          "category": "ridiculous"
        },
        {
          "text": "Detection is when the model detects vibes.",
          "explanation": "Detection refers to locating objects with coordinates.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Computer vision: image classification, object detection, semantic segmentation"
    },
    {
      "id": "12167a4b7e7bb67059c99e80adf1815a6cd2e35c1289f6ea5e51cacd0a14874e",
      "question": "What is the purpose of bounding boxes in object detection, and what does “multiple classes” imply for model outputs?",
      "tags": [
        "object_detection",
        "bounding_boxes",
        "multiclass",
        "outputs"
      ],
      "answers": [
        {
          "text": "Boxes specify where the object is; multiple classes means each box has a class prediction and confidence, possibly many classes across boxes.",
          "explanation": "Detectors typically output coordinates plus class probabilities/scores per detection.",
          "category": "correct"
        },
        {
          "text": "Outputs typically include coordinates, class probabilities, and scores for each detected instance.",
          "explanation": "Multiple classes expands the label space for each detected box.",
          "category": "correct"
        },
        {
          "text": "Boxes show object location.",
          "explanation": "Correct; also include class labels/scores per box.",
          "category": "partially_correct"
        },
        {
          "text": "Multiple classes means multiple label types are possible.",
          "explanation": "Yes—each detection includes a class label among multiple possible classes.",
          "category": "partially_correct"
        },
        {
          "text": "Bounding boxes are only for training; they disappear at inference.",
          "explanation": "At inference, boxes are a key output of detection.",
          "category": "incorrect"
        },
        {
          "text": "Multiple classes means multiple boxes for the same object always.",
          "explanation": "Multiple classes refers to label space, not guaranteed duplicate boxes.",
          "category": "incorrect"
        },
        {
          "text": "Bounding boxes keep objects from escaping the image.",
          "explanation": "Boxes are metadata outputs, not containment.",
          "category": "ridiculous"
        },
        {
          "text": "Multiple classes means the model is taking multiple courses.",
          "explanation": "Not applicable.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Computer vision: image classification, object detection, semantic segmentation"
    },
    {
      "id": "39661fd28b45def760ae4f5aabebb2f475115fdd3962d69180d55b1d0fd0b184",
      "question": "When would you choose Semantic Segmentation instead of object detection for an image problem?",
      "tags": [
        "semantic_segmentation",
        "computer_vision",
        "pixel_labels",
        "use_cases"
      ],
      "answers": [
        {
          "text": "When you need pixel-level masks (exact region boundaries), not coarse boxes (e.g., medical masks, lanes, background removal).",
          "explanation": "Segmentation provides precise regions per class.",
          "category": "correct"
        },
        {
          "text": "Choose segmentation when ‘how much/which pixels’ matters more than ‘roughly where’.",
          "explanation": "Pixel precision is the key difference.",
          "category": "correct"
        },
        {
          "text": "When you need more detail than detection.",
          "explanation": "Yes—specifically pixel-level boundaries.",
          "category": "partially_correct"
        },
        {
          "text": "When boxes are too coarse.",
          "explanation": "Correct—segmentation yields masks rather than rectangles.",
          "category": "partially_correct"
        },
        {
          "text": "When you don’t have any labels.",
          "explanation": "Segmentation typically requires pixel-level labels for supervised training.",
          "category": "incorrect"
        },
        {
          "text": "When you only need one label for the image.",
          "explanation": "That’s image classification, not segmentation.",
          "category": "incorrect"
        },
        {
          "text": "When you want the model to repaint the image.",
          "explanation": "Segmentation labels pixels; it doesn’t recolor images as output.",
          "category": "ridiculous"
        },
        {
          "text": "Segmentation is for separating your lunch into sections.",
          "explanation": "Not applicable.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Computer vision: image classification, object detection, semantic segmentation"
    },
    {
      "id": "7a34f3c3ddb717c6910719e690cb4188d14caa3f5f7698a94baa7af31ad83616",
      "question": "What does transfer learning mean in the context of SageMaker image algorithms (MXNet/TensorFlow), and why is it valuable?",
      "tags": [
        "transfer_learning",
        "computer_vision",
        "mxnet",
        "tensorflow"
      ],
      "answers": [
        {
          "text": "Fine-tune a pretrained CNN backbone (e.g., ResNet) on your dataset; it reduces data needs and training time and often boosts accuracy.",
          "explanation": "Pretrained features generalize well and reduce label requirements.",
          "category": "correct"
        },
        {
          "text": "Reuse learned visual features from large datasets; update final layers (and sometimes deeper layers) for your classes.",
          "explanation": "This accelerates convergence and improves performance with limited labeled images.",
          "category": "correct"
        },
        {
          "text": "Reuse a pretrained model and retrain a bit.",
          "explanation": "Yes—fine-tuning is the essence of transfer learning.",
          "category": "partially_correct"
        },
        {
          "text": "Start with weights that already know general image features.",
          "explanation": "Correct—pretrained weights encode useful visual primitives.",
          "category": "partially_correct"
        },
        {
          "text": "Transfer learning means transferring images from S3 to EBS.",
          "explanation": "That’s data movement, not model transfer learning.",
          "category": "incorrect"
        },
        {
          "text": "It means copying the model without training.",
          "explanation": "Transfer learning typically includes fine-tuning to your target task.",
          "category": "incorrect"
        },
        {
          "text": "The model learns by teleportation.",
          "explanation": "Not applicable.",
          "category": "ridiculous"
        },
        {
          "text": "You transfer learning by emailing weights to the GPU.",
          "explanation": "Weights must be loaded and fine-tuned in a training job; email is irrelevant.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Computer vision: image classification, object detection, semantic segmentation"
    },
    {
      "id": "00222b3e551c2e401fe93fe8775b529b973c5bb6f97a3f8feba76c7bfede5dde",
      "question": "If inference needs to be near real-time and you need to locate objects, which built-in computer vision capability is designed for this use case?",
      "tags": [
        "object_detection",
        "realtime_inference",
        "computer_vision",
        "latency"
      ],
      "answers": [
        {
          "text": "Object Detection (e.g., SSD-style) for bounding boxes + classes with low-latency serving.",
          "explanation": "Detection outputs localization plus classification in a real-time-friendly format.",
          "category": "correct"
        },
        {
          "text": "Use an object detector when the output must include localization coordinates in real-time.",
          "explanation": "Classification alone can’t provide locations; segmentation is often heavier than detection for this need.",
          "category": "correct"
        },
        {
          "text": "Use an image model that outputs boxes.",
          "explanation": "Right idea—an object detector specifically outputs boxes.",
          "category": "partially_correct"
        },
        {
          "text": "Use detection, not classification.",
          "explanation": "Correct—because localization is required.",
          "category": "partially_correct"
        },
        {
          "text": "Use PCA to find the object coordinates.",
          "explanation": "PCA is dimensionality reduction, not object localization.",
          "category": "incorrect"
        },
        {
          "text": "Use topic modeling to locate objects.",
          "explanation": "Topic models are for text, not images.",
          "category": "incorrect"
        },
        {
          "text": "Use BlazingText to detect objects by reading the pixels aloud.",
          "explanation": "BlazingText is for NLP, not computer vision.",
          "category": "ridiculous"
        },
        {
          "text": "Use DeepAR to forecast where the object will be.",
          "explanation": "DeepAR is for time-series forecasting, not image localization.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Computer vision: image classification, object detection, semantic segmentation"
    },
    {
      "id": "8c2135a4675a90383fb3ffd1ad998b6ae95feb7afbff91e32d4f944cd120e654",
      "question": "What is PCA doing mathematically at a high level, and what problem does it commonly solve in ML pipelines?",
      "tags": [
        "pca",
        "dimensionality_reduction",
        "unsupervised",
        "feature_engineering"
      ],
      "answers": [
        {
          "text": "Find orthogonal directions (principal components) capturing maximum variance; project onto them for dimensionality reduction/denoising/compression.",
          "explanation": "PCA reduces dimensionality while preserving variance structure.",
          "category": "correct"
        },
        {
          "text": "Transforms features into a lower-dimensional basis that preserves as much variance as possible, often helping downstream modeling.",
          "explanation": "Reducing dimensions can speed training and reduce noise.",
          "category": "correct"
        },
        {
          "text": "It reduces the number of features.",
          "explanation": "Yes—by projecting into fewer components.",
          "category": "partially_correct"
        },
        {
          "text": "It compresses the dataset.",
          "explanation": "Correct—compression via low-dimensional representation.",
          "category": "partially_correct"
        },
        {
          "text": "It clusters points into groups.",
          "explanation": "Clustering is a different unsupervised objective (e.g., k-means).",
          "category": "incorrect"
        },
        {
          "text": "It’s a supervised classifier.",
          "explanation": "PCA is unsupervised dimensionality reduction.",
          "category": "incorrect"
        },
        {
          "text": "It stands for Please Compute Accuracy.",
          "explanation": "PCA stands for Principal Component Analysis.",
          "category": "ridiculous"
        },
        {
          "text": "It predicts customer churn by rotating axes.",
          "explanation": "PCA is not a classifier; it transforms features.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Unsupervised + anomaly + dimensionality reduction"
    },
    {
      "id": "1a41ec048551eb5f58950506df4758b822dda64851713fae538b88af0e1badea",
      "question": "What is K-Means clustering optimizing for, and why must you choose k up front?",
      "tags": [
        "kmeans",
        "clustering",
        "unsupervised",
        "hyperparameter_k"
      ],
      "answers": [
        {
          "text": "Minimizes within-cluster sum of squared distances to centroids; k defines how many clusters to form.",
          "explanation": "k is a core modeling assumption/hyperparameter.",
          "category": "correct"
        },
        {
          "text": "It optimizes compact clusters around k centers; you must set k because it’s a model assumption.",
          "explanation": "Different k values produce different partitions; you must decide based on goals/validation.",
          "category": "correct"
        },
        {
          "text": "It groups points into k clusters.",
          "explanation": "Yes; more precisely it minimizes within-cluster variance around centroids.",
          "category": "partially_correct"
        },
        {
          "text": "It finds centroids for k groups.",
          "explanation": "Correct; centroids define the cluster assignment objective.",
          "category": "partially_correct"
        },
        {
          "text": "It finds the best k automatically with no assumptions.",
          "explanation": "Standard k-means requires k; selecting k is an external choice (e.g., elbow/silhouette).",
          "category": "incorrect"
        },
        {
          "text": "It maximizes distance within clusters.",
          "explanation": "It minimizes within-cluster distances.",
          "category": "incorrect"
        },
        {
          "text": "k is chosen by the GPU based on its mood.",
          "explanation": "k is user-specified (or chosen by a separate selection procedure).",
          "category": "ridiculous"
        },
        {
          "text": "k stands for karma and depends on your behavior.",
          "explanation": "Not applicable.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Unsupervised + anomaly + dimensionality reduction"
    },
    {
      "id": "e81527624e4d6eaf53ccee6fe2f211037646e556ef96844f9cdf45108024891c",
      "question": "What is Random Cut Forest (RCF) used for, and what does it output for each data point?",
      "tags": [
        "random_cut_forest",
        "anomaly_detection",
        "unsupervised",
        "scores"
      ],
      "answers": [
        {
          "text": "Unsupervised anomaly detection; outputs an anomaly score per point based on how easily it gets isolated in random partitioning trees.",
          "explanation": "Outliers tend to be isolated with fewer cuts, yielding higher anomaly scores.",
          "category": "correct"
        },
        {
          "text": "Each observation gets a score indicating rarity/outlierness (and sometimes attribution-like signals).",
          "explanation": "RCF quantifies how unusual a point is relative to learned structure.",
          "category": "correct"
        },
        {
          "text": "It detects outliers.",
          "explanation": "Yes—by scoring points by anomaly likelihood.",
          "category": "partially_correct"
        },
        {
          "text": "It flags anomalies with a score.",
          "explanation": "Correct—the output is typically an anomaly score.",
          "category": "partially_correct"
        },
        {
          "text": "It predicts the next value in a time series.",
          "explanation": "That’s forecasting; RCF is for anomaly detection.",
          "category": "incorrect"
        },
        {
          "text": "It classifies images into categories.",
          "explanation": "RCF is not a computer-vision classifier.",
          "category": "incorrect"
        },
        {
          "text": "It’s a forest that randomly cuts down trees until an anomaly confesses.",
          "explanation": "Cutting is a metaphor for recursive partitioning.",
          "category": "ridiculous"
        },
        {
          "text": "It detects anomalies by counting leaves.",
          "explanation": "The anomaly score is not based on leaf counts alone.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Unsupervised + anomaly + dimensionality reduction"
    },
    {
      "id": "f395da31ad6f2d3d50c4f91e5a5cfe89f6956f8f5f3c7389c71df661c4c4bfc2",
      "question": "What is IP Insights designed to learn, and why is it useful for anomaly detection in network/security contexts?",
      "tags": [
        "ip_insights",
        "anomaly_detection",
        "security",
        "network_behavior"
      ],
      "answers": [
        {
          "text": "Learns patterns/embeddings of relationships between IPs and entities (accounts/users); flags unusual IP–entity pairings indicative of compromise/fraud.",
          "explanation": "Rare pairings can signal account takeover or suspicious access.",
          "category": "correct"
        },
        {
          "text": "Models normal connection behavior so rare combinations (new IP for account, strange co-occurrence) get higher anomaly scores.",
          "explanation": "It learns typical association structure and highlights deviations.",
          "category": "correct"
        },
        {
          "text": "It flags suspicious IP activity.",
          "explanation": "Yes—more specifically it flags unusual association patterns, not just raw volume.",
          "category": "partially_correct"
        },
        {
          "text": "It detects unusual logins from unfamiliar IPs.",
          "explanation": "Correct—unfamiliar IP-to-entity pairings can be suspicious.",
          "category": "partially_correct"
        },
        {
          "text": "It translates IP addresses into English sentences.",
          "explanation": "It models relationships/associations for anomaly detection, not translation.",
          "category": "incorrect"
        },
        {
          "text": "It only works for image metadata, not network behavior.",
          "explanation": "It’s intended for network/security association patterns, not images.",
          "category": "incorrect"
        },
        {
          "text": "It tells you an IP address’s personality type.",
          "explanation": "Not applicable.",
          "category": "ridiculous"
        },
        {
          "text": "It guesses the ISP’s favorite color.",
          "explanation": "Not applicable.",
          "category": "ridiculous"
        }
      ],
      "topicId": "domain5-amazon-sagemaker-built-in-algorithms",
      "subtopic": "Unsupervised + anomaly + dimensionality reduction"
    }
  ]
}
